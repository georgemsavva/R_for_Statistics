---
title: "Introduction to ‘R’ for statistical analysis"
date: September 2023
author: 
  - George Savva, QIB
  - Pirita Paajanen, JIC
always_allow_html: true
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 2
    smooth-scroll: true
    number-sections: true
---



```{r echo=FALSE}
#### I WROTE THESE FUNCTIONS TO CREATE NUMBERED SECTIONS/EXERCISES/EXAMPLES WHEN I STARTED MAKING THE NOTES
#### QUARTO CAN PROBABLY DO THIS AUTOMATICALLY NOW, I'LL REPLACE THEM WHEN I GET AROUND TO IT!

exerciseNumber=0;
exercise <- function(x="") {
  exerciseNumber <<- exerciseNumber+1; return(sprintf("## Exercise %d. %s", exerciseNumber, x))
  }

subsection <- function(x="") {
  return(sprintf("## %s", x))
  }
exampleNumber=0;
example <- function(x="") {
  exampleNumber <<- exampleNumber+1; return(sprintf("## Worked Example %d. %s", exampleNumber, x))
  }

```

# Introduction

This course will introduce 

* R and RStudio statistical software
* examples of performing common tasks in scientific data analysis
* where to go for further support

The aim is to become familiar with the R/RStudio environment and some common functions and workflows.  This will enable you to learn the specific functions that you need on your own or with further training.

All of the commands for the worked examples and the exercises is in the file TutorialScript.R.  The commands for the worked examples are also typed out here.

Powerpoint slides for today's session are here:

* [Day 1 slides](day1.pptx)

This handout was written in RStudio using the Quarto document preparation system.  The source code is here: [day1.qmd](day1.qmd)

`r subsection("Learning objectives")` 

We will focus on the tasks used in a typical analysis of a single scientific dataset, mirroring the tasks usually conducted in other statistical software.

### Specific tasks:


Day 1:  R and RStudio basics

*	Familiarity with R and RStudio
*	Exploring data and calculating descriptive statistics

Day 2:  Some more statistics

* Revise and consolidate day 1 learning
* Loading and wrangling data
*	Simple hypothesis tests

Day 3:  Linear models using R with real data

* Estimating, diagnosing and interpreting linear models
* Making graphs using the ggplot2 package


# R and RStudio

## What are R and RStudio?

**R** is a free and open source statistics package, initially developed during the 1990s, and that has now become the world’s most widely used and comprehensive statistical software.  R calls itself a *programming language and environment for statistic computing*.  

That is, 'R' refers both to the software itself and the programming language that you use to interact with it.

**RStudio** is a free open source integrated development environment (IDE) for R that makes working R much easier.  Most R users use RStudio and we recommend using RStudio for new users.

The great strength of R is in its contributed packages, these are community written add-ons that make R much more powerful and easy to use.  We will introduce some commonly used packages for data management, analysis and graphing during this course.

## Getting R and RStudio

*  If you are using a PC in the JIC IT training suite it will already have an up-to-date version of R and RStudio.  

*  For other NBI managed devices you can install R and RStudio from the NBI software catalogue.

*  If you want to install R and RStudio on your own device:

    1. Download and install the latest version of R from <https://cran.r-project.org/>

    2. Then download and install RStudio from <https://www.rstudio.com/>

## Starting RStudio

Start RStudio.  It will detect your installation of R, and you should see a screen like this:

![Figure:  RStudio Window](./rstudio.png)

On the left is the console window, where you type commands and see output.  The windows on the right hold various useful tabs, including your data, graphs, help files, and your command history.

## Check R and RStudio are working, run your first command


1. Click in the console window and type:

```{r eval=FALSE, echo=TRUE}
1+2
```

Press return on your keyboard.  You should see: 

```{r echo=FALSE, eval=TRUE}
1+2
```

2. Try a few other mathematical functions at the R console (eg):

```{r echo=TRUE, eval=FALSE}
sin(pi/2)
log(10)
exp(2)
1e4
1/0
```

# Using projects and scripts

Before we go any further, we are going to start an RStudio ‘project’ to organise our work during this course. Using projects helps us to keep all of the data and analysis for a particular piece of work in the same place.

* Click on ‘New &rarr; New project’ in the main toolbar.  
* Click ‘Start a new project in a brand new working directory’.  
* Finally click ‘new project’ on the next screen.

Now you can choose where to create the new directory for your R project, and what to call it.  

* Make a project called ‘Rtraining’ or something like that, somewhere in your *personal filestore*.

* **If you are not comfortable making directories and working with files in your personal filestore, we'll fix that now because it's important!**

Now, when you return to the main RStudio window you are working within your project.  Notice that:

* The 'working directory' has automatically switched to the new directory that you created.
* The 'files' tab on the bottom left of the window is now showing the root of the project directory.

In practice an RStudio project directory should include all of the data files, source files and generated results corresponding to an analysis project.  Eg for each paper I work on I tend to keep an RStudio project including all of the data files and scripts for that project.  Using projects also help you to work with version control systems (like Github).

## Making a script

We could do everything by typing commands into the console window as we have already seen, but this is not good if we want to remember or repeat something we have done, or share it with others.

So instead we will type our commands into files called R scripts and run the commands from there.  With a script you can run and re-run bigger analyses that chain together all the functions you need for data loading, cleaning, analysing, reporting etc.

Using scripts mean we can develop complex analyses, and that when we come back to them later, eg if something changes in our data that means we need to redo everything, or we want to tweak something in our analysis because of a reviewer's comment, we can easily do this.  

::: {callout-tip}
It is good to keep a separate R script for each analysis that you do, such that each starts with the functions to load the required data, then do any cleaning or coding that is necessary, then to perform and report the data analysis.

If you start making more complex projects you may want to write separate scripts for each of these elements.
:::

An example R script, annotated with comments, is in the files that accompany this handout.

### Make a script

1. Make a new script.  Click on File &rarr; New File &rarr; R Script in the main RStudio window.  An empty file will appear in the top-left pane.

2. Save your script with a sensible filename (even though it is empty).  Having unsaved scripts is a bad idea, RStudio is sometimes unstable and while it will try to recover unsaved work it is not always guaranteed to.  Get into the habit of saving your scripts regularly.  Make sure it has the file extension `.R`

3. Put some of the mathematical functions that you have already tried into your script, with one on each line.

You can now run code from scripts in several ways.  Try each of these:

1. Press 'run' or type Ctrl+Enter on the keyboard, RStudio will send the line that the cursor is on to the R console and will run it.

2. If you highlight an area of the script and then hit 'run' (or press Ctrl+Enter) then RStudio will send all the highlighted code to the R console.

3. If you save the file, then press 'source', R will load the file from disk and run all the commands from that file in sequence.

If you have your raw data saved, and you keep your scripts, then you don’t need to save your results or any of the variables that you generated or modified during your analysis.  So long as the original data doesn’t change, running the script will reproduce all of your analysis and output.  This is usually a better way of working than trying to save your environment with all of your results and tables in.

I have created a script including all the analyses from this tutorial, in TutorialScript.R.  Open this and have a look around.  Notice my comments to remind myself why I did things, this might be helpful when I next come to revise the analysis!

See:

* https://kdestasio.github.io/post/r_best_practices/
* https://r4ds.had.co.nz/workflow-projects.html
* https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects
* https://rstats.wtf

for more information on using projects and scripts

# Objects and functions

So far we have used R as a calculator.  Next we'll see how R stores objects and applies functions to data.  

[Objects, functions, packages and pipes tutorial](praise.qmd)

## Functions

Everything in R is done by executing ‘functions’.  When you typed `1+2` at the console above you were calling the `+` function, with `1` and `2` as its arguments, and the result was printed in the console window.

Simple mathematical functions can be written using standard notation in this way (eg `1+2` or `3/4`) but functions are more commonly called by their *name*, with their *arguments* in brackets, separated by commas.  For example, to get say the logarithm (base 10) of 100, we would type

```{r }
### Try this directly in the console, and by running it from your new script.
log(x=100, base=10)
```

Here, `log` is the name of the function, with `x` and `base` its arguments.  The result is the *value* of the function (the value is what is returned).

* Try each of the following commands.  Do you understand what they do any why?

```{r eval=FALSE }
### From now on, keep everything you try in a script file.

log(x=100, base=10)
log(x=100)
log(base=10, x=100)

log(100,10)
log(10,100)

1000 |> log()           #  Note the use of the pipe operator |> as an alternative way to call a function. 
100 |> log(base=10)

log()
log

```

### Getting help

In the console, type: 
```{r eval=FALSE}
?log
```

to read the help file for the `log` function.  All R help files are structured in this way.  Look at the `Usage`,  `Arguments` and `Value` sections.  These will be invaluable when you come to use R and use functions that you do not already know.

## Objects

Instead of directly displaying the value of the function (‘value’ is what R calls the result of a function), you can give it a name and store it for later use:

```{r eval=FALSE} 
x = 1+2
```

or

```{r eval=FALSE}
x <- 1+2
```

This does exactly the same thing; some R users use the arrow `<-` instead of `=` for assignment, so both forms will come up when you’re looking at help files or other people’s code.  I (annoyingly!) tend to use either interchangeably.

Now you have an object called `x` in your *environment* that holds the number 3 (check your environment window).  You can ask R to display the value of ‘x’ by just entering x (just entering the name of an object *prints* that object):

```{r eval=FALSE}
x
```

Or you could do something else with `x`

```{r eval=FALSE}
x*2
log(x)
x |> log()
x |> log(10)
```

What does this do?

```{r eval=FALSE}

x |> log() |> sqrt()

```

Hint:  it might help to understand if you read the pipe operator `|>` as "and then..."

To see the *class* of an object (what kind of thing is stored in the object), use the `class()` function.

```{r eval=FALSE }
class(x)
```

Objects of different classes store different kinds of information.  We will come across objects of different classes later.

## Test yourself

1.  Make a new object called y which has the value of x+3.  Then display y.
2.	Now change the value of x (eg using  x <- 6 ).  Does the value of y change?
3.	Objects can hold text strings instead of numbers.  Try:

```{r eval=FALSE}
myname <- "George"  #(or whatever your name is).
myname
```

What is the class of the ‘myname’ object?

4.	(Difficult!)  Look up the function to turn a text string into upper case (an internet search will help you).  Use this function to make a new object which has the same text as ‘myname’ but in upper case.

# Using R to analyse data

We can R as a simple calculator like this but we'll most often be using R to analyse datasets.

Later we'll look into getting real data into R from Excel files, but for now we'll work on a built-in dataset to understand how data is stored and how to describe it.  

We'll make some descriptive statistics.  

::: {.callout-note}
There are two types of statistical analysis:

*Descriptive* statistics are used to describe your sample.

We use *analytic* statistics to test our hypotheses, estimate treatment effects and learn about populations.

Both types are important!
:::

## Explore the structure of the dataset

Start a new script to begin your analysis of the 'iris' dataset.

Type:

```{r }
data(iris)
```

Check your 'environment' tab for a new object.  Click on the object name to see it in the Viewer, or click on the small blue button next to it to expand the view in the environment window.

What is the class of the `iris` object that you have created?

What is in this data?  What kind of descriptive statistics might we calculate to learn about this dataset?

There are a few different functions you can use to explore a dataset.  Test each to see what they do.

```{r eval=FALSE}
str(iris)
head(iris)
summary(iris)
dim(iris)
nrow(iris)
View(iris)
```

The `iris` data is an example of a dataset in *tidy* form.  We'll learn more about this in the next session, but in short in has:

* A single rectangular table to represent the data
* A column for each variable
* A row for each observation

This is very similar to how datasets are stored in software like SPSS or Stata, or in a database, but is different to how you might record data using GraphPad Prism.

## Use the data in calculations

We can extract individual variables (characteristics) and individual elements like this:

```{r eval=FALSE}

iris$Sepal.Length 

## What do these lines do?

iris$Sepal.Length[1]

iris$Sepal.Length[c(2,4,6)]

iris$Sepal.Length[10:20]

## What do these lines do?

c(2,4,6)

10:20

## What does this do?

plot(iris, col=iris$Species)

```

::: {.callout-note}
`iris$Sepal.length` is a 'vector'.  A vector is a list of items all of the same type.  Here its a vector of numbers.  A data frame is a collection of vectors.  

What are the types of each of the other vectors in the `iris` data frame?
:::

We can use functions to get descriptive statistics for a vector:

```{r eval=FALSE}

mean(iris$Sepal.Length)

iris$Sepal.Length |> mean()

sd(iris$Sepal.Length)

iris$Sepal.Width |> hist()

```
Can you make a histogram with red bars?

We can work on more than one variable at a time:

```{r eval=FALSE}

cor(iris$Sepal.Length, iris$Sepal.Width)

cor.test(iris$Sepal.Length, iris$Sepal.Width)

```

```{r eval=FALSE}
with(iris , cor(Sepal.Length , Sepal.Width))
```


## Test yourself - More exploration of the iris dataset

1. How many rows does the `iris` dataset have?
2. What is the class of each of its columns?
3. What is the mean sepal length of the flowers
4. What is the smallest (minimum) sepal width
5. (Difficult) How many `viriginca` flowers are included in the dataset?  Can you tabulate the `species` variable?
6. Can you make a cow say the median petal length?

## Other packages for descriptive statistics (if you have time)

(If we have time) `Base` R does not have good functions for making descriptive tables and graphs.  But there are many other packages that have been developed to help with this.  In particular the `gt` system is very good for making tables, and the `GGally` package has a nice function for making pairs plots.  You could install these packages and try them out:

```{r eval=FALSE}
install.packages("gtsummary")
gtsummary::tbl_summary(iris)
```

Can you stratify the table by species? (Check the help)

```{r eval=FALSE}
install.packages("GGally")
GGally::ggpairs(iris)
```

Compare this to the output from the 'base' function `pairs(iris)` or `plot(iris)`.

# Tidyverse

So far we haven't tried to subset or stratify any analysis.  Usually we'll be interested in comparing different parts of the data, or looking at subgroups.

Before we go any further we're going to install another set of packages.  `Base' R has a few ways to manipulate datasets, but these can get very clunky and you won't use them much in practice so it's better to start using add-on packages as soon as possible.

The two major packages of data wrangling functions are `data.table` and `tidyverse`.  

Tidyverse is more widely used and so this is what we will focus on here.  If you become more familiar with R you might want to explore `data.table`.  

::: {.callout-note}
I am only going to teach `tidyverse` methods because they are the most widely used.  See [https://wetlandscapes.com/blog/a-comparison-of-r-dialects/] for how to use base or data.table to perform the same options.
:::

Tidyverse is in fact a large collection of packages.  You can see all of the details here https://www.tidyverse.org/packages/.  In this course we will use functions from the following packages:

* **dplyr** for data manipulation
* **ggplot2** for making graphics
* **broom** for organising analysis results

Check if you have `tidyverse` installed on the computer you are using.  Use the `packages` pane in the bottom right hand section of the screen and search for `tidyverse`.  If it is installed you will see a corresponding line in the `packages` pane with the current installed version number.

If you do not have the tidyverse installed, then install it now from the menu or by using

```{r eval=FALSE}
install.packages("tidyverse")
```

This will install a lot of packages and might take a few minutes.

Once the tidyverse packages are installed you can use all of their functions.  There are two ways to use functions installed as part of a package.  For example, to use the `glimpse` function from the `dplyr` package to look at the `iris` dataset, you can type:

```{r eval=FALSE}
dplyr::glimpse(iris)
```

or

```{r eval=FALSE, warning=FALSE}
library(dplyr) # or library(tidyverse)
glimpse(iris)
```

Using `library` adds the library to the search path for the whole of your R session, so R will be able to find all of the functions in this library without explicitly referencing the package every time.

I will load the tidyverse library, which will attach all of the 'core' tidyverse packages into my current session:

```{r eval=TRUE, echo=TRUE, warning=FALSE}
library(tidyverse)
```

## Using dplyr functions to manipulate and summarise data

The five main dplyr functions are `select`, `filter`, `mutate`, `group_by`, and `summarise`.  

Let's learn by example what each of these functions does:

```{r eval=FALSE}
iris |> select(Sepal.Length, Sepal.Width)

iris |> filter(Species=="setosa")

iris |> filter(Sepal.Length > 7.5)

iris |> arrange(Sepal.Length)

iris |> summarise(`Median Petal Width` = median(Petal.Width))

iris |> summarise(`Mean` = median(Petal.Width), `SD` = sd(Petal.Width))

iris |> mutate(Sepal.Area = Sepal.Length * Sepal.Width)

iris2 <- iris |> mutate(logPetalLength = log(Petal.Length))

iris2 <- iris |> mutate(PetalLengthGrouped = ntile(Petal.Length) )


```

So with `dplyr` we can take subsets of our data or our samples, split our samples up into groups, create new derived variables and make summary statistics.

Using pipes you can chain functions together.  Suppose you wanted the mean sepal length of `setosa` flowers:

```{r eval=FALSE}
iris |> filter(Species=="setosa") |> summarise(mean(Sepal.Length))
```

`group_by` and `summarise` are often used together, to make summaries over groups.

```{r eval=FALSE}
iris |> group_by(Species) |> summarise(mean(Sepal.Length), mean(Sepal.Width), n=n())
```

## Test yourself: data wrangling and summarising

Can you use dplyr to find the mean of the sepal length for flowers with a petal length less than 2?

# Graphics

R has a simple built in system for making graphs but the outputs are not very good!

```{r }
boxplot(Petal.Length ~ Species, data=iris)
```

:::{callout-note}
Although this `boxplot` function isn't very useful, note the `~` syntax as this is used widely elsewhere.
:::

The ggplot() system is much more flexible, and produces publication quality outputs:

```{r }
iris |> ggplot() + aes(x=Sepal.Length, y=Sepal.Width, color=Species) + geom_point()

iris |> ggplot() + 
  aes(x=Species, y=Petal.Length, fill=Species) + 
  geom_boxplot() + 
  theme_bw()

```

We'll see more ggplot in later sessions, but note the syntax, in partidular how a dataset is passed to the ggplot function, how we specify the linkage between the data and the plot, and how different elements are added to make the plot.

# Hypothesis tests and linear models

Descriptives are useful but our goal in science is usually to test hypotheses or understand the mechanisms behind our data generation.

So, we might observe a difference in petal length between species in our sample, but we want to know if this reflects a real difference in average petal length in the population.  That is, does petal length depend on species?  

First, we can generate a simple box plot as previously to explore this question:

```{r }
ggplot(data=iris, aes(x=Species, y=Petal.Length, fill=Species)) + 
  geom_boxplot() + 
  labs(x="Species", y="Petal Length") + 
  theme_bw()
```

It looks like the two variables are related,  but we would like to perform a hypothesis test to see if the difference is statistically significant, and to get an estimate (with confidence interval) for the difference.

To get the coefficients (mean differences between groups) we can estimate a linear model:

```{r }
model1 <- lm( Sepal.Length ~ Species, data=iris)
```

When you estimate a model R will not display any output, it is all stored in the model object that is created.  Then you can access whatever output you want using other functions, eg:

```{r }
summary(model1)
anova(model1)
```

You can use either the base function `confint` or the `tidy` function from the `broom` package (`broom` is installed with `tidyverse`) to get the confidence intervals for the mean differences:

## Test yourself

1. Can you estimate the linear model and display the summary in one line using pipes?
2. How much of the output summary do you understand?

```{r }
# Base R methods:
confint(model1)

# Tidyverse method:
broom::tidy(model1, conf.int=TRUE)
```

::: {.callout-note}
Note - you can run t-tests like this, since a t-test is equivalent to a linear model with two group categorical predictor.
:::

All pairwise comparisons using `emmeans` or t-tests for individual pairs with `filter` then `lm`?

### More complex models

Now. suppose we want to fit a more complex model, whereby sepal length depends on petal length, with the relationship allowed to vary by species.

```{r }

# First make a model with Sepal.Length depending on Species and Petal.Length
model2 <- lm( Sepal.Length ~ Species + Petal.Length, data=iris)
summary(model2)

# Now add the interaction term.  These two models are the same.
model3 <- lm( Sepal.Length ~ Species + Petal.Length + Species:Petal.Length, data=iris)
model3 <- lm( Sepal.Length ~ Species * Petal.Length, data=iris)
summary(model3)

```

We can use `augment` from the `broom` package to add predicted values to our dataset, with confidence intervals.  

```{r }
# look carefully at the output from here:
library(broom)
model3 |> augment(interval="confidence")
```

Finally we can use ggplot to show these predicted values and confidence intervals.  Take care to understand how the output from each function (ie `lm`, `augment`) is piped into the next.

```{r }


lm( Sepal.Length ~ Species * Petal.Length, data=iris) |>
  augment(interval="confidence") |> 
  ggplot() + 
    aes(x=Petal.Length, y=Sepal.Length, color=Species) + 
    geom_ribbon(aes(y=.fitted,ymin=.lower, ymax=.upper), alpha=0.1) + 
    geom_point() + 
    geom_line(aes(y=.fitted),lwd=1) + 
    theme_bw()

# Try changing the forumula to `Sepal.Length ~ Species + Petal.Length`
# Which model has the better fit? (you can use anova(model2, model3) to compare models)


```



# More detail on types and classes

We saw two object of two different 'classes' earlier. These classes were 'numeric' and 'character'.  The class of an object defines what kind of information it holds, and how other functions act on it.

There are four basic classes (modes) that you will commonly use and should be aware of.  These correspond to the types of data you might have.  The basic types are:

*	'numeric' – For keeping numbers, can be discrete or continuous
*	'logical' – can only take the values (TRUE or FALSE)
*	'character' – for strings of text
*	'factor' – for labelled categorical variables (ordered or unordered)

Later in this tutorial we will see objects of more complex classes, these can store lots of different information of different modes:

*	‘data.frame’ – storing datasets
*	‘lm’ – stores the all results of a linear regression model

## Character strings

Character strings represent text rather than numbers.  Strings are used to label categories in a dataset, to identify columns in a dataset, to make your outputs more readable.  You also might find that part of your data has been entered as a string, for example patient identifiers or gene names in a database, or responses to open ended questions.

Strings are identified in R (and in most other programming languages) by enclosing them in quotes.  Single quotes and double quotes can be used (and are treated almost identically), but double quotes are preferred.  For example try:

```{r eval=FALSE}
print("Hello")

print('Hello')

# What happens here?
print(Hello)
```

A common mistake in R is to forget to enclose strings in quotes.  In which case R tries to interpret your input as an object name, leading to an error message if that name doesn’t exist.

## Logicals

Logicals represent binary information in the form `TRUE` or `FALSE`.  They most often arise as the result of a comparison, for example try:

```{r eval=FALSE}
3>2

"Hello" == "hello"  # note the double equals sign, this distinguishes assignment from comparison

```

## Converting between types

Sometimes it is possible to convert an object from one class to another.  For example, a number might be stored as a character string in your data, and you will need to convert it into a numeric before you can do any analysis with it. For example:

```{r eval=FALSE}
x <- "3"
x*2 # What is the error message here?  What does it mean?

y <- as.numeric(x)
y*2
```


## Missing elements in vectors

Often your data will include missing values.  R uses `NA` to represent missing values.  For example the following creates a vector (a single variable, like a single column of a data frame) with a missing value in the fourth position:

```{r eval=FALSE}
weights <- c(10,21,32,NA,14)
weights
```
:::{.callout-warning}
Note the difference between `NA` (a missing value) and `"NA"` (a character string containing the letters N and A.  I have been tripped up by this a few times when `"NA"` has been entered into a dataset.)
:::

`r exercise("Effect of missing values")`

Try some other functions with myvector to see what impact the missing data point has.

```{r eval=FALSE}
class(weights)

plot(weights)

weights>20

mean(weights) # what happens here?  Why?  Can you fix it?

is.na(weights) # what does this do?

sum(is.na(weights)) # can you explain what this does?
```

# More descriptive statistics

In this rest of this session we'll use base and tidyverse functions to complete some common descriptive and analytic tasks using the iris dataset.

## A continuous variable stratified by a grouping variable, and the formula interface

We already saw a way to get means over a categorical variable with Tidyverse:

```{r eval=FALSE}
library(tidyverse)
iris |> group_by(Species) |> summarise(mean(Sepal.Length))
```


If we want more than one summary statistic for example, it's easy with tidyverse (this is an example of a task that is difficult with base R):

```{r eval=FALSE}
iris |> group_by(Species) |> summarise(n(),mean(Sepal.Length),sd(Sepal.Length))
```

While dplyr is good for making summary statistics, but to make publication ready tables you can explore the `gtsummary` package.  
https://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html

```{r }
library(gtsummary)
iris |> tbl_summary( by=Species )
```


## Recoding a variable into groups

Suppose we want to classify flowers into three groups based on their petal length.  We need to add another categorical variable to the dataset.

What class should that new variable be?

Base R has the function `cut` that divides continuous variables into groups.  Tidyverse has a few extensions of this (weirdly in the ggplot2 package), including `cut_number` that can divide up a continuous variable into three equal groups, and allow us to add labels to them.

```{r eval=FALSE}
cut_number(iris$Petal.Length, 3, labels=c("Short", "Medium", "Long") )
```

```{r }
# Using tidyverse to create this variable and add it to the dataset:
library(ggplot2)

iris <- iris |> mutate(PetalLengthGrouped = cut_number(Petal.Length, 3, labels=c("Short", "Medium", "Long") )) 

```

## Tabulating a categorical variable over groups

For a categorical variable, a summary of frequency counts might be the most appropriate descriptive statistic.  We can get this with the `table()` function.  Suppose we wanted to know the distribution of species in our dataset.

```{r eval=FALSE}
table(iris$Species)
```

The ‘table’ function can also generate cross-tabs, by specifying two or three variables.

```{r eval=FALSE}
table(iris$Species, iris$PetalLengthGrouped) # Base R version

iris |> janitor::tabyl(Species, PetalLengthGrouped)  #  The tidyverse version using `tabyl` from the `janitor` package.
iris |> group_by(Species, PetalLengthGrouped) |> tally()

```


Tables of counts are useful, but it might be more helpful to see the proportion of healthy trees by species.  To get this we can pass the table we just made into the prop.table() function:

```{r }
table1 <- table(iris$Species, iris$PetalLengthGrouped)
prop.table(table1)
```

This is where pipes might be more intuitive:

```{r eval=FALSE}
table(iris$Species, iris$PetalLengthGrouped) |> prop.table()
```
The table above has calculated the ‘cell proportions’.  If we want the row percentages we need to set the `margin` option in `prop.table()` appropriately. `margin=1` is the rows, `margin=2` is the columns.  If you left out margins altogether, you’d get the cell proportions.

We could round this (or any numeric) to 2 d.p. by passing the resulting proportion table into the `round()` function. and using the function `multiply_by` from the `magrittr` package (installed as part of tidyverse) we can turn our proportions into percentages:

```{r }
library(magrittr)

table(iris$Species, iris$PetalLengthGrouped) |> 
  prop.table(margin=1) |> 
  multiply_by(100) |> 
  round(digits=2)
```

You might decide that using a function like `gtsummary::tbl_summary()` is easier!
