[
  {
    "objectID": "datatabletidyverse.html",
    "href": "datatabletidyverse.html",
    "title": "Descriptive tables using base R, data.table and tidyverse",
    "section": "",
    "text": "With only base R (that is, R without add on packages) it can be unexpectedly difficult to perform some simple tasks.\nA good example is making a table of summary statistics. This is difficult with base R but is simple with using function from add-on packages.\nHere I illustrate this using two widely used systems for data manipulation in R, namely data.table and tidyverse. Both can be used to make summary tables of descriptive statistics. that can be exported\nFinally I describe a package, gtsummary that is specifically designed for creation of publication ready summary tables."
  },
  {
    "objectID": "datatabletidyverse.html#breaking-down-the-data.table-syntax",
    "href": "datatabletidyverse.html#breaking-down-the-data.table-syntax",
    "title": "Descriptive tables using base R, data.table and tidyverse",
    "section": "Breaking down the data.table syntax",
    "text": "Breaking down the data.table syntax\nThe [ operator in data.table has three arguments. In short, we express a command on a dataset (here called dat) by specifying:\n\ndat[ which rows to use , what to do , which columns to group on ]\n\nIn the first version of the command above we left the first entry blank (so used all the rows), placed mean(height) in the second position and specified by=sex in the third. In the second version we expanded the second argument to return a list of elements, and gave them new names.\nFor more details of using data.table, see: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html"
  },
  {
    "objectID": "datatabletidyverse.html#the-tidyverse-dplyr-syntax",
    "href": "datatabletidyverse.html#the-tidyverse-dplyr-syntax",
    "title": "Descriptive tables using base R, data.table and tidyverse",
    "section": "The tidyverse (dplyr) syntax",
    "text": "The tidyverse (dplyr) syntax\ndplyr introduces six main functions for manipulating and summarising data, these are mutate, arrange, select, filter, summarise, and group_by. Using combinations of these functions you can perform most simple data operations. Functions are chained together using the pipe operator %&gt;% which passes the output from one into the next.\nSo the first command above reads something like: “take dat, then group it by sex, then for each group return the summary statistics we specified”.\nVisit https://www.tidyverse.org/learn for more."
  },
  {
    "objectID": "day1.html",
    "href": "day1.html",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "",
    "text": "This course will introduce\n\nR and RStudio statistical software\nexamples of performing common tasks in scientific data analysis\nwhere to go for further support\n\nThe aim is to become familiar with the R/RStudio environment and some common functions and workflows. This will enable you to learn the specific functions that you need on your own or with further training.\nAll of the commands for the worked examples and the exercises is in the file TutorialScript.R. The commands for the worked examples are also typed out here.\nPowerpoint slides for today’s session are here:\n\nDay 1 slides\n\nThis handout was written in RStudio using the Quarto document preparation system. The source code is here: day1.qmd\n\n\nWe will focus on the tasks used in a typical analysis of a single scientific dataset, mirroring the tasks usually conducted in other statistical software.\n\n\nDay 1: R and RStudio basics\n\nFamiliarity with R and RStudio\nExploring data and calculating descriptive statistics\n\nDay 2: Some more statistics\n\nRevise and consolidate day 1 learning\nLoading and wrangling data\nSimple hypothesis tests\n\nDay 3: Linear models using R with real data\n\nEstimating, diagnosing and interpreting linear models\nMaking graphs using the ggplot2 package"
  },
  {
    "objectID": "day1.html#learning-objectives",
    "href": "day1.html#learning-objectives",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "",
    "text": "We will focus on the tasks used in a typical analysis of a single scientific dataset, mirroring the tasks usually conducted in other statistical software.\n\n\nDay 1: R and RStudio basics\n\nFamiliarity with R and RStudio\nExploring data and calculating descriptive statistics\n\nDay 2: Some more statistics\n\nRevise and consolidate day 1 learning\nLoading and wrangling data\nSimple hypothesis tests\n\nDay 3: Linear models using R with real data\n\nEstimating, diagnosing and interpreting linear models\nMaking graphs using the ggplot2 package"
  },
  {
    "objectID": "day1.html#what-are-r-and-rstudio",
    "href": "day1.html#what-are-r-and-rstudio",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.1 What are R and RStudio?",
    "text": "2.1 What are R and RStudio?\nR is a free and open source statistics package, initially developed during the 1990s, and that has now become the world’s most widely used and comprehensive statistical software. R calls itself a programming language and environment for statistic computing.\nThat is, ‘R’ refers both to the software itself and the programming language that you use to interact with it.\nRStudio is a free open source integrated development environment (IDE) for R that makes working R much easier. Most R users use RStudio and we recommend using RStudio for new users.\nThe great strength of R is in its contributed packages, these are community written add-ons that make R much more powerful and easy to use. We will introduce some commonly used packages for data management, analysis and graphing during this course."
  },
  {
    "objectID": "day1.html#getting-r-and-rstudio",
    "href": "day1.html#getting-r-and-rstudio",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.2 Getting R and RStudio",
    "text": "2.2 Getting R and RStudio\n\nIf you are using a PC in the JIC IT training suite it will already have an up-to-date version of R and RStudio.\nFor other NBI managed devices you can install R and RStudio from the NBI software catalogue.\nIf you want to install R and RStudio on your own device:\n\nDownload and install the latest version of R from https://cran.r-project.org/\nThen download and install RStudio from https://www.rstudio.com/"
  },
  {
    "objectID": "day1.html#starting-rstudio",
    "href": "day1.html#starting-rstudio",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.3 Starting RStudio",
    "text": "2.3 Starting RStudio\nStart RStudio. It will detect your installation of R, and you should see a screen like this:\n\n\n\nFigure: RStudio Window\n\n\nOn the left is the console window, where you type commands and see output. The windows on the right hold various useful tabs, including your data, graphs, help files, and your command history."
  },
  {
    "objectID": "day1.html#check-r-and-rstudio-are-working-run-your-first-command",
    "href": "day1.html#check-r-and-rstudio-are-working-run-your-first-command",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.4 Check R and RStudio are working, run your first command",
    "text": "2.4 Check R and RStudio are working, run your first command\n\nClick in the console window and type:\n\n\n1+2\n\nPress return on your keyboard. You should see:\n\n\n[1] 3\n\n\n\nTry a few other mathematical functions at the R console (eg):\n\n\nsin(pi/2)\nlog(10)\nexp(2)\n1e4\n1/0"
  },
  {
    "objectID": "day1.html#making-a-script",
    "href": "day1.html#making-a-script",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "3.1 Making a script",
    "text": "3.1 Making a script\nWe could do everything by typing commands into the console window as we have already seen, but this is not good if we want to remember or repeat something we have done, or share it with others.\nSo instead we will type our commands into files called R scripts and run the commands from there. With a script you can run and re-run bigger analyses that chain together all the functions you need for data loading, cleaning, analysing, reporting etc.\nUsing scripts mean we can develop complex analyses, and that when we come back to them later, eg if something changes in our data that means we need to redo everything, or we want to tweak something in our analysis because of a reviewer’s comment, we can easily do this.\n\nIt is good to keep a separate R script for each analysis that you do, such that each starts with the functions to load the required data, then do any cleaning or coding that is necessary, then to perform and report the data analysis.\nIf you start making more complex projects you may want to write separate scripts for each of these elements.\n\nAn example R script, annotated with comments, is in the files that accompany this handout.\n\n3.1.1 Make a script\n\nMake a new script. Click on File → New File → R Script in the main RStudio window. An empty file will appear in the top-left pane.\nSave your script with a sensible filename (even though it is empty). Having unsaved scripts is a bad idea, RStudio is sometimes unstable and while it will try to recover unsaved work it is not always guaranteed to. Get into the habit of saving your scripts regularly. Make sure it has the file extension .R\nPut some of the mathematical functions that you have already tried into your script, with one on each line.\n\nYou can now run code from scripts in several ways. Try each of these:\n\nPress ‘run’ or type Ctrl+Enter on the keyboard, RStudio will send the line that the cursor is on to the R console and will run it.\nIf you highlight an area of the script and then hit ‘run’ (or press Ctrl+Enter) then RStudio will send all the highlighted code to the R console.\nIf you save the file, then press ‘source’, R will load the file from disk and run all the commands from that file in sequence.\n\nIf you have your raw data saved, and you keep your scripts, then you don’t need to save your results or any of the variables that you generated or modified during your analysis. So long as the original data doesn’t change, running the script will reproduce all of your analysis and output. This is usually a better way of working than trying to save your environment with all of your results and tables in.\nI have created a script including all the analyses from this tutorial, in TutorialScript.R. Open this and have a look around. Notice my comments to remind myself why I did things, this might be helpful when I next come to revise the analysis!\nSee:\n\nhttps://kdestasio.github.io/post/r_best_practices/\nhttps://r4ds.had.co.nz/workflow-projects.html\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\nhttps://rstats.wtf\n\nfor more information on using projects and scripts"
  },
  {
    "objectID": "day1.html#functions",
    "href": "day1.html#functions",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "4.1 Functions",
    "text": "4.1 Functions\nEverything in R is done by executing ‘functions’. When you typed 1+2 at the console above you were calling the + function, with 1 and 2 as its arguments, and the result was printed in the console window.\nSimple mathematical functions can be written using standard notation in this way (eg 1+2 or 3/4) but functions are more commonly called by their name, with their arguments in brackets, separated by commas. For example, to get say the logarithm (base 10) of 100, we would type\n\n### Try this directly in the console, and by running it from your new script.\nlog(x=100, base=10)\n\n[1] 2\n\n\nHere, log is the name of the function, with x and base its arguments. The result is the value of the function (the value is what is returned).\n\nTry each of the following commands. Do you understand what they do any why?\n\n\n### From now on, keep everything you try in a script file.\n\nlog(x=100, base=10)\nlog(x=100)\nlog(base=10, x=100)\n\nlog(100,10)\nlog(10,100)\n\n1000 |&gt; log()           #  Note the use of the pipe operator |&gt; as an alternative way to call a function. \n100 |&gt; log(base=10)\n\nlog()\nlog\n\n\n4.1.1 Getting help\nIn the console, type:\n\n?log\n\nto read the help file for the log function. All R help files are structured in this way. Look at the Usage, Arguments and Value sections. These will be invaluable when you come to use R and use functions that you do not already know."
  },
  {
    "objectID": "day1.html#objects",
    "href": "day1.html#objects",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "4.2 Objects",
    "text": "4.2 Objects\nInstead of directly displaying the value of the function (‘value’ is what R calls the result of a function), you can give it a name and store it for later use:\n\nx = 1+2\n\nor\n\nx &lt;- 1+2\n\nThis does exactly the same thing; some R users use the arrow &lt;- instead of = for assignment, so both forms will come up when you’re looking at help files or other people’s code. I (annoyingly!) tend to use either interchangeably.\nNow you have an object called x in your environment that holds the number 3 (check your environment window). You can ask R to display the value of ‘x’ by just entering x (just entering the name of an object prints that object):\n\nx\n\nOr you could do something else with x\n\nx*2\nlog(x)\nx |&gt; log()\nx |&gt; log(10)\n\nWhat does this do?\n\nx |&gt; log() |&gt; sqrt()\n\nHint: it might help to understand if you read the pipe operator |&gt; as “and then…”\nTo see the class of an object (what kind of thing is stored in the object), use the class() function.\n\nclass(x)\n\nObjects of different classes store different kinds of information. We will come across objects of different classes later."
  },
  {
    "objectID": "day1.html#test-yourself",
    "href": "day1.html#test-yourself",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "4.3 Test yourself",
    "text": "4.3 Test yourself\n\nMake a new object called y which has the value of x+3. Then display y.\nNow change the value of x (eg using x &lt;- 6 ). Does the value of y change?\nObjects can hold text strings instead of numbers. Try:\n\n\nmyname &lt;- \"George\"  #(or whatever your name is).\nmyname\n\nWhat is the class of the ‘myname’ object?\n\n(Difficult!) Look up the function to turn a text string into upper case (an internet search will help you). Use this function to make a new object which has the same text as ‘myname’ but in upper case."
  },
  {
    "objectID": "day1.html#explore-the-structure-of-the-dataset",
    "href": "day1.html#explore-the-structure-of-the-dataset",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.1 Explore the structure of the dataset",
    "text": "5.1 Explore the structure of the dataset\nStart a new script to begin your analysis of the ‘iris’ dataset.\nType:\n\ndata(iris)\n\nCheck your ‘environment’ tab for a new object. Click on the object name to see it in the Viewer, or click on the small blue button next to it to expand the view in the environment window.\nWhat is the class of the iris object that you have created?\nWhat is in this data? What kind of descriptive statistics might we calculate to learn about this dataset?\nThere are a few different functions you can use to explore a dataset. Test each to see what they do.\n\nstr(iris)\nhead(iris)\nsummary(iris)\ndim(iris)\nnrow(iris)\nView(iris)\n\nThe iris data is an example of a dataset in tidy form. We’ll learn more about this in the next session, but in short in has:\n\nA single rectangular table to represent the data\nA column for each variable\nA row for each observation\n\nThis is very similar to how datasets are stored in software like SPSS or Stata, or in a database, but is different to how you might record data using GraphPad Prism."
  },
  {
    "objectID": "day1.html#use-the-data-in-calculations",
    "href": "day1.html#use-the-data-in-calculations",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.2 Use the data in calculations",
    "text": "5.2 Use the data in calculations\nWe can extract individual variables (characteristics) and individual elements like this:\n\niris$Sepal.Length \n\n## What do these lines do?\n\niris$Sepal.Length[1]\n\niris$Sepal.Length[c(2,4,6)]\n\niris$Sepal.Length[10:20]\n\n## What do these lines do?\n\nc(2,4,6)\n\n10:20\n\n## What does this do?\n\nplot(iris, col=iris$Species)\n\n\n\n\n\n\n\nNote\n\n\n\niris$Sepal.length is a ‘vector’. A vector is a list of items all of the same type. Here its a vector of numbers. A data frame is a collection of vectors.\nWhat are the types of each of the other vectors in the iris data frame?\n\n\nWe can use functions to get descriptive statistics for a vector:\n\nmean(iris$Sepal.Length)\n\niris$Sepal.Length |&gt; mean()\n\nsd(iris$Sepal.Length)\n\niris$Sepal.Width |&gt; hist()\n\nCan you make a histogram with red bars?\nWe can work on more than one variable at a time:\n\ncor(iris$Sepal.Length, iris$Sepal.Width)\n\ncor.test(iris$Sepal.Length, iris$Sepal.Width)\n\n\nwith(iris , cor(Sepal.Length , Sepal.Width))"
  },
  {
    "objectID": "day1.html#test-yourself---more-exploration-of-the-iris-dataset",
    "href": "day1.html#test-yourself---more-exploration-of-the-iris-dataset",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.3 Test yourself - More exploration of the iris dataset",
    "text": "5.3 Test yourself - More exploration of the iris dataset\n\nHow many rows does the iris dataset have?\nWhat is the class of each of its columns?\nWhat is the mean sepal length of the flowers\nWhat is the smallest (minimum) sepal width\n(Difficult) How many viriginca flowers are included in the dataset? Can you tabulate the species variable?\nCan you make a cow say the median petal length?"
  },
  {
    "objectID": "day1.html#other-packages-for-descriptive-statistics-if-you-have-time",
    "href": "day1.html#other-packages-for-descriptive-statistics-if-you-have-time",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.4 Other packages for descriptive statistics (if you have time)",
    "text": "5.4 Other packages for descriptive statistics (if you have time)\n(If we have time) Base R does not have good functions for making descriptive tables and graphs. But there are many other packages that have been developed to help with this. In particular the gt system is very good for making tables, and the GGally package has a nice function for making pairs plots. You could install these packages and try them out:\n\ninstall.packages(\"gtsummary\")\ngtsummary::tbl_summary(iris)\n\nCan you stratify the table by species? (Check the help)\n\ninstall.packages(\"GGally\")\nGGally::ggpairs(iris)\n\nCompare this to the output from the ‘base’ function pairs(iris) or plot(iris)."
  },
  {
    "objectID": "day1.html#using-dplyr-functions-to-manipulate-and-summarise-data",
    "href": "day1.html#using-dplyr-functions-to-manipulate-and-summarise-data",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "6.1 Using dplyr functions to manipulate and summarise data",
    "text": "6.1 Using dplyr functions to manipulate and summarise data\nThe five main dplyr functions are select, filter, mutate, group_by, and summarise.\nLet’s learn by example what each of these functions does:\n\niris |&gt; select(Sepal.Length, Sepal.Width)\n\niris |&gt; filter(Species==\"setosa\")\n\niris |&gt; filter(Sepal.Length &gt; 7.5)\n\niris |&gt; arrange(Sepal.Length)\n\niris |&gt; summarise(`Median Petal Width` = median(Petal.Width))\n\niris |&gt; summarise(`Mean` = median(Petal.Width), `SD` = sd(Petal.Width))\n\niris |&gt; mutate(Sepal.Area = Sepal.Length * Sepal.Width)\n\niris2 &lt;- iris |&gt; mutate(logPetalLength = log(Petal.Length))\n\niris2 &lt;- iris |&gt; mutate(PetalLengthGrouped = ntile(Petal.Length) )\n\nSo with dplyr we can take subsets of our data or our samples, split our samples up into groups, create new derived variables and make summary statistics.\nUsing pipes you can chain functions together. Suppose you wanted the mean sepal length of setosa flowers:\n\niris |&gt; filter(Species==\"setosa\") |&gt; summarise(mean(Sepal.Length))\n\ngroup_by and summarise are often used together, to make summaries over groups.\n\niris |&gt; group_by(Species) |&gt; summarise(mean(Sepal.Length), mean(Sepal.Width), n=n())"
  },
  {
    "objectID": "day1.html#test-yourself-data-wrangling-and-summarising",
    "href": "day1.html#test-yourself-data-wrangling-and-summarising",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "6.2 Test yourself: data wrangling and summarising",
    "text": "6.2 Test yourself: data wrangling and summarising\nCan you use dplyr to find the mean of the sepal length for flowers with a petal length less than 2?"
  },
  {
    "objectID": "day1.html#test-yourself-1",
    "href": "day1.html#test-yourself-1",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "8.1 Test yourself",
    "text": "8.1 Test yourself\n\nCan you estimate the linear model and display the summary in one line using pipes?\nHow much of the output summary do you understand?\n\n\n# Base R methods:\nconfint(model1)\n\n                      2.5 %   97.5 %\n(Intercept)       4.8621258 5.149874\nSpeciesversicolor 0.7265312 1.133469\nSpeciesvirginica  1.3785312 1.785469\n\n# Tidyverse method:\nbroom::tidy(model1, conf.int=TRUE)\n\n# A tibble: 3 × 7\n  term              estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)          5.01     0.0728     68.8  1.13e-113    4.86       5.15\n2 Speciesversicolor    0.930    0.103       9.03 8.77e- 16    0.727      1.13\n3 Speciesvirginica     1.58     0.103      15.4  2.21e- 32    1.38       1.79\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote - you can run t-tests like this, since a t-test is equivalent to a linear model with two group categorical predictor.\n\n\nAll pairwise comparisons using emmeans or t-tests for individual pairs with filter then lm?\n\n8.1.1 More complex models\nNow. suppose we want to fit a more complex model, whereby sepal length depends on petal length, with the relationship allowed to vary by species.\n\n# First make a model with Sepal.Length depending on Species and Petal.Length\nmodel2 &lt;- lm( Sepal.Length ~ Species + Petal.Length, data=iris)\nsummary(model2)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species + Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75310 -0.23142 -0.00081  0.23085  1.03100 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.68353    0.10610  34.719  &lt; 2e-16 ***\nSpeciesversicolor -1.60097    0.19347  -8.275 7.37e-14 ***\nSpeciesvirginica  -2.11767    0.27346  -7.744 1.48e-12 ***\nPetal.Length       0.90456    0.06479  13.962  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.338 on 146 degrees of freedom\nMultiple R-squared:  0.8367,    Adjusted R-squared:  0.8334 \nF-statistic: 249.4 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n# Now add the interaction term.  These two models are the same.\nmodel3 &lt;- lm( Sepal.Length ~ Species + Petal.Length + Species:Petal.Length, data=iris)\nmodel3 &lt;- lm( Sepal.Length ~ Species * Petal.Length, data=iris)\nsummary(model3)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species * Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73479 -0.22785 -0.03132  0.24375  0.93608 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      4.2132     0.4074  10.341  &lt; 2e-16 ***\nSpeciesversicolor               -1.8056     0.5984  -3.017  0.00302 ** \nSpeciesvirginica                -3.1535     0.6341  -4.973 1.85e-06 ***\nPetal.Length                     0.5423     0.2768   1.959  0.05200 .  \nSpeciesversicolor:Petal.Length   0.2860     0.2951   0.969  0.33405    \nSpeciesvirginica:Petal.Length    0.4534     0.2901   1.563  0.12029    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3365 on 144 degrees of freedom\nMultiple R-squared:  0.8405,    Adjusted R-squared:  0.8349 \nF-statistic: 151.7 on 5 and 144 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use augment from the broom package to add predicted values to our dataset, with confidence intervals.\n\n# look carefully at the output from here:\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.2.3\n\nmodel3 |&gt; augment(interval=\"confidence\")\n\n# A tibble: 150 × 11\n   Sepal.Length Species Petal.Length .fitted .lower .upper  .resid   .hat .sigma\n          &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1          5.1 setosa           1.4    4.97   4.87   5.07  0.128  0.0226  0.337\n 2          4.9 setosa           1.4    4.97   4.87   5.07 -0.0724 0.0226  0.338\n 3          4.7 setosa           1.3    4.92   4.79   5.05 -0.218  0.0378  0.337\n 4          4.6 setosa           1.5    5.03   4.93   5.12 -0.427  0.0210  0.336\n 5          5   setosa           1.4    4.97   4.87   5.07  0.0276 0.0226  0.338\n 6          5.4 setosa           1.7    5.14   4.97   5.30  0.265  0.0583  0.337\n 7          4.6 setosa           1.4    4.97   4.87   5.07 -0.372  0.0226  0.336\n 8          5   setosa           1.5    5.03   4.93   5.12 -0.0266 0.0210  0.338\n 9          4.4 setosa           1.4    4.97   4.87   5.07 -0.572  0.0226  0.334\n10          4.9 setosa           1.5    5.03   4.93   5.12 -0.127  0.0210  0.337\n# ℹ 140 more rows\n# ℹ 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\nFinally we can use ggplot to show these predicted values and confidence intervals. Take care to understand how the output from each function (ie lm, augment) is piped into the next.\n\nlm( Sepal.Length ~ Species * Petal.Length, data=iris) |&gt;\n  augment(interval=\"confidence\") |&gt; \n  ggplot() + \n    aes(x=Petal.Length, y=Sepal.Length, color=Species) + \n    geom_ribbon(aes(y=.fitted,ymin=.lower, ymax=.upper), alpha=0.1) + \n    geom_point() + \n    geom_line(aes(y=.fitted),lwd=1) + \n    theme_bw()\n\n\n\n# Try changing the forumula to `Sepal.Length ~ Species + Petal.Length`\n# Which model has the better fit? (you can use anova(model2, model3) to compare models)"
  },
  {
    "objectID": "day1.html#character-strings",
    "href": "day1.html#character-strings",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.1 Character strings",
    "text": "9.1 Character strings\nCharacter strings represent text rather than numbers. Strings are used to label categories in a dataset, to identify columns in a dataset, to make your outputs more readable. You also might find that part of your data has been entered as a string, for example patient identifiers or gene names in a database, or responses to open ended questions.\nStrings are identified in R (and in most other programming languages) by enclosing them in quotes. Single quotes and double quotes can be used (and are treated almost identically), but double quotes are preferred. For example try:\n\nprint(\"Hello\")\n\nprint('Hello')\n\n# What happens here?\nprint(Hello)\n\nA common mistake in R is to forget to enclose strings in quotes. In which case R tries to interpret your input as an object name, leading to an error message if that name doesn’t exist."
  },
  {
    "objectID": "day1.html#logicals",
    "href": "day1.html#logicals",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.2 Logicals",
    "text": "9.2 Logicals\nLogicals represent binary information in the form TRUE or FALSE. They most often arise as the result of a comparison, for example try:\n\n3&gt;2\n\n\"Hello\" == \"hello\"  # note the double equals sign, this distinguishes assignment from comparison"
  },
  {
    "objectID": "day1.html#converting-between-types",
    "href": "day1.html#converting-between-types",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.3 Converting between types",
    "text": "9.3 Converting between types\nSometimes it is possible to convert an object from one class to another. For example, a number might be stored as a character string in your data, and you will need to convert it into a numeric before you can do any analysis with it. For example:\n\nx &lt;- \"3\"\nx*2 # What is the error message here?  What does it mean?\n\ny &lt;- as.numeric(x)\ny*2"
  },
  {
    "objectID": "day1.html#missing-elements-in-vectors",
    "href": "day1.html#missing-elements-in-vectors",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.4 Missing elements in vectors",
    "text": "9.4 Missing elements in vectors\nOften your data will include missing values. R uses NA to represent missing values. For example the following creates a vector (a single variable, like a single column of a data frame) with a missing value in the fourth position:\n\nweights &lt;- c(10,21,32,NA,14)\nweights\n\n\n\n\n\n\n\nWarning\n\n\n\nNote the difference between NA (a missing value) and \"NA\" (a character string containing the letters N and A. I have been tripped up by this a few times when \"NA\" has been entered into a dataset.)"
  },
  {
    "objectID": "day1.html#exercise-1.-effect-of-missing-values",
    "href": "day1.html#exercise-1.-effect-of-missing-values",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.5 Exercise 1. Effect of missing values",
    "text": "9.5 Exercise 1. Effect of missing values\nTry some other functions with myvector to see what impact the missing data point has.\n\nclass(weights)\n\nplot(weights)\n\nweights&gt;20\n\nmean(weights) # what happens here?  Why?  Can you fix it?\n\nis.na(weights) # what does this do?\n\nsum(is.na(weights)) # can you explain what this does?"
  },
  {
    "objectID": "day1.html#a-continuous-variable-stratified-by-a-grouping-variable-and-the-formula-interface",
    "href": "day1.html#a-continuous-variable-stratified-by-a-grouping-variable-and-the-formula-interface",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "10.1 A continuous variable stratified by a grouping variable, and the formula interface",
    "text": "10.1 A continuous variable stratified by a grouping variable, and the formula interface\nWe already saw a way to get means over a categorical variable with Tidyverse:\n\nlibrary(tidyverse)\niris |&gt; group_by(Species) |&gt; summarise(mean(Sepal.Length))\n\nIf we want more than one summary statistic for example, it’s easy with tidyverse (this is an example of a task that is difficult with base R):\n\niris |&gt; group_by(Species) |&gt; summarise(n(),mean(Sepal.Length),sd(Sepal.Length))\n\nWhile dplyr is good for making summary statistics, but to make publication ready tables you can explore the gtsummary package.\nhttps://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.2.3\n\niris |&gt; tbl_summary( by=Species )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      setosa, N = 501\n      versicolor, N = 501\n      virginica, N = 501\n    \n  \n  \n    Sepal.Length\n5.00 (4.80, 5.20)\n5.90 (5.60, 6.30)\n6.50 (6.23, 6.90)\n    Sepal.Width\n3.40 (3.20, 3.68)\n2.80 (2.53, 3.00)\n3.00 (2.80, 3.18)\n    Petal.Length\n1.50 (1.40, 1.58)\n4.35 (4.00, 4.60)\n5.55 (5.10, 5.88)\n    Petal.Width\n0.20 (0.20, 0.30)\n1.30 (1.20, 1.50)\n2.00 (1.80, 2.30)\n  \n  \n  \n    \n      1 Median (IQR)"
  },
  {
    "objectID": "day1.html#recoding-a-variable-into-groups",
    "href": "day1.html#recoding-a-variable-into-groups",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "10.2 Recoding a variable into groups",
    "text": "10.2 Recoding a variable into groups\nSuppose we want to classify flowers into three groups based on their petal length. We need to add another categorical variable to the dataset.\nWhat class should that new variable be?\nBase R has the function cut that divides continuous variables into groups. Tidyverse has a few extensions of this (weirdly in the ggplot2 package), including cut_number that can divide up a continuous variable into three equal groups, and allow us to add labels to them.\n\ncut_number(iris$Petal.Length, 3, labels=c(\"Short\", \"Medium\", \"Long\") )\n\n\n# Using tidyverse to create this variable and add it to the dataset:\nlibrary(ggplot2)\n\niris &lt;- iris |&gt; mutate(PetalLengthGrouped = cut_number(Petal.Length, 3, labels=c(\"Short\", \"Medium\", \"Long\") ))"
  },
  {
    "objectID": "day1.html#tabulating-a-categorical-variable-over-groups",
    "href": "day1.html#tabulating-a-categorical-variable-over-groups",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "10.3 Tabulating a categorical variable over groups",
    "text": "10.3 Tabulating a categorical variable over groups\nFor a categorical variable, a summary of frequency counts might be the most appropriate descriptive statistic. We can get this with the table() function. Suppose we wanted to know the distribution of species in our dataset.\n\ntable(iris$Species)\n\nThe ‘table’ function can also generate cross-tabs, by specifying two or three variables.\n\ntable(iris$Species, iris$PetalLengthGrouped) # Base R version\n\niris |&gt; janitor::tabyl(Species, PetalLengthGrouped)  #  The tidyverse version using `tabyl` from the `janitor` package.\niris |&gt; group_by(Species, PetalLengthGrouped) |&gt; tally()\n\nTables of counts are useful, but it might be more helpful to see the proportion of healthy trees by species. To get this we can pass the table we just made into the prop.table() function:\n\ntable1 &lt;- table(iris$Species, iris$PetalLengthGrouped)\nprop.table(table1)\n\n            \n                  Short     Medium       Long\n  setosa     0.33333333 0.00000000 0.00000000\n  versicolor 0.00000000 0.32000000 0.01333333\n  virginica  0.00000000 0.04000000 0.29333333\n\n\nThis is where pipes might be more intuitive:\n\ntable(iris$Species, iris$PetalLengthGrouped) |&gt; prop.table()\n\nThe table above has calculated the ‘cell proportions’. If we want the row percentages we need to set the margin option in prop.table() appropriately. margin=1 is the rows, margin=2 is the columns. If you left out margins altogether, you’d get the cell proportions.\nWe could round this (or any numeric) to 2 d.p. by passing the resulting proportion table into the round() function. and using the function multiply_by from the magrittr package (installed as part of tidyverse) we can turn our proportions into percentages:\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\ntable(iris$Species, iris$PetalLengthGrouped) |&gt; \n  prop.table(margin=1) |&gt; \n  multiply_by(100) |&gt; \n  round(digits=2)\n\n            \n             Short Medium Long\n  setosa       100      0    0\n  versicolor     0     96    4\n  virginica      0     12   88\n\n\nYou might decide that using a function like gtsummary::tbl_summary() is easier!"
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "Intro to R for statistics, Day 2",
    "section": "",
    "text": "Today’s worksheet introduces you to a real dataset.\nThe tasks you will cover are:"
  },
  {
    "objectID": "day2.html#the-dataset",
    "href": "day2.html#the-dataset",
    "title": "Intro to R for statistics, Day 2",
    "section": "The dataset",
    "text": "The dataset\nThe data are from a randomised clinical trial of a new rehabilition intervention (compared to standard post-stroke care) aimed at improving the walking speed of hospital patients. Better walking speed is a good indicator of general stroke recovery.\nWe have recorded:\n\nThe age and sex of each participant,\nThe treatment allocation,\nThe hospital department from which they were recruited and\nTime they take to complete a walking task.\n\nOur research questions are:\n\nDoes the treatment improve walking speed compared to controls?\nBy how much does it improve, and how certain are we of this?\n\nThe dataset can be found at walkingspeed_day2.xlsx\n\nPowerpoint slides to support this material are at:\n\nday3.pptx\nday3_data_lm.pptx"
  },
  {
    "objectID": "day2.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "href": "day2.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "title": "Intro to R for statistics, Day 2",
    "section": "Make sure the data looks OK and is in the right place on your computer",
    "text": "Make sure the data looks OK and is in the right place on your computer\nBefore we dive in and import it, we need to make sure our data is in a sensible place.\n\nOpen RStudio and switch to the project you created in day 1, or start a new project if necessary.\nThen, save the example data walkingspeed_day2.xlsx for this tutorial into your project folder. Check that it has appeared in the ‘files’ pane in RStudio.\nStart a new script file that will eventually include all of the commands we need to import, clean, visualise and analyse the data.\nNow open the dataset in Excel and explore the file so that you understand what is there."
  },
  {
    "objectID": "day2.html#read-the-help",
    "href": "day2.html#read-the-help",
    "title": "Intro to R for statistics, Day 2",
    "section": "Read the help!",
    "text": "Read the help!\nWe are nearly ready to import our data. But before using a new function its always good to read its documentation.\nR and R packages are not as self-explanatory as other software, and so you should expect to spend a fair amount of time, particularly as you are learning R, reading documentation, vignettes, blogs, etc on what R can do, which packages exist, and how to use them.\nThe read_excel() function has a few different options so first we should look at the help file:\n\n?read_excel # where does the helpfile appear?\n\nMake sure to check:\n\nDescription what does the function do,\nUsage what is the syntax\nArguments detail of what all the options mean\nValue what do I get when I run this\nExamples (usually very helpful)\n\nNote from the help file that read_excel() can extract data from different sheets and ranges of an Excel workbook, can use or ignore column names, and allows you to specify the type of data (numeric, dates, text etc) if you want to, or leave it to R to guess.\nMany R packages also have vignettes or websites including simpler guides to their use in specific cases. readxl has a website that you might find helpful: https://readxl.tidyverse.org/\nNow we’ll load the data. We want to use the ‘walking speed’ data from the walkingspeed.xlsx spreadsheet.\n\nOpen the spreadsheet in Excel and find this sheet. The data we want is in the sheet called ‘day2’.\n\nFrom the read_excel() help file we can deduce the syntax to load this data into R:\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.2.2\n\nwalkingdat &lt;- read_excel(path=\"walkingspeed_day2.xlsx\", sheet=\"day2\")\n\nThis line assumes that the file ‘walkingspeed_day2.xlsx’ is in the current working directory (you can check what this is with getwd(). The current working directory is shown just above the R console window. You can see the files in the current working directory in the ‘Files’ tab on the bottom right of the RStudio window. When you create or load a project RStudio will set the working directory to the root of the project directory.\nThis line calls the read_excel() function, with the arguments ‘path’, ‘sheet’ set. The other arguments will be set to their default values, which you can see from the help file.\nWe could have set the range of the data in the spreadsheet (I usually do this for safety), but read_excel() can figure it out automatically most of the time; by default it picks the biggest continuous chunk of data starting in the top left of the sheet.\nNow you should have a ‘data frame’ object called walkingdat in your environment, which includes the data from the Excel sheet ready to process and analyse.\nOur workflow now is:\n\nClean and code\nVisualise\nDescribe\nModel\nDiagnose model\nInterpret"
  },
  {
    "objectID": "day2.html#dealing-with-outliers",
    "href": "day2.html#dealing-with-outliers",
    "title": "Intro to R for statistics, Day 2",
    "section": "Dealing with outliers",
    "text": "Dealing with outliers\nIt looks like there are some unreasonably high and low values of walking time.\nWe can make another graph of walking speed against age, this time on a logarithmic scale so both the extreme high and extreme low points are visible, to see what is going on.\n\n## ggplot2 version\nlibrary(ggplot2)\nggplot(walkingdat) + aes(x=age, y=time) + geom_point() + scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n## base R version\nplot(walkingdat$age, walkingdat$time, log=\"y\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): 1 y value &lt;= 0 omitted from\nlogarithmic plot\n\n\n\n\n\nIt seems there are some values for time that are likely to be technical errors. We can remove these values (set them to missing) in a few different ways:\n\n## base R method\nwalkingdat$time[walkingdat$time&lt;0.1] &lt;- NA\nwalkingdat$time[walkingdat$time&gt;100] &lt;- NA\n\n## Using mutate and ifelse\nwalkingdat &lt;- walkingdat |&gt; mutate(time = ifelse(time&lt;0.1,NA,time))\nwalkingdat &lt;- walkingdat |&gt; mutate(time = ifelse(time&gt;100,NA,time))\n\n## the 'pure' tidyverse way with case_when is a bit clunky.\n## look up 'case_when()' to understand this line\nwalkingdat &lt;- walkingdat |&gt; mutate( time = case_when(time&lt;0.1 ~ NA_real_ , \n                                                     time&gt;100 ~ NA_real_, \n                                                     TRUE ~ time) )\n\n# Now check the distribution of time again.\nhist(walkingdat$time, breaks=100)\n\n\n\nrange(walkingdat$time, na.rm = TRUE)\n\n[1]  1.435 17.641\n\n\nNow we have a cleaned dataset in our environment we can proceed with our visualisation and analysis:"
  },
  {
    "objectID": "day2MSc.html",
    "href": "day2MSc.html",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "",
    "text": "Today’s worksheet introduces you to a real dataset.\nThe tasks you will cover are:"
  },
  {
    "objectID": "day2MSc.html#the-dataset",
    "href": "day2MSc.html#the-dataset",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "The dataset",
    "text": "The dataset\nThe data are from a randomised clinical trial of a new rehabilition intervention (compared to standard post-stroke care) aimed at improving the walking speed of hospital patients. Better walking speed is a good indicator of general stroke recovery.\nWe have recorded:\n\nThe age and sex of each participant,\nThe treatment allocation,\nThe hospital department from which they were recruited and\nTime they take to complete a walking task.\n\nOur research questions are:\n\nDoes the treatment improve walking speed compared to controls?\nBy how much does it improve, and how certain are we of this?\n\nThe dataset can be found at walkingspeed_day2.xlsx\n\nPowerpoint slides to support this material are at:\n\nday3.pptx\nOur workflow is:\n\nClean and code\nVisualise\nDescribe\nModel\nDiagnose model\nInterpret"
  },
  {
    "objectID": "day2MSc.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "href": "day2MSc.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Make sure the data looks OK and is in the right place on your computer",
    "text": "Make sure the data looks OK and is in the right place on your computer\nBefore we dive in and import it, we need to make sure our data is in a sensible place.\n\nOpen RStudio and open the project you created in day 1 (if not already opened).\nThen, save the example data walkingspeed_msc.xlsx for this tutorial into your project folder. Check that it has appeared in the ‘files’ pane in RStudio.\nStart a new script file that will eventually include all of the commands we need to import, clean, visualise and analyse the data.\nNow open the dataset in Excel and explore the file so that you understand what is there."
  },
  {
    "objectID": "day2MSc.html#read-the-help",
    "href": "day2MSc.html#read-the-help",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Read the help!",
    "text": "Read the help!\nWe are nearly ready to import our data. But remember that before using a new function its always good to read its documentation. The read_excel() function has a few different options so first we should look at the help file:\n\n?read_excel\n\nNote from the help file that read_excel() can extract data from different sheets and ranges of an Excel workbook, can use or ignore column names, and allows you to specify the type of data (numeric, dates, text etc) if you want to, or leave it to R to guess.\nMany R packages also have vignettes or websites including simpler guides to their use in specific cases. readxl has a website that you might find helpful: https://readxl.tidyverse.org/\nNow we’ll load the data. We want to use the ‘walking speed’ data from the walkingspeed_msc.xlsx spreadsheet.\n\nOpen the spreadsheet in Excel and find this sheet. The data we want is in the sheet called ‘day2’.\n\nFrom the read_excel() help file we can deduce the syntax to load this data into R:\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.2.2\n\nwalkingdat &lt;- read_excel(path=\"walkingspeed_msc.xlsx\", sheet=\"day2\")\n\nThis line assumes that the file walkingspeed_msc.xlsx is in the current working directory (you can check what this is with getwd()). The current working directory is shown just above the R console window. You can see the files in the current working directory in the ‘Files’ tab on the bottom right of the RStudio window. When you create or load a project RStudio will set the working directory to the root of the project directory.\nThis line calls the read_excel() function, with the arguments path, sheet set. The other arguments will be set to their default values, which you can see from the help file.\nWe could have set the range of the data in the spreadsheet (I usually do this for safety), but read_excel() can figure it out automatically most of the time; by default it picks the biggest continuous chunk of data starting in the top left of the sheet.\nNow you should have a ‘data frame’ object called walkingdat in your environment, which includes the data from the Excel sheet ready to process and analyse."
  },
  {
    "objectID": "day2MSc.html#dealing-with-outliers",
    "href": "day2MSc.html#dealing-with-outliers",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Dealing with outliers",
    "text": "Dealing with outliers\nIt looks like there are some unreasonably high and low values of walking time.\nWe can make another graph of walking speed against age, this time on a logarithmic scale so both the extreme high and extreme low points are visible, to see what is going on.\n\n## base R version\nplot(walkingdat$age, walkingdat$time, log=\"y\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): 1 y value &lt;= 0 omitted from\nlogarithmic plot\n\n\n\n\n## ggplot2 version\nlibrary(ggplot2)\nggplot(data = walkingdat) + aes(x=age, y=time) + geom_point() + scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIt seems there are some values for time that are likely to be technical errors. We can remove these values (set them to missing) in a few different ways:\n\n## base R method to replace the values with NA ('missing')\nwalkingdat$time[walkingdat$time&lt;0.1] &lt;- NA\nwalkingdat$time[walkingdat$time&gt;100] &lt;- NA\n\n## Using filter we could remove those datapoints\nwalkingdat &lt;- walkingdat |&gt; filter(time &lt; 100) |&gt; filter(time &gt; 0.1)\n\n\n# Now check the distribution of time again.\nhist(walkingdat$time, breaks=100)\n\n\n\nrange(walkingdat$time, na.rm = TRUE)\n\n[1]  1.435 17.641\n\n\nNow we have a cleaned dataset in our environment we can proceed with our visualisation and analysis:"
  },
  {
    "objectID": "day2MSc.html#graphics",
    "href": "day2MSc.html#graphics",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Graphics",
    "text": "Graphics\n\nGraphing\n\nConsider what kind of graph you might make to illustrate the difference in walking speed between treatment groups, and try to make it using ggplot. Eg:\nMake a box plot of time by sex, with a different coloured box per sex\nLabel the axes appropriately\nTry a violin plot instead of a boxplot (with geom_violin). Which do you prefer?\nMake any other adjustments you think are informative!"
  },
  {
    "objectID": "day2MSc.html#modelling",
    "href": "day2MSc.html#modelling",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Modelling",
    "text": "Modelling\nI have left these sections in from the PhD workshop. Attempt them if you get time.\n\nHypothesis Testing\n\nWhat is the difference in average walking speed between control patients and treated patients?\nIs this difference statistically significant? Use t-test t.test() and Mann-Whitney tests wilcox.test() to examine the differences between the groups. Look up the syntax for t.test() and wilcox.test() using the help system, and use them to check whether the time taken to complete the task varies by treatment status. In each case use the version of the function that takes a formula agument. Remember the formula syntax from the boxplot and lm functions in day 1.\nCompare the p-values for each method.\nConsider the assumptions for a t-test, and whether they are met in this dataset.\n\nDo you believe that treatment affects walking speed?\n\nLinear model diagnostics\n\nEstimate a linear model (with lm) to estimate/test the effect of treatment on task completion time.\nUse summary to get the model coefficients and p-values\nUse broom::tidy to get the model coefficients and p-values\nCompare the model results to the equivalent t-test\nExtend the model to include the effects of age and sex as potential covariates.\nUse plot() to explore the model diagnostics\nUse performance::check_model() to explore the model diagnostics\nNow estimate a new model to compare the task completion speed between groups.\nGet the model coefficients and p-values\nCheck the model diagnostics using plot or performance::check_model\nWhy are the result from this model different? How do you interpret the results?\nCan you get the confidence intervals for the treatment effect (use the help for broom::tidy)\nUse model_summary package to make a pretty model summary table"
  },
  {
    "objectID": "day3.html",
    "href": "day3.html",
    "title": "Intro to R day 3 - Notes on Graphing",
    "section": "",
    "text": "The dataset to support today’s work is here: walkingspeed.xlsx:\nPowerpoint slides are here: day3.pptx\nAn explanation/breakdown of the code for a typical ggplot2 graph is here: flipbook.html\nPlotting data is an essential part of data analysis and reporting. Your plots communicate your results, and a good plot can be the difference between a successful and unsuccessful communication.\nIn this session we’ll think about how to plot your data, what makes a good vs a bad plot, and illustrate some concepts for plotting using R.\nLearning objectives:"
  },
  {
    "objectID": "day3.html#descriptive-plots",
    "href": "day3.html#descriptive-plots",
    "title": "Intro to R day 3 - Notes on Graphing",
    "section": "Descriptive plots",
    "text": "Descriptive plots\nBefore we model data we should visualise it. This crucial first step is often omitted in analyses and reports. Consider the follow example, that I have designed after a real analysis I worked on earlier this year, where an outcome was compared between two treatment conditions:\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nset.seed(12345) # Set seed is used to ensure that 'random' samples will come out the same each time.\nN=12\ny &lt;- c(runif(N), c(runif(N-2), 4.5,6))\nx &lt;- factor(rep(c(1,2), each=N))\nggplot(data.frame(Group=x,y), aes(x,y)) + \n  stat_summary(geom=\"col\",width=0.5, fill=\"red\") + \n  stat_summary(geom=\"errorbar\", width=0.2) + \n  theme_bw()+ stat_compare_means(label.y = 2)\n\n\n\n\nOn the face of it, it looks like the outcome is higher in group 2 than group 1. If this is all you see then this is surely the conclusion you would come to.\nBut why is the p-value so high? And how well do you feel like you understand the data from this graph?\nConsider now how you would interpret this:\n\nlibrary(ggplot2)\nggplot(data.frame(Group=x,y), aes(x,y)) + \n  geom_point() + \n  theme_bw()\n\n\n\n\nQuite differently? At least with this second visualisation we can see the data and draw our conclusions about what is going on directly.\nAlso consider, the first graph only shows you four values, two means and two standard errors (if that is what they are, I never actually told you). It’s a complete waste of ink. But how often do you see this first presentation in the scientific papers your read? When I see one of these I can’t help but wonder what horrors it is hiding.\nThe second graph tells us quite a lot. It tells us to maybe check our outlying data points, or to try transforming our data before analysis. We might even conclude that the two groups are more-or-less the same, except for two individuals, which may well be a real effect of treatment that is limited to specific individuals. In any case we learn a lot.\nSo our most important rule is: always plot your data, and not summaries of the data.\nIdeally, if we have a dataset with several variables, we will make something like a graph matrix showing every variable against every other variable. This will help us identify any potential problems or outliers in 2-dimensions.\nThe ggpairs function in the GGally package gives us a grid layout showing all of these combinations. I think its a reasonable summary.\n\nlibrary(readxl)\nlibrary(GGally)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\", sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\nggpairs(walkingdata, columns=1:6)\n\n\n\n\nNote I’m not wasting a lot of time making this graph pretty. It’s for me only, I don’t care what it looks like, I just want to see the data as quickly and as effectively as possible. I’ve also left patient ID in here as a variable, there’s no harm doing this, and it might show me if any data errors have occurred.\nDoes this graph meet all of the objectives that we set out for our ‘descriptive’ analyses above?"
  },
  {
    "objectID": "day3.html#inferential-graphs",
    "href": "day3.html#inferential-graphs",
    "title": "Intro to R day 3 - Notes on Graphing",
    "section": "Inferential graphs",
    "text": "Inferential graphs\nWhile the descriptive graphs tells us about the data, it doesn’t tell us much about the comparison we are interested in making.\nTo think about how we could graph that, first think about what exactly it is we are trying to show.\nLets go back to our linear model from last time:\n\nmodel1 &lt;- lm(data=walkingdata, log(time) ~ group + sex + age + factor(department))\nsummary(model1)\n\n\nCall:\nlm(formula = log(time) ~ group + sex + age + factor(department), \n    data = walkingdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66614 -0.17268 -0.02574  0.09920  1.76808 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)          0.180866   0.343877   0.526  0.59985   \ngrouptreat          -0.184903   0.059587  -3.103  0.00237 **\nsexM                 0.038621   0.072702   0.531  0.59620   \nage                  0.014986   0.006238   2.402  0.01777 * \nfactor(department)2  0.108484   0.088572   1.225  0.22295   \nfactor(department)3 -0.097133   0.084065  -1.155  0.25011   \nfactor(department)4 -0.057919   0.087749  -0.660  0.51044   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3378 on 125 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.1723,    Adjusted R-squared:  0.1325 \nF-statistic: 4.336 on 6 and 125 DF,  p-value: 0.0005211\n\n\nWhat is the key information here that we need to communicate?\nWe should say what our estimate of the treatment effect is, and how sure we are of this. That is the ‘actionable’ result from this work, and that is what we want people to take away from our analysis. The fact of the treatments being ‘significantly’ different is interesting but not enough on its own. So from our analysis we should be trying to communicate the estimate of treatment effect, the standard error of treatment effect, and potentially the p-value and a confidence interval for the difference.\nThe mean time in each group is perhaps interesting descriptively, so people can understand our sample. It’s hard to see why the standard error within each group should be of interest.\nSo - do I even need a graph? Should this summary statistic, mean difference = 0.19 (standard error=0.06; p=0.0015) be enough? Recall that this was calculated on a logarithmic scale, so it’s probably best to exponentiate it and report a ratio:\n\nlibrary(emmeans)\nem1 &lt;- emmeans(model1, \n               trt.vs.ctrl~group, \n               type=\"response\")$contrast\nconfint(em1)\n\n contrast        ratio     SE  df lower.CL upper.CL\n treat / control 0.831 0.0495 125    0.739    0.935\n\nResults are averaged over the levels of: sex, department \nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nSo we could say “times for treated group were 83% of the times for the control group (95% CI=74% to 93%)”. Or: “treatment improved times by 17% (95% CI=7-26%).”\nIs this enough? I think so, if combined with a visual summary that persuades us that the model is reasonable, and we have the descriptive graph that shows us this difference in the context of the variance in the data.\nCompare this with the traditional presentation:\n\nlibrary(ggpubr) # includes the stat_compare_means function\nggplot(walkingdata, aes(x=group, y=time)) + \n  stat_summary(geom=\"col\", width=0.5, fill=\"red\") + \n  stat_summary(geom=\"errorbar\", width=.2) + \n  scale_x_discrete(na.translate=FALSE) + \n  theme_bw() + \n  stat_compare_means()\n\n\n\n\nor better, but still not so informative:\n\nlibrary(ggpubr)\nlibrary(ggbeeswarm)\nggplot(remove_missing(walkingdata), aes(x=group, y=time)) + \n  geom_beeswarm(col=\"grey\") + \n  stat_summary(geom=\"errorbar\", width=.2) + \n  stat_summary(geom=\"point\", width=.2) + \n  theme_bw()+ scale_y_log10() + \n  stat_compare_means(method=\"t.test\",\n                     label = \"p.signif\",\n                     comparisons=list(c(\"treated\", \"control\")))\n\n\n\n\nCould you say what the treatment effect is by looking at this graph? How sure would you be about it? Also, how would you represent a model other than a simple comparison of means (for example, the multiple linear regression model that we estimated). By confusing the descriptive with the inferential graph we are limiting our ability to conduct the appropriate statistical analysis.\nSuppose we wanted to compare several groups, say time over department, then a visualisation might be useful:\n\nlibrary(emmeans)\nlibrary(broom)\nem2 &lt;- emmeans(model1, \n               trt.vs.ctrl~department, \n               type=\"response\")$contrast\ntd &lt;- tidy(em2, conf.int = TRUE)\ntd\n\n# A tibble: 3 × 11\n  term       contrast  null.value ratio std.error    df conf.low conf.high  null\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 department departme…          0 1.11     0.0987   125    0.902      1.38     1\n2 department departme…          0 0.907    0.0763   125    0.742      1.11     1\n3 department departme…          0 0.944    0.0828   125    0.765      1.16     1\n# ℹ 2 more variables: statistic &lt;dbl&gt;, adj.p.value &lt;dbl&gt;\n\nggplot(td) + \n  aes(x=contrast, y=ratio, ymax=conf.high, ymin=conf.low) + \n  geom_pointrange() + \n  coord_flip() + \n  geom_hline(yintercept=1) + theme_bw()"
  },
  {
    "objectID": "dichot.html",
    "href": "dichot.html",
    "title": "Why is ‘grouping’ your samples bad for analysis?",
    "section": "",
    "text": "Summary\nIf we have continuous data then we should keep it continuous in analysis. Grouping samples into (say) ‘high’ vs ‘low’ or ‘recovered’ vs ‘not recovered’ throws away information and makes it more difficult to detect associations. The power of your study is reduced and the sample size needed goes up.\n\n\nMotivation\nSuppose we are interested in the effect of an exposure on an outcome, and we have measured both in a sample. Both our exposure and our outcome are measured as continuous variables, for example we might be interested in the effect of fibre intake on gut microbial diversity.\nThere are a couple of possible approaches to the analysis. First, we could estimate or test for a correlation between the exposure and the outcome.\nAlternatively we could dichotomise the exposure, splitting the samples into a “high fibre” and a “low fibre” group, before comparing the microbial diversity in each group. Or we could dichotomise the outcome into “high diversity” and “low diversity”.\nWhich should we choose? Intuitively we might prefer the dichotomised version for “ease of interpretation”, but how does it affect our ability to detect any association?\n\n\nPackages\nWe’ll need the following R packages for this simulation:\n\nlibrary(pwr)\nlibrary(rmarkdown)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(MASS)\nlibrary(data.table)\n\n\n\nDataset\nSuppose our exposure and outcome are both normally distributed with a correlation of 0.5.\nWe can sample 50 points as follows:\n\nset.seed(21)\ndat = data.frame(mvrnorm(n=30, mu=c(10,10), Sigma = matrix(c(1,0.5,0.5,1),nrow=2)))\nnames(dat) &lt;- c(\"Exposure\", \"Outcome\")\n\nggplot(dat, aes(x=Exposure, y=Outcome)) + geom_point() + theme_bw()\n\n\n\n\nHow can we estimate or test this correlation by statistical analysis?\nThe simplest way would be to test\n\ncor.test(dat$Exposure, dat$Outcome)\n\n\n    Pearson's product-moment correlation\n\ndata:  dat$Exposure and dat$Outcome\nt = 3.0861, df = 28, p-value = 0.004535\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1753575 0.7313294\nsample estimates:\n      cor \n0.5037987 \n\n\nSo in our sample this correlation is clearly detectable (estimated r=0.50, p=0.0045).\n\n\nDichotomising variables\nDichotomising means splitting variables into two groups. In our example we might decide to compare those with a high vs a low exposure. Lets make a variable corresponding to whether the exposure for each participant is above observed median. This will split the data into two groups, and we can describe how the outcome differs between the high and low exposure groups:\n\ndat$HighExposure &lt;- dat$Exposure &gt; median(dat$Exposure)\n(ggplot(dat, aes(Exposure, Outcome, color=HighExposure)) + \n  geom_point() + theme_bw() + \n  scale_color_manual(values=c(\"black\", \"red\")))+\n(ggplot(dat, aes(HighExposure, Outcome)) + \n  geom_boxplot() + geom_point(aes(color=HighExposure)) + theme_bw()+ \n  scale_color_manual(values=c(\"black\", \"red\")))\n\n\n\n\nNow we can use (for example) a t-test to for difference in outcome between “high” and “low” exposure groups:\n\nt.test(data=dat, Outcome ~ HighExposure)\n\n\n    Welch Two Sample t-test\n\ndata:  Outcome by HighExposure\nt = -2.3855, df = 27.985, p-value = 0.02407\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -1.5826037 -0.1203041\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           9.723858           10.575312 \n\n\nThe difference is now barely detectable (p=0.024)!\nWe could go further, and dichotomise both the outcome and the exposure.\n\ndat$HighOutcome &lt;- dat$Outcome &gt; median(dat$Outcome)\n(ggplot(dat, aes(Exposure, Outcome, color=HighExposure, shape=HighOutcome)) + \n  geom_point() + theme_bw() + geom_hline(yintercept=median(dat$Outcome),lty=2)+\n  scale_color_manual(values=c(\"black\", \"red\")))+\n(ggplot(dat, aes(HighExposure, as.numeric(HighOutcome))) + \n  geom_bar(stat=\"summary\", fun.y=mean, col=\"black\", aes(fill=HighExposure)) + theme_bw()+\n   labs(y=\"Proportion with high outcome\")+\n  scale_fill_manual(values=c(\"black\", \"red\")))\n\n\n\n\nNow our relationship seems obscured. Our analysis consists of analysing a 2x2 contingency table:\n\ntable(dat$HighExposure, dat$HighOutcome)\n\n       \n        FALSE TRUE\n  FALSE    10    5\n  TRUE      5   10\n\nchisq.test(table(dat$HighExposure, dat$HighOutcome))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(dat$HighExposure, dat$HighOutcome)\nX-squared = 2.1333, df = 1, p-value = 0.1441\n\n\nThe difference in ‘high outcome’ proportions between exposure groups is not statistically significant! (p=0.14). This shows that it is much more difficult to see the relationship if we throw away the exact values of the data points.\nSo although there is a real relationship between exposure and outcome that we have been able to detect by correlating the values in our sample, it was harder to detect when we dichotomised the exposure, and we could not detect it at all in the grouped data.\n\n\nImplications for power and sample size\nOur single example illustrated that dichotomising data made analysis difficult or impossible by discarding data. But it’s easy to cherry pick examples, how does this work in general?\nWe can quantify the average loss of information by power calculations (repeated simulations), and consider how much our sample size would need to increase to overcome this.\nThe graph below shows the power to detect a correlation of 0.5 as the sample size changes using each of the methods described above.\nIf we analyse the continuous data then we need a sample size of about 28 for a power of 80%. If we dichotomise the exposure only we would need 43 samples, if we dichotomise both and analyse the 2x2 table we would need 69, more than twice the original sample.\n\nsampleSizes &lt;- seq(10,100,5)\n\npowers &lt;- data.table(t(sapply(sampleSizes, function(n){c(\n  sampleSizes=n,\n  continuous=100*pwr.r.test(r=0.5, n = n)$power,\n  dichotone=100*pwr.t.test(d=.8 / 0.91, n = n/2)$power,\n  dichotboth=100*pwr.2p.test(h=ES.h(1/3, 2/3), n = n/2)$power\n  )\n  })))\n\nggplot(melt(powers,id.vars = \"sampleSizes\", variable.name = \"Analysis\", value.name = \"Power\"), \n       aes(sampleSizes, Power, color=Analysis)) + \n  geom_line() + \n  geom_point() + \n  theme_bw() + \n  scale_x_continuous(limits=c(0,100)) + \n  labs(x=\"Sample Size (total)\", y=\"Power (%)\") + \n  scale_color_manual(labels=c(dichotboth=\"Dichotomise both\", \n                              dichotone=\"Dichtomise outcome only\", \n                              continuous=\"Correlate continuous measures\"), \n                     values=c(\"black\", \"red\", \"blue\")) + \n  theme(legend.position = c(0.7,0.3), legend.background = element_rect(linetype = 1, size = .5, colour = \"black\")) + \n  geom_hline(yintercept=80, lty=2)\n\n\n\n\n\n\nConclusion\nGrouping up our continuous variables for analysis throws away information. It makes associations more difficult to detect, and increases the number of samples we need to analyse.\nThere are other harmful consequences to dichotomising data. Sometimes a defense of the practice is that by collapsing quantiative findings into yes/no outcomes makes results easier to interpret, but Stephen Senn makes a good case that this is wrong and can even be misleading.\nSo in general, if the phenomenon you are interested in can be measured continuously then you should do so, and be sure to use all of this quantitative information in the analysis.\n\n\nReferences (todo - tidy this up)\nThis phenomenon has been discussed many times in the statistical literature and in blogs\nhttps://errorstatistics.com/2016/08/02/s-senn-painful-dichotomies-guest-post/ (Senn 2016 - a not identical but related problem with dichtomies)\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC1458573/ (Altman and Royston 2006)\nhttp://www.psychology.sunysb.edu/attachment/measures/content/maccallum_on_dichotomizing.pdf (MacCallum et al 2002)\nhttps://bjo.bmj.com/content/98/6/841 (Cumberland et al 2014)"
  },
  {
    "objectID": "flipbook.html#section",
    "href": "flipbook.html#section",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)"
  },
  {
    "objectID": "flipbook.html#section-1",
    "href": "flipbook.html#section-1",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")"
  },
  {
    "objectID": "flipbook.html#section-2",
    "href": "flipbook.html#section-2",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)"
  },
  {
    "objectID": "flipbook.html#section-3",
    "href": "flipbook.html#section-3",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)"
  },
  {
    "objectID": "flipbook.html#section-4",
    "href": "flipbook.html#section-4",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA"
  },
  {
    "objectID": "flipbook.html#section-5",
    "href": "flipbook.html#section-5",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA"
  },
  {
    "objectID": "flipbook.html#section-6",
    "href": "flipbook.html#section-6",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-7",
    "href": "flipbook.html#section-7",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-8",
    "href": "flipbook.html#section-8",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata)\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-9",
    "href": "flipbook.html#section-9",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group)\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-10",
    "href": "flipbook.html#section-10",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time)\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-11",
    "href": "flipbook.html#section-11",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA)\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-12",
    "href": "flipbook.html#section-12",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10()\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-13",
    "href": "flipbook.html#section-13",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10() +\n  labs(x=\"Treatment Group\")\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-14",
    "href": "flipbook.html#section-14",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10() +\n  labs(x=\"Treatment Group\") +\n  labs(y=\"Time (seconds)\")\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-15",
    "href": "flipbook.html#section-15",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10() +\n  labs(x=\"Treatment Group\") +\n  labs(y=\"Time (seconds)\") +\n  facet_wrap(~sex)\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-16",
    "href": "flipbook.html#section-16",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10() +\n  labs(x=\"Treatment Group\") +\n  labs(y=\"Time (seconds)\") +\n  facet_wrap(~sex) +\n  theme_bw()\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-17",
    "href": "flipbook.html#section-17",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10() +\n  labs(x=\"Treatment Group\") +\n  labs(y=\"Time (seconds)\") +\n  facet_wrap(~sex) +\n  theme_bw() +\n  coord_flip()\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-18",
    "href": "flipbook.html#section-18",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10() +\n  labs(x=\"Treatment Group\") +\n  labs(y=\"Time (seconds)\") +\n  facet_wrap(~sex) +\n  theme_bw() +\n  coord_flip() +\n  ggtitle(\"The relationship between walking\n  speed and treatment group, stratified by sex\")\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "flipbook.html#section-19",
    "href": "flipbook.html#section-19",
    "title": "Basic ggplot for scientific graphing",
    "section": "",
    "text": "library(readxl)\nwalkingdata &lt;- read_excel(path=\"walkingspeed.xlsx\",\n                          sheet=\"fixed\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\n\nhead(walkingdata)\n\nlibrary(ggplot2)\n\nggplot(walkingdata) +\n  aes(x=group) +\n  aes(y=time) +\n\n  geom_boxplot(outlier.shape = NA, fill=NA) +\n  scale_y_log10() +\n  labs(x=\"Treatment Group\") +\n  labs(y=\"Time (seconds)\") +\n  facet_wrap(~sex) +\n  theme_bw() +\n  coord_flip() +\n  ggtitle(\"The relationship between walking\n  speed and treatment group, stratified by sex\")\n\n\n\n\n# A tibble: 6 × 6\n  patid group    time sex     age department\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;     \n1     1 treat    1.90 M        53 3         \n2     2 control  3.54 M        61 3         \n3     3 treat    2.93 M        65 1         \n4     4 control  1.82 M        48 2         \n5     5 treat    2.20 M        62 2         \n6     6 control  3.04 M        62 4"
  },
  {
    "objectID": "hi.html",
    "href": "hi.html",
    "title": "Untitled",
    "section": "",
    "text": "Hi!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R and RStudio for Statistics",
    "section": "",
    "text": "This site hosts my course notes, presentations and data files accompanying my courses on R and RStudio.\n\nNBI/JIC MSc Introduction to R\n\nDay 1\nDay 2\nDay 2 (MSc version)\nDay 3\n\nStatistics notes\n\nDon’t dichotomise your data\n\nNotes on using R\n\nTidyverse, data.table and others for making descriptive tables\nUsing packages, functions and pipes\nUsing ggplot2\n\n\nOlder stuff\n\nJIC MSc Workshop (2021/2022)\n\nTutorial"
  },
  {
    "objectID": "praise.html",
    "href": "praise.html",
    "title": "Packages and Pipes",
    "section": "",
    "text": "R is mainly for statistical computing but there are add-on packages to do many other things. You are likely to need to use add-on packages for reading Excel files, data manipulation, running specific analyses, creating nice reports etc.\nIn this tutorial we will get used to using R packages, objects, functions, pipes and how to use the R help system by making ascii animals say nice things to us."
  },
  {
    "objectID": "praise.html#ok-thats-enough-fun.",
    "href": "praise.html#ok-thats-enough-fun.",
    "title": "Packages and Pipes",
    "section": "5.1 OK that’s enough fun.",
    "text": "5.1 OK that’s enough fun.\nOK now back to the real work."
  },
  {
    "objectID": "workshopscript.html",
    "href": "workshopscript.html",
    "title": "JIC MSc Workshop: Analysis Walkthrough",
    "section": "",
    "text": "Introduction\nThis page supports a short workshop in R and RStudio for Statistics. It is not intended as a comprehensive tutorial but as a vehicle for demonstrating and discussing some aspects of a typical analysis using R, with signposting in the lecture notes for further self-directed learning.\nA simple dataset is introduced along with some research questions and I demonstrate a typical process of loading, visualising, cleaning, analysing and reporting the analysis. The workshop will very briefly introduce:\n\nthe RStudio interface\nsources of help\nusing projects and scripts\nbasics of the R language\nloading data from excel\ntidy data\nthe tidyverse and data.table systems for data wrangling\nmerging and appending datasets\nrunning a R function with named arguments\nthe formula interface\nhow to estimate a linear models\nggplot\n\nSupporting material (presentation slides, dataset) is linked.\nA more detailed R tutorial is also available on this site.\n\n\nBackground to the dataset\nWe have an Excel spreadsheet with data corresponding to a rehabilitation intervention for stroke patients.\nHospital patients were recruited from five hospital departments and were randomised to either standard care or an experimental treatment. The time they took to complete a walking speed task was recorded as the outcome. A lower time corresponds to a better outcome.\nThe dataset is here walkingspeed.xlsx: A R script including only the R command needed for the analysis is here: workshopscript.R:\nWe will answer the following questions:\n\nWhat is the mean and standard deviation of walking speed in each treatment group?\nDoes the treatment improve walking speed?\nIs the treatment effect different between men and women?\n\nOur workflow is typical of most staistical analyses:\n\nLoad data\nWrangle\nDescribe\nVisualise\nClean and recode\nTest and model\nReport\n\n\n\nSet up\nWe will need to install the libraries below if we don’t already have them. We should also start a new project in the project directory, and download the data and the code if necessary.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.2.2\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\n\n\nLoad data\nWe should inspect the data in Excel. Note there are three sheets that we need to combine to do our analysis.\nReview the “tidy data” powerpoint presentation here: day2_tidydata.pptx.\n\ntreated &lt;- read_excel(path=\"walkingspeed.xlsx\", \n                      sheet = \"treated\",\n                      range = \"A1:B68\")\n\nNotes:\n\nThe library readxl for reading Excel sheets. There are alternatives but I find this works well.\nMultiline function call\nNamed arguments\nAssigning the outcome to the variable\nHelp file, how did we know how this function worked.\n\nWe need to load all three sheets as separate data frames.\n\ncontrol &lt;- read_excel(path=\"walkingspeed.xlsx\", \n                      sheet = \"control\",\n                      range = \"A1:B70\")\n\nmetadata &lt;- read_excel(path=\"walkingspeed.xlsx\", \n                      sheet = \"meta\",\n                      range = \"A1:D139\")\n\n\n\nExplore data\nR includes several functions to inspect data\n\nclass(treated)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nstr(treated)\n\ntibble [67 × 2] (S3: tbl_df/tbl/data.frame)\n $ patid: num [1:67] 1 3 5 7 9 11 13 15 17 19 ...\n $ time : chr [1:67] \"1.8975120000000001\" \"2.927432\" \"2.2042579999999998\" \"2.1441910000000002\" ...\n\nsummary(treated)\n\n     patid         time          \n Min.   :  1   Length:67         \n 1st Qu.: 34   Class :character  \n Median : 67   Mode  :character  \n Mean   : 67                     \n 3rd Qu.:100                     \n Max.   :133                     \n\nhead(treated)\n\n# A tibble: 6 × 2\n  patid time              \n  &lt;dbl&gt; &lt;chr&gt;             \n1     1 1.8975120000000001\n2     3 2.927432          \n3     5 2.2042579999999998\n4     7 2.1441910000000002\n5     9 1.7203250000000001\n6    11 2.1476410000000001\n\n# View(treated)\nstr(control)\n\ntibble [69 × 2] (S3: tbl_df/tbl/data.frame)\n $ patid   : num [1:69] 2 4 6 8 10 12 14 16 18 20 ...\n $ walktime: num [1:69] 3.54 1.82 3.04 2.47 2.48 ...\n\n\nNotes:\n\nData can be numeric, character strings, (factors or logical)\nDo we know what each of these types is for?\n\n\n\nAccess elements from the dataframe\n\ncontrol$walktime\n\n [1]   3.537158   1.819787   3.038065   2.469580   2.483921   2.440482\n [7]   2.779616   3.739146   1.956132   5.415308   3.067604 185.362000\n[13]   2.690378   0.015400   2.716427   1.952741   2.707647   5.056214\n[19]   3.319593   1.493927   2.654815   2.856972   2.401613   1.714169\n[25]   3.183433   2.897221  10.590513   2.572139   2.380559   3.528461\n[31]  12.168967   2.274398   2.631071   2.524958   2.191847   3.943916\n[37]   3.390101   5.146895   2.426002   3.340182   2.392610   2.375177\n[43]   2.210679   3.344224   2.233431   2.749903   3.361010   2.803598\n[49]   4.499523   3.642821   2.225054   2.318357   2.241562   2.498969\n[55]   2.378422   2.370767   2.169139   2.373494   2.959015   3.881843\n[61]   2.296210   3.075860   5.033359   2.870730   3.980520   2.290122\n[67]   1.843314   2.083927   2.778637\n\ncontrol$walktime[1]\n\n[1] 3.537158\n\ncontrol$walktime[1:10]\n\n [1] 3.537158 1.819787 3.038065 2.469580 2.483921 2.440482 2.779616 3.739146\n [9] 1.956132 5.415308\n\nmean(control$walktime)\n\n[1] 5.712487\n\nmean(control$walktime[1:5])\n\n[1] 2.669702\n\nlog(control$walktime)\n\n [1]  1.2633236  0.5987195  1.1112208  0.9040481  0.9098384  0.8921956\n [7]  1.0223128  1.3188572  0.6709691  1.6892298  1.1208968  5.2223107\n[13]  0.9896817 -4.1733878  0.9993174  0.6692340  0.9960800  1.6206180\n[19]  1.1998422  0.4014082  0.9763750  1.0497623  0.8761406  0.5389284\n[25]  1.1579602  1.0637520  2.3599586  0.9447378  0.8673353  1.2608618\n[31]  2.4988890  0.8217154  0.9673910  0.9262244  0.7847446  1.3721741\n[37]  1.2208597  1.6383936  0.8862446  1.2060253  0.8723848  0.8650720\n[43]  0.7932997  1.2072347  0.8035390  1.0115656  1.2122415  1.0309036\n[49]  1.5039714  1.2927584  0.7997812  0.8408587  0.8071729  0.9158782\n[55]  0.8664372  0.8632135  0.7743303  0.8643631  1.0848564  1.3563100\n[61]  0.8312599  1.1235845  1.6160876  1.0545664  1.3814125  0.8286051\n[67]  0.6115650  0.7342541  1.0219605\n\nlog(control$walktime[1:5])\n\n[1] 1.2633236 0.5987195 1.1112208 0.9040481 0.9098384\n\n\n\n\nWrangle\nFor analysis we will need all the data into one data frame. We need to append (row bind) the treatment and control results, then merge (join) the meta data.\n\n# Remind ourselves of the structure of the dataset\nstr(treated)\n\ntibble [67 × 2] (S3: tbl_df/tbl/data.frame)\n $ patid: num [1:67] 1 3 5 7 9 11 13 15 17 19 ...\n $ time : chr [1:67] \"1.8975120000000001\" \"2.927432\" \"2.2042579999999998\" \"2.1441910000000002\" ...\n\nstr(control)\n\ntibble [69 × 2] (S3: tbl_df/tbl/data.frame)\n $ patid   : num [1:69] 2 4 6 8 10 12 14 16 18 20 ...\n $ walktime: num [1:69] 3.54 1.82 3.04 2.47 2.48 ...\n\n\nWe need to make sure the vectors we are merging have the same type and name!\nThere are a lot of ways to do the same thing. Here I am illustrating the ‘base’ R way, the ‘tidyverse’ way and the ‘data.table’ way to convert a new numeric variable from a character variable.\n\n# Base R\ntreated$walktime &lt;- as.numeric(treated$time)\n\nWarning: NAs introduced by coercion\n\n# data.table\nsetDT(treated)\ntreated[ , walktime := as.numeric(time)  ]\n\nWarning in eval(jsub, SDenv, parent.frame()): NAs introduced by coercion\n\n# tidyverse\ntreated &lt;- treated %&gt;% mutate(walktime = as.numeric(time))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `walktime = as.numeric(time)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nNow we can append the rows and merge them with the metadata. Again there is a tidyverse function for this, and a data.table function for this.\n\n# data.table\ncombined &lt;- rbind(treated, control, fill=TRUE)\ncombined &lt;- rbind(treated=treated, \n                  control=control, \n                  fill=TRUE, idcol=\"group\")\n# tidyverse\ncombined &lt;- bind_rows(treated, control)\ncombined &lt;- bind_rows(treated = treated, \n                      control = control, \n                      .id = \"group\")\n\nstr(metadata)\n\ntibble [138 × 4] (S3: tbl_df/tbl/data.frame)\n $ patient   : num [1:138] 1 2 3 4 5 6 7 8 9 10 ...\n $ sex       : chr [1:138] \"M\" \"M\" \"M\" \"M\" ...\n $ age       : num [1:138] 53 61 65 48 62 62 57 57 57 55 ...\n $ department: num [1:138] 3 3 1 2 2 4 2 4 3 2 ...\n\nstr(combined)\n\nClasses 'data.table' and 'data.frame':  136 obs. of  4 variables:\n $ group   : chr  \"treated\" \"treated\" \"treated\" \"treated\" ...\n $ patid   : num  1 3 5 7 9 11 13 15 17 19 ...\n $ time    : chr  \"1.8975120000000001\" \"2.927432\" \"2.2042579999999998\" \"2.1441910000000002\" ...\n $ walktime: num  1.9 2.93 2.2 2.14 1.72 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\nwalkingdata &lt;- merge(combined, metadata, \n                by.x = \"patid\", by.y = \"patient\")\n\nhead(walkingdata)\n\n   patid   group               time walktime sex age department\n1:     1 treated 1.8975120000000001 1.897512   M  53          3\n2:     2 control               &lt;NA&gt; 3.537158   M  61          3\n3:     3 treated           2.927432 2.927432   M  65          1\n4:     4 control               &lt;NA&gt; 1.819787   M  48          2\n5:     5 treated 2.2042579999999998 2.204258   M  62          2\n6:     6 control               &lt;NA&gt; 3.038065   M  62          4\n\nstr(walkingdata)\n\nClasses 'data.table' and 'data.frame':  136 obs. of  7 variables:\n $ patid     : num  1 2 3 4 5 6 7 8 9 10 ...\n $ group     : chr  \"treated\" \"control\" \"treated\" \"control\" ...\n $ time      : chr  \"1.8975120000000001\" NA \"2.927432\" NA ...\n $ walktime  : num  1.9 3.54 2.93 1.82 2.2 ...\n $ sex       : chr  \"M\" \"M\" \"M\" \"M\" ...\n $ age       : num  53 61 65 48 62 62 57 57 57 55 ...\n $ department: num  3 3 1 2 2 4 2 4 3 2 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n - attr(*, \"sorted\")= chr \"patid\"\n\nsummary(walkingdata)\n\n     patid           group               time              walktime       \n Min.   :  1.00   Length:136         Length:136         Min.   :  0.0154  \n 1st Qu.: 34.75   Class :character   Class :character   1st Qu.:  2.1688  \n Median : 68.50   Mode  :character   Mode  :character   Median :  2.4287  \n Mean   : 68.52                                         Mean   :  4.1956  \n 3rd Qu.:102.25                                         3rd Qu.:  2.9491  \n Max.   :138.00                                         Max.   :185.3620  \n                                                        NA's   :1         \n     sex                 age          department   \n Length:136         Min.   :45.00   Min.   :1.000  \n Class :character   1st Qu.:54.00   1st Qu.:2.000  \n Mode  :character   Median :57.00   Median :3.000  \n                    Mean   :57.55   Mean   :2.596  \n                    3rd Qu.:60.25   3rd Qu.:3.250  \n                    Max.   :72.00   Max.   :4.000  \n                                                   \n\n\n\n\nDescribe\nOur first task was to describe the mean and standard deviation of walking time by group. There is no simple way to do this with base R. Possible tidyverse and data.table approaches are shown below.\n\n# Tidyverse\nwalkingdata %&gt;% \n  filter(!is.na(walktime)) %&gt;% \n  group_by(group) %&gt;% \n  summarise(Mean=mean(walktime), SD=sd(walktime))\n\n# A tibble: 2 × 3\n  group    Mean    SD\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 control  5.71 22.0 \n2 treated  2.61  2.00\n\n# data.table\nwalkingdata[!is.na(walktime) ,  \n            .(Mean=mean(walktime), SD=sd(walktime)),\n            group]\n\n     group     Mean        SD\n1: treated 2.609674  1.999246\n2: control 5.712487 22.011155\n\n\n\n\nVisualise\nBase R graphics are difficult to work with. ggplot2 provides an excellent system for graphing scientific data using R. See the associated slides and flipbook.\n\n# A very bad graph\nplot(walkingdata$age , walkingdata$walktime)\n\n\n\n# A better graph\nggplot(walkingdata) + \n  aes(x=age, y=walktime) + \n  geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n# Adorn the graph\nggplot(walkingdata) + \n  aes(x=age, y=walktime) + \n  geom_point() + \n  labs(x=\"Age (years)\", y=\"Time (seconds)\") + \n  scale_y_log10() + \n  facet_wrap(~sex) +\n  theme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\nggplot(walkingdata) + \n  aes(x=group, y=walktime) + \n  geom_boxplot() + \n  labs(x=\"Treatment group\", y=\"Time (seconds)\") + \n  scale_y_log10() + \n  facet_wrap(~sex) +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\nClean data\nOur graphics suggest that there are some data points that are probably technical failures. We should remove these.\n\n# base\nwalkingdata$walktime[ walkingdata$walktime &gt; 100 ] &lt;- NA\nwalkingdata$walktime[ walkingdata$walktime &lt; 0.1 ] &lt;- NA\n# data.table\nwalkingdata[ walktime&gt;100 , walktime := NA]\nwalkingdata[ walktime&lt;0.1 , walktime := NA]\n\n\n\nSimple tests\nNow we can conduct a simple statistical test of the walking speed across groups. Note the ‘formula’ interface:\n\nt.test( data = walkingdata , walktime ~ group)\n\n\n    Welch Two Sample t-test\n\ndata:  walktime by group\nt = 1.5788, df = 126.69, p-value = 0.1169\nalternative hypothesis: true difference in means between group control and group treated is not equal to 0\n95 percent confidence interval:\n -0.1283567  1.1413738\nsample estimates:\nmean in group control mean in group treated \n             3.116183              2.609674 \n\nttest1 &lt;- t.test( data = walkingdata , walktime ~ group)\nttest1$p.value\n\n[1] 0.1168802\n\n\nWhat does this suggest about the treatment effectiveness?\n\n\nModel\nThis test ignores much of what we know about these participants, and may not be suitable. A linear model is a better paradiagm for statistical analysis. It allows us to build more complex analyses, and easily test our assumptions.\n\nlm1 &lt;- lm( data = walkingdata , walktime ~ group)\nsummary(lm1)\n\n\nCall:\nlm(formula = walktime ~ group, data = walkingdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6223 -0.7297 -0.3963  0.0604 15.0317 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.1162     0.2257  13.806   &lt;2e-16 ***\ngrouptreated  -0.5065     0.3204  -1.581    0.116    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.848 on 131 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.01872,   Adjusted R-squared:  0.01123 \nF-statistic: 2.499 on 1 and 131 DF,  p-value: 0.1163\n\nconfint(lm1)\n\n                 2.5 %    97.5 %\n(Intercept)   2.669671 3.5626939\ngrouptreated -1.140358 0.1273411\n\n\n\n\nDiagnose\nThe diagnostics suggest something is wrong. We can transform the data so that the assumptions of the model are met.\n\nplot(lm1)\n\n\n\n\n\n\n\n\n\n\n\n\nwalkingdata[ , speed := 1/walktime]\n\nlm2 &lt;- lm( data = walkingdata , log(walktime) ~ group)\nlm3 &lt;- lm( data = walkingdata , 1/walktime ~ group)\nplot(lm2)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(lm3)\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(lm3)\n\n\nCall:\nlm(formula = 1/walktime ~ group, data = walkingdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.38121 -0.06170  0.00132  0.06453  0.30257 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.36681    0.01287  28.500  &lt; 2e-16 ***\ngrouptreated  0.07108    0.01827   3.891 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1053 on 131 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.1036,    Adjusted R-squared:  0.09674 \nF-statistic: 15.14 on 1 and 131 DF,  p-value: 0.0001583\n\ngtsummary::tbl_regression(lm3)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    group\n\n\n\n        control\n—\n—\n\n        treated\n0.07\n0.03, 0.11\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nIs the interpretation different now?\n\n\nAugment model\nWe can develop the model by adding terms for age and department. We should always include these because they explain variance in the outcome measure.\n\nlm4 &lt;- lm( data = walkingdata , 1/walktime ~ group + age + sex + department)\ngtsummary::tbl_regression(lm4)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    group\n\n\n\n        control\n—\n—\n\n        treated\n0.07\n0.04, 0.11\n&lt;0.001\n    age\n0.00\n-0.01, 0.00\n0.029\n    sex\n\n\n\n        F\n—\n—\n\n        M\n-0.01\n-0.05, 0.03\n0.6\n    department\n0.02\n0.00, 0.03\n0.060\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\nlm5 &lt;- lm( data = walkingdata , 1/walktime ~ group + age + sex + factor(department))\ngtsummary::tbl_regression(lm5)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    group\n\n\n\n        control\n—\n—\n\n        treated\n0.07\n0.03, 0.11\n&lt;0.001\n    age\n0.00\n-0.01, 0.00\n0.028\n    sex\n\n\n\n        F\n—\n—\n\n        M\n-0.01\n-0.05, 0.04\n0.7\n    factor(department)\n\n\n\n        1\n—\n—\n\n        2\n-0.01\n-0.06, 0.04\n0.7\n        3\n0.04\n-0.01, 0.09\n0.10\n        4\n0.03\n-0.02, 0.08\n0.2\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\nanova(lm5 , update(lm5, . ~ . -age))\n\nAnalysis of Variance Table\n\nModel 1: 1/walktime ~ group + age + sex + factor(department)\nModel 2: 1/walktime ~ group + sex + factor(department)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    126 1.3190                              \n2    127 1.3709 -1 -0.051897 4.9574 0.02776 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInteractions\nTo test whether the treatment effect varies by sex we should test the group*sex interaction.\n\nlm6 &lt;- lm( data = walkingdata , 1/walktime ~ group*sex + age +  factor(department))\n\ngtsummary::tbl_regression(lm6)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    group\n\n\n\n        control\n—\n—\n\n        treated\n0.05\n-0.02, 0.12\n0.14\n    sex\n\n\n\n        F\n—\n—\n\n        M\n-0.02\n-0.08, 0.04\n0.5\n    age\n0.00\n-0.01, 0.00\n0.029\n    factor(department)\n\n\n\n        1\n—\n—\n\n        2\n-0.01\n-0.07, 0.04\n0.6\n        3\n0.04\n-0.01, 0.09\n0.10\n        4\n0.03\n-0.02, 0.08\n0.2\n    group * sex\n\n\n\n        treated * M\n0.03\n-0.05, 0.11\n0.5\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\nanova( lm5 , lm6 )\n\nAnalysis of Variance Table\n\nModel 1: 1/walktime ~ group + age + sex + factor(department)\nModel 2: 1/walktime ~ group * sex + age + factor(department)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    126 1.3190                           \n2    125 1.3148  1 0.0042764 0.4066 0.5249\n\nlibrary(emmeans)\n\nWarning: package 'emmeans' was built under R version 4.2.3\n\nemmeans(lm6, pairwise ~ group | sex)\n\n$emmeans\nsex = F:\n group   emmean     SE  df lower.CL upper.CL\n control  0.379 0.0246 125    0.330    0.428\n treated  0.430 0.0260 125    0.379    0.482\n\nsex = M:\n group   emmean     SE  df lower.CL upper.CL\n control  0.359 0.0151 125    0.330    0.389\n treated  0.436 0.0150 125    0.407    0.466\n\nResults are averaged over the levels of: department \nConfidence level used: 0.95 \n\n$contrasts\nsex = F:\n contrast          estimate     SE  df t.ratio p.value\n control - treated  -0.0513 0.0346 125  -1.481  0.1411\n\nsex = M:\n contrast          estimate     SE  df t.ratio p.value\n control - treated  -0.0771 0.0211 125  -3.654  0.0004\n\nResults are averaged over the levels of: department \n\ntreatmentestimates &lt;- as.data.frame(confint(emmeans(lm6, pairwise ~ group | sex)$contrast))\n\n\ng1 &lt;- ggplot(treatmentestimates) + aes(x=sex, y=estimate, ymin=lower.CL, ymax=upper.CL) + \n  geom_point() + \n  geom_errorbar(width=0.2) + \n  geom_hline(yintercept = 0) + \n  theme_bw() + \n  labs(x=\"Sex\", y=\"Treatment effect\") \n  \ng1 \n\n\n\ng1 + coord_flip()\n\n\n\n\nWhat does this suggest. Does the treatment work in men and women?"
  }
]