[
  {
    "objectID": "workshopscript.html",
    "href": "workshopscript.html",
    "title": "JIC MSc Workshop: Analysis Walkthrough",
    "section": "",
    "text": "Introduction\nThis page supports a short workshop in R and RStudio for Statistics. It is not intended as a comprehensive tutorial but as a vehicle for demonstrating and discussing some aspects of a typical analysis using R, with signposting in the lecture notes for further self-directed learning.\nA simple dataset is introduced along with some research questions and I demonstrate a typical process of loading, visualising, cleaning, analysing and reporting the analysis. The workshop will very briefly introduce:\n\nthe RStudio interface\nsources of help\nusing projects and scripts\nbasics of the R language\nloading data from excel\ntidy data\nthe tidyverse and data.table systems for data wrangling\nmerging and appending datasets\nrunning a R function with named arguments\nthe formula interface\nhow to estimate a linear models\nggplot\n\nSupporting material (presentation slides, dataset) is linked.\nA more detailed R tutorial is also available on this site.\n\n\nBackground to the dataset\nWe have an Excel spreadsheet with data corresponding to a rehabilitation intervention for stroke patients.\nHospital patients were recruited from five hospital departments and were randomised to either standard care or an experimental treatment. The time they took to complete a walking speed task was recorded as the outcome. A lower time corresponds to a better outcome.\nThe dataset is here walkingspeed.xlsx: A R script including only the R command needed for the analysis is here: workshopscript.R:\nWe will answer the following questions:\n\nWhat is the mean and standard deviation of walking speed in each treatment group?\nDoes the treatment improve walking speed?\nIs the treatment effect different between men and women?\n\nOur workflow is typical of most staistical analyses:\n\nLoad data\nWrangle\nDescribe\nVisualise\nClean and recode\nTest and model\nReport\n\n\n\nSet up\nWe will need to install the libraries below if we don’t already have them. We should also start a new project in the project directory, and download the data and the code if necessary.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.1.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.1.3\n\n\n\n\nLoad data\nWe should inspect the data in Excel. Note there are three sheets that we need to combine to do our analysis.\nReview the “tidy data” powerpoint presentation here: day2_tidydata.pptx.\n\ntreated &lt;- read_excel(path=\"walkingspeed.xlsx\", \n                      sheet = \"treated\",\n                      range = \"A1:B68\")\n\nNotes:\n\nThe library readxl for reading Excel sheets. There are alternatives but I find this works well.\nMultiline function call\nNamed arguments\nAssigning the outcome to the variable\nHelp file, how did we know how this function worked.\n\nWe need to load all three sheets as separate data frames.\n\ncontrol &lt;- read_excel(path=\"walkingspeed.xlsx\", \n                      sheet = \"control\",\n                      range = \"A1:B70\")\n\nmetadata &lt;- read_excel(path=\"walkingspeed.xlsx\", \n                      sheet = \"meta\",\n                      range = \"A1:D139\")\n\n\n\nExplore data\nR includes several functions to inspect data\n\nclass(treated)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nstr(treated)\n\ntibble [67 x 2] (S3: tbl_df/tbl/data.frame)\n $ patid: num [1:67] 1 3 5 7 9 11 13 15 17 19 ...\n $ time : chr [1:67] \"1.8975120000000001\" \"2.927432\" \"2.2042579999999998\" \"2.1441910000000002\" ...\n\nsummary(treated)\n\n     patid         time          \n Min.   :  1   Length:67         \n 1st Qu.: 34   Class :character  \n Median : 67   Mode  :character  \n Mean   : 67                     \n 3rd Qu.:100                     \n Max.   :133                     \n\nhead(treated)\n\n# A tibble: 6 x 2\n  patid time              \n  &lt;dbl&gt; &lt;chr&gt;             \n1     1 1.8975120000000001\n2     3 2.927432          \n3     5 2.2042579999999998\n4     7 2.1441910000000002\n5     9 1.7203250000000001\n6    11 2.1476410000000001\n\n# View(treated)\nstr(control)\n\ntibble [69 x 2] (S3: tbl_df/tbl/data.frame)\n $ patid   : num [1:69] 2 4 6 8 10 12 14 16 18 20 ...\n $ walktime: num [1:69] 3.54 1.82 3.04 2.47 2.48 ...\n\n\nNotes:\n\nData can be numeric, character strings, (factors or logical)\nDo we know what each of these types is for?\n\n\n\nAccess elements from the dataframe\n\ncontrol$walktime\n\n [1]   3.537158   1.819787   3.038065   2.469580   2.483921   2.440482\n [7]   2.779616   3.739146   1.956132   5.415308   3.067604 185.362000\n[13]   2.690378   0.015400   2.716427   1.952741   2.707647   5.056214\n[19]   3.319593   1.493927   2.654815   2.856972   2.401613   1.714169\n[25]   3.183433   2.897221  10.590513   2.572139   2.380559   3.528461\n[31]  12.168967   2.274398   2.631071   2.524958   2.191847   3.943916\n[37]   3.390101   5.146895   2.426002   3.340182   2.392610   2.375177\n[43]   2.210679   3.344224   2.233431   2.749903   3.361010   2.803598\n[49]   4.499523   3.642821   2.225054   2.318357   2.241562   2.498969\n[55]   2.378422   2.370767   2.169139   2.373494   2.959015   3.881843\n[61]   2.296210   3.075860   5.033359   2.870730   3.980520   2.290122\n[67]   1.843314   2.083927   2.778637\n\ncontrol$walktime[1]\n\n[1] 3.537158\n\ncontrol$walktime[1:10]\n\n [1] 3.537158 1.819787 3.038065 2.469580 2.483921 2.440482 2.779616 3.739146\n [9] 1.956132 5.415308\n\nmean(control$walktime)\n\n[1] 5.712487\n\nmean(control$walktime[1:5])\n\n[1] 2.669702\n\nlog(control$walktime)\n\n [1]  1.2633236  0.5987195  1.1112208  0.9040481  0.9098384  0.8921956\n [7]  1.0223128  1.3188572  0.6709691  1.6892298  1.1208968  5.2223107\n[13]  0.9896817 -4.1733878  0.9993174  0.6692340  0.9960800  1.6206180\n[19]  1.1998422  0.4014082  0.9763750  1.0497623  0.8761406  0.5389284\n[25]  1.1579602  1.0637520  2.3599586  0.9447378  0.8673353  1.2608618\n[31]  2.4988890  0.8217154  0.9673910  0.9262244  0.7847446  1.3721741\n[37]  1.2208597  1.6383936  0.8862446  1.2060253  0.8723848  0.8650720\n[43]  0.7932997  1.2072347  0.8035390  1.0115656  1.2122415  1.0309036\n[49]  1.5039714  1.2927584  0.7997812  0.8408587  0.8071729  0.9158782\n[55]  0.8664372  0.8632135  0.7743303  0.8643631  1.0848564  1.3563100\n[61]  0.8312599  1.1235845  1.6160876  1.0545664  1.3814125  0.8286051\n[67]  0.6115650  0.7342541  1.0219605\n\nlog(control$walktime[1:5])\n\n[1] 1.2633236 0.5987195 1.1112208 0.9040481 0.9098384\n\n\n\n\nWrangle\nFor analysis we will need all the data into one data frame. We need to append (row bind) the treatment and control results, then merge (join) the meta data.\n\n# Remind ourselves of the structure of the dataset\nstr(treated)\n\ntibble [67 x 2] (S3: tbl_df/tbl/data.frame)\n $ patid: num [1:67] 1 3 5 7 9 11 13 15 17 19 ...\n $ time : chr [1:67] \"1.8975120000000001\" \"2.927432\" \"2.2042579999999998\" \"2.1441910000000002\" ...\n\nstr(control)\n\ntibble [69 x 2] (S3: tbl_df/tbl/data.frame)\n $ patid   : num [1:69] 2 4 6 8 10 12 14 16 18 20 ...\n $ walktime: num [1:69] 3.54 1.82 3.04 2.47 2.48 ...\n\n\nWe need to make sure the vectors we are merging have the same type and name!\nThere are a lot of ways to do the same thing. Here I am illustrating the ‘base’ R way, the ‘tidyverse’ way and the ‘data.table’ way to convert a new numeric variable from a character variable.\n\n# Base R\ntreated$walktime &lt;- as.numeric(treated$time)\n\nWarning: NAs introduced by coercion\n\n# data.table\nsetDT(treated)\ntreated[ , walktime := as.numeric(time)  ]\n\nWarning in eval(jsub, SDenv, parent.frame()): NAs introduced by coercion\n\n# tidyverse\ntreated &lt;- treated %&gt;% mutate(walktime = as.numeric(time))\n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `walktime = as.numeric(time)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nNow we can append the rows and merge them with the metadata. Again there is a tidyverse function for this, and a data.table function for this.\n\n# data.table\ncombined &lt;- rbind(treated, control, fill=TRUE)\ncombined &lt;- rbind(treated=treated, \n                  control=control, \n                  fill=TRUE, idcol=\"group\")\n# tidyverse\ncombined &lt;- bind_rows(treated, control)\ncombined &lt;- bind_rows(treated = treated, \n                      control = control, \n                      .id = \"group\")\n\nstr(metadata)\n\ntibble [138 x 4] (S3: tbl_df/tbl/data.frame)\n $ patient   : num [1:138] 1 2 3 4 5 6 7 8 9 10 ...\n $ sex       : chr [1:138] \"M\" \"M\" \"M\" \"M\" ...\n $ age       : num [1:138] 53 61 65 48 62 62 57 57 57 55 ...\n $ department: num [1:138] 3 3 1 2 2 4 2 4 3 2 ...\n\nstr(combined)\n\nClasses 'data.table' and 'data.frame':  136 obs. of  4 variables:\n $ group   : chr  \"treated\" \"treated\" \"treated\" \"treated\" ...\n $ patid   : num  1 3 5 7 9 11 13 15 17 19 ...\n $ time    : chr  \"1.8975120000000001\" \"2.927432\" \"2.2042579999999998\" \"2.1441910000000002\" ...\n $ walktime: num  1.9 2.93 2.2 2.14 1.72 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\nwalkingdata &lt;- merge(combined, metadata, \n                by.x = \"patid\", by.y = \"patient\")\n\nhead(walkingdata)\n\n   patid   group               time walktime sex age department\n1:     1 treated 1.8975120000000001 1.897512   M  53          3\n2:     2 control               &lt;NA&gt; 3.537158   M  61          3\n3:     3 treated           2.927432 2.927432   M  65          1\n4:     4 control               &lt;NA&gt; 1.819787   M  48          2\n5:     5 treated 2.2042579999999998 2.204258   M  62          2\n6:     6 control               &lt;NA&gt; 3.038065   M  62          4\n\nstr(walkingdata)\n\nClasses 'data.table' and 'data.frame':  136 obs. of  7 variables:\n $ patid     : num  1 2 3 4 5 6 7 8 9 10 ...\n $ group     : chr  \"treated\" \"control\" \"treated\" \"control\" ...\n $ time      : chr  \"1.8975120000000001\" NA \"2.927432\" NA ...\n $ walktime  : num  1.9 3.54 2.93 1.82 2.2 ...\n $ sex       : chr  \"M\" \"M\" \"M\" \"M\" ...\n $ age       : num  53 61 65 48 62 62 57 57 57 55 ...\n $ department: num  3 3 1 2 2 4 2 4 3 2 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n - attr(*, \"sorted\")= chr \"patid\"\n\nsummary(walkingdata)\n\n     patid           group               time              walktime       \n Min.   :  1.00   Length:136         Length:136         Min.   :  0.0154  \n 1st Qu.: 34.75   Class :character   Class :character   1st Qu.:  2.1688  \n Median : 68.50   Mode  :character   Mode  :character   Median :  2.4287  \n Mean   : 68.52                                         Mean   :  4.1956  \n 3rd Qu.:102.25                                         3rd Qu.:  2.9491  \n Max.   :138.00                                         Max.   :185.3620  \n                                                        NA's   :1         \n     sex                 age          department   \n Length:136         Min.   :45.00   Min.   :1.000  \n Class :character   1st Qu.:54.00   1st Qu.:2.000  \n Mode  :character   Median :57.00   Median :3.000  \n                    Mean   :57.55   Mean   :2.596  \n                    3rd Qu.:60.25   3rd Qu.:3.250  \n                    Max.   :72.00   Max.   :4.000  \n                                                   \n\n\n\n\nDescribe\nOur first task was to describe the mean and standard deviation of walking time by group. There is no simple way to do this with base R. Possible tidyverse and data.table approaches are shown below.\n\n# Tidyverse\nwalkingdata %&gt;% \n  filter(!is.na(walktime)) %&gt;% \n  group_by(group) %&gt;% \n  summarise(Mean=mean(walktime), SD=sd(walktime))\n\n# A tibble: 2 x 3\n  group    Mean    SD\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 control  5.71 22.0 \n2 treated  2.61  2.00\n\n# data.table\nwalkingdata[!is.na(walktime) ,  \n            .(Mean=mean(walktime), SD=sd(walktime)),\n            group]\n\n     group     Mean        SD\n1: treated 2.609674  1.999246\n2: control 5.712487 22.011155\n\n\n\n\nVisualise\nBase R graphics are difficult to work with. ggplot2 provides an excellent system for graphing scientific data using R. See the associated slides and flipbook.\n\n# A very bad graph\nplot(walkingdata$age , walkingdata$walktime)\n\n\n\n\n\n\n\n# A better graph\nggplot(walkingdata) + \n  aes(x=age, y=walktime) + \n  geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n# Adorn the graph\nggplot(walkingdata) + \n  aes(x=age, y=walktime) + \n  geom_point() + \n  labs(x=\"Age (years)\", y=\"Time (seconds)\") + \n  scale_y_log10() + \n  facet_wrap(~sex) +\n  theme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(walkingdata) + \n  aes(x=group, y=walktime) + \n  geom_boxplot() + \n  labs(x=\"Treatment group\", y=\"Time (seconds)\") + \n  scale_y_log10() + \n  facet_wrap(~sex) +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nClean data\nOur graphics suggest that there are some data points that are probably technical failures. We should remove these.\n\n# base\nwalkingdata$walktime[ walkingdata$walktime &gt; 100 ] &lt;- NA\nwalkingdata$walktime[ walkingdata$walktime &lt; 0.1 ] &lt;- NA\n# data.table\nwalkingdata[ walktime&gt;100 , walktime := NA]\nwalkingdata[ walktime&lt;0.1 , walktime := NA]\n\n\n\nSimple tests\nNow we can conduct a simple statistical test of the walking speed across groups. Note the ‘formula’ interface:\n\nt.test( data = walkingdata , walktime ~ group)\n\n\n    Welch Two Sample t-test\n\ndata:  walktime by group\nt = 1.5788, df = 126.69, p-value = 0.1169\nalternative hypothesis: true difference in means between group control and group treated is not equal to 0\n95 percent confidence interval:\n -0.1283567  1.1413738\nsample estimates:\nmean in group control mean in group treated \n             3.116183              2.609674 \n\nttest1 &lt;- t.test( data = walkingdata , walktime ~ group)\nttest1$p.value\n\n[1] 0.1168802\n\n\nWhat does this suggest about the treatment effectiveness?\n\n\nModel\nThis test ignores much of what we know about these participants, and may not be suitable. A linear model is a better paradiagm for statistical analysis. It allows us to build more complex analyses, and easily test our assumptions.\n\nlm1 &lt;- lm( data = walkingdata , walktime ~ group)\nsummary(lm1)\n\n\nCall:\nlm(formula = walktime ~ group, data = walkingdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6223 -0.7297 -0.3963  0.0604 15.0317 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3.1162     0.2257  13.806   &lt;2e-16 ***\ngrouptreated  -0.5065     0.3204  -1.581    0.116    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.848 on 131 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.01872,   Adjusted R-squared:  0.01123 \nF-statistic: 2.499 on 1 and 131 DF,  p-value: 0.1163\n\nconfint(lm1)\n\n                 2.5 %    97.5 %\n(Intercept)   2.669671 3.5626939\ngrouptreated -1.140358 0.1273411\n\n\n\n\nDiagnose\nThe diagnostics suggest something is wrong. We can transform the data so that the assumptions of the model are met.\n\nplot(lm1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwalkingdata[ , speed := 1/walktime]\n\nlm2 &lt;- lm( data = walkingdata , log(walktime) ~ group)\nlm3 &lt;- lm( data = walkingdata , 1/walktime ~ group)\nplot(lm2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(lm3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(lm3)\n\n\nCall:\nlm(formula = 1/walktime ~ group, data = walkingdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.38121 -0.06170  0.00132  0.06453  0.30257 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.36681    0.01287  28.500  &lt; 2e-16 ***\ngrouptreated  0.07108    0.01827   3.891 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1053 on 131 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.1036,    Adjusted R-squared:  0.09674 \nF-statistic: 15.14 on 1 and 131 DF,  p-value: 0.0001583\n\ngtsummary::tbl_regression(lm3)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ngroup\n\n\n\n\n\ncontrol\n—\n—\n\n\n\ntreated\n0.07\n0.03, 0.11\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nIs the interpretation different now?\n\n\nAugment model\nWe can develop the model by adding terms for age and department. We should always include these because they explain variance in the outcome measure.\n\nlm4 &lt;- lm( data = walkingdata , 1/walktime ~ group + age + sex + department)\ngtsummary::tbl_regression(lm4)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ngroup\n\n\n\n\n\ncontrol\n—\n—\n\n\n\ntreated\n0.07\n0.04, 0.11\n&lt;0.001\n\n\nage\n0.00\n-0.01, 0.00\n0.029\n\n\nsex\n\n\n\n\n\nF\n—\n—\n\n\n\nM\n-0.01\n-0.05, 0.03\n0.6\n\n\ndepartment\n0.02\n0.00, 0.03\n0.060\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\nlm5 &lt;- lm( data = walkingdata , 1/walktime ~ group + age + sex + factor(department))\ngtsummary::tbl_regression(lm5)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ngroup\n\n\n\n\n\ncontrol\n—\n—\n\n\n\ntreated\n0.07\n0.03, 0.11\n&lt;0.001\n\n\nage\n0.00\n-0.01, 0.00\n0.028\n\n\nsex\n\n\n\n\n\nF\n—\n—\n\n\n\nM\n-0.01\n-0.05, 0.04\n0.7\n\n\nfactor(department)\n\n\n\n\n\n1\n—\n—\n\n\n\n2\n-0.01\n-0.06, 0.04\n0.7\n\n\n3\n0.04\n-0.01, 0.09\n0.10\n\n\n4\n0.03\n-0.02, 0.08\n0.2\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\nanova(lm5 , update(lm5, . ~ . -age))\n\nAnalysis of Variance Table\n\nModel 1: 1/walktime ~ group + age + sex + factor(department)\nModel 2: 1/walktime ~ group + sex + factor(department)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    126 1.3190                              \n2    127 1.3709 -1 -0.051897 4.9574 0.02776 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInteractions\nTo test whether the treatment effect varies by sex we should test the group*sex interaction.\n\nlm6 &lt;- lm( data = walkingdata , 1/walktime ~ group*sex + age +  factor(department))\n\ngtsummary::tbl_regression(lm6)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ngroup\n\n\n\n\n\ncontrol\n—\n—\n\n\n\ntreated\n0.05\n-0.02, 0.12\n0.14\n\n\nsex\n\n\n\n\n\nF\n—\n—\n\n\n\nM\n-0.02\n-0.08, 0.04\n0.5\n\n\nage\n0.00\n-0.01, 0.00\n0.029\n\n\nfactor(department)\n\n\n\n\n\n1\n—\n—\n\n\n\n2\n-0.01\n-0.07, 0.04\n0.6\n\n\n3\n0.04\n-0.01, 0.09\n0.10\n\n\n4\n0.03\n-0.02, 0.08\n0.2\n\n\ngroup * sex\n\n\n\n\n\ntreated * M\n0.03\n-0.05, 0.11\n0.5\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\nanova( lm5 , lm6 )\n\nAnalysis of Variance Table\n\nModel 1: 1/walktime ~ group + age + sex + factor(department)\nModel 2: 1/walktime ~ group * sex + age + factor(department)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    126 1.3190                           \n2    125 1.3148  1 0.0042764 0.4066 0.5249\n\nlibrary(emmeans)\n\nWarning: package 'emmeans' was built under R version 4.1.3\n\nemmeans(lm6, pairwise ~ group | sex)\n\n$emmeans\nsex = F:\n group   emmean     SE  df lower.CL upper.CL\n control  0.379 0.0246 125    0.330    0.428\n treated  0.430 0.0260 125    0.379    0.482\n\nsex = M:\n group   emmean     SE  df lower.CL upper.CL\n control  0.359 0.0151 125    0.330    0.389\n treated  0.436 0.0150 125    0.407    0.466\n\nResults are averaged over the levels of: department \nConfidence level used: 0.95 \n\n$contrasts\nsex = F:\n contrast          estimate     SE  df t.ratio p.value\n control - treated  -0.0513 0.0346 125  -1.481  0.1411\n\nsex = M:\n contrast          estimate     SE  df t.ratio p.value\n control - treated  -0.0771 0.0211 125  -3.654  0.0004\n\nResults are averaged over the levels of: department \n\ntreatmentestimates &lt;- as.data.frame(confint(emmeans(lm6, pairwise ~ group | sex)$contrast))\n\n\ng1 &lt;- ggplot(treatmentestimates) + aes(x=sex, y=estimate, ymin=lower.CL, ymax=upper.CL) + \n  geom_point() + \n  geom_errorbar(width=0.2) + \n  geom_hline(yintercept = 0) + \n  theme_bw() + \n  labs(x=\"Sex\", y=\"Treatment effect\") \n  \ng1 \n\n\n\n\n\n\n\ng1 + coord_flip()\n\n\n\n\n\n\n\n\nWhat does this suggest. Does the treatment work in men and women?"
  },
  {
    "objectID": "repeats.html",
    "href": "repeats.html",
    "title": "Running bigger experiments is better than internal replication of smaller experiments",
    "section": "",
    "text": "TLDR: Internal replication of experiments (repeatedly running the same experiment within a lab) is unnecessary and wasteful, if your experiments are well designed to start with.\nExternal replication (later replication of ideas or experiments by different people in different contexts) is not discussed here.\nThis draft is long and needs some attention but I thought it was still valuable to post because there are some important ideas here. Any suggestions or comments welcome."
  },
  {
    "objectID": "repeats.html#introduction",
    "href": "repeats.html#introduction",
    "title": "Running bigger experiments is better than internal replication of smaller experiments",
    "section": "Introduction",
    "text": "Introduction\nAt the end of a long conversation on the importance of experimental design and why nobody likes talking to statisticians, a colleague finally dismissed the possibility of erroneous findings with “..but of course, we always repeat all our experiments three times anyway.” Up until now, in the interests of collegiality and to avoid being considered a troublemaker, I’ve let these sorts of provocations slide. But this was the third time in as many weeks that experiments in triplicate had been presented to me as obviously necessary despite having no apparent statistical basis.\nOnline searches revealed that ‘why do biologists replicate everything three times?’ is a question occasionally asked by newcomers to the field but never satisfactorily answered, and revealed a lot of confusion about the role of technical replicates, biological replicates, internal vs external replicates of experiments, when each is necessary and what each is for. Peer-reviewed literature, grey literature and textbooks seem also to be fairly quiet on the topic of this ‘internal’ replication, despite its (apparent?) widespread use.\nIn honesty it’s been a while since I thought about this, but I was recently motivated to revisit this topic by the MRC guidance for the design of animal experiments in funding applications, which states:\n\n[we require] an indication of the number of independent replications of each experiment to be performed with the objective of minimising the likelihood of spurious non replicable results. If there are no plans for studies to be independently replicated within the current proposal then this will need to be justified.\n\nIt’s not completely clear what the MRC mean by this, but if you want to argue against running repeats of experiments (and perform single better larger experiments instead) then this essay might help you with your justification. Below I explore some of the statistical implications of this strategy for error control, compared to a (more conventional?) approach of using a single larger (possibly blocked) experiment, and discuss what this reveals about how statisticians and biologists think about what scientific experiments can ever actually tell us.\nSpecifically, I discuss a strategy whereby a researcher repeats their experiment three times, and then considers a finding to have been demonstrated (positive or negative) if two of the three agree. I’ll call this the ‘2 of 3’ strategy."
  },
  {
    "objectID": "repeats.html#face-validity",
    "href": "repeats.html#face-validity",
    "title": "Running bigger experiments is better than internal replication of smaller experiments",
    "section": "Face validity",
    "text": "Face validity\nOn the face of it, replicating an experiment three times does seem like reasonable mitigation against the false positive or false negative arising from a single trial.\nThat is, if something does work on at least two out of three occasions, wouldn’t you be reasonably sure that it was going to work again or at least was reflecting an underlying biological truth? Whereas if your experiment failed at least two times out of three wouldn’t you think it likely that the anomalous positive was the result of some fluke or technical error?\nWell yes, and no.\nFalse positives or negatives due to biases or natural variation are inevitable, and are exactly what good experimental design and analysis are there to control. And if experiment 1 still does go wrong in an unknowable and uncontrollable way, surely there’s a good chance that experiments 2 and 3 will behave similarly? And if you are studying an effect that genuinely varies across subpopulations, or with time, place or the alignment of the stars, then you need to design and analyse your experiment accordingly.\nBut is there any actual harm in taking this approach, and what threshold for discovery are you in fact demanding of your experiments if you do this? This clearly needs some exploration; to understand what the statistical properties of this process of replication are, and how can we get the desired outcome (low error rates and good generalisability) in as efficient a way as possible."
  },
  {
    "objectID": "repeats.html#the-implications-of-repeating-three-times-on-error-rates",
    "href": "repeats.html#the-implications-of-repeating-three-times-on-error-rates",
    "title": "Running bigger experiments is better than internal replication of smaller experiments",
    "section": "The implications of repeating three times on error rates",
    "text": "The implications of repeating three times on error rates\nSo what are the implications of using your valuable time, energy and mice to repeat every experiment on three separate occasions, and then democratically deciding among the replicates which answer is the truth? Let’s work this out in a simple hypothetical case.\nSuppose you consult your statistician (humour me) and plan a well designed experiment to test a null hypothesis (H0) against an alternative (H1) such that the experiment has a power of 80% at a size of 0.05 (that is, you determine statistical significance at p&lt;0.05).\nThen you run this experiment three times, and only accept the result as true (reject H0) if at least two of the three results agree. Let’s ignore for now the possibility that findings could be statistically significant but in opposite directions.\nWhat are the error rates of this combined experimental procedure (the 2 of 3 strategy) as a whole?\n\nImplications for power (or type 2 error rate)\nFirst, let’s work out the power. This is probability of detecting an effect if that effect was in fact there (a true positive). Each individual experiment has, independently, a power of 80%. You need at least two of the three to show statistical significance.\nThe chance of this happening is the simple binomial probability:\n\\(0.8^3 + 3\\times 0.8^2\\times 0.2 = 0.896\\)\nSo if H1 is true you have a 89.6% chance of getting a positive result using this procedure, corresponding to a Type 2 error rate of 10.4%. This is an improvement on the 20% type 2 error rate for the single trial with 80% power.\n\n\nImplications for size\nWhat about false positive rate? If there is no effect (H0 is true) and your size is 0.05, that is you require p&lt;0.05 in each individual trial, then the chance of a false positive for your two in three procedure is again easy to find:\n\\(0.05^3 + 3×0.05^2×0.095 = 0.0075\\)\nSo a less than 1% chance of a false positive if H0 is true!\nSo this looks great! With three repeats (using the 2 from 3 strategy) you have reduced the risk of a false positive (if no real effect) from 5% to 0.75%, and increased power from 80% to as good as 90%!\nBut this has come at a cost of needing three times as much resource.\n\n\nEquivalent single experiment\nWas it worth it? How big would a single well-designed experiment have to be to achieve the same power and size? If your initial goal had been to design an experiment with power of 89.6% and size of 0.0075, what sample size would have been needed?\nWe can find this using a power and sample size calculator. (R code available)\nIf we work out the effect size needed such that any given N (our original single trial sample size) yields a power of 80% and size of 0.05, and then require that for the same effect size we want a power of 89.6% and a size of 0.0075, then the ratio of the original N to the revised N will reflect how much extra sample size we need.\nIt turns out that this same improvement in error rates is achieved with almost exactly 2N samples.\nSo by running one well designed experiment with twice the sample size instead of using three repeats, you will achieve the same error control with two thirds of the time and material you would otherwise have needed.\n\n\nOptional stopping\n“But wait!” you retort. “I would obviously only conduct experiment 3 if experiments 1 and 2 were in conflict. So I will not often use all three repeats.”\nThis is a good point. We can work out the probability of you actually needing all three experiments, hence what your expected average gain would be by using a single larger experiment instead.\nBut we do need to introduce another parameter, the probability that your supposed effect is real (that is, H1 is true). For any such probability, the chance that your first two experiments agree is shown below:\n\n\n\nThe probability of two experiments with 80% power and 5% size ‘agreeing’, depending on the probability that the underlying effect is true\n\n\nSo in all cases there’s actually a good chance (between 70% and 90%) that you’ll stop before needing the third trial. So the average cost of this 2 of 3 procedure is between 2.1 and 2.3 times N, the original sample size of each trial (so long as you stop after two trials that agree), compared with only 2.0 times N for running one large experiment. (Also, if we’re going to use optional stopping we can probably design a more efficient experiment in the first place).\nSo, all things considered, this isn’t an enormous improvement. In this case, where the original study was well designed, and if you stop after two trials with the same result then you’ll save roughly 10% of your resources by using a single larger experiment to get your error control rather than a strategy of 2 in 3 small experiments.\nSo in this case at least, using the 2 of 3 trials is a isn’t a terribly inefficient way to improve your Type 1 and Type 2 error rates. But the point does still stand that you get the same error rate control with 2N samples instead of needing the possibility of the third replicate, so long as you analyse them as one big experiment and not two smaller ones that may or may not agree.\n\n\nUnderpowered studies\nAll of the above supposed that the three individual independent experiments were each well designed to begin with. Suppose however that your initial experiment was underpowered, such that you only had a 50% chance of detecting a real effect at p&lt;0.05.\nIn this underpowered case your 2 of 3 strategy yields a power of 0.5, so there is no increase in power at all. Your Type 1 error is still 0.0075 though, so you have still improved type 1 error control by running three reps. The equivalent single stage experiment would need to have a sample size of 1.9 times the size of a single trial to achieve this error control criteria, and the chance of stopping after 2 is between 0.9 and 0.5 (say 0.7) depending on the chance that your hypothesis is actually true.\nSo your expected cost for the 2 of 3 strategy is 2.3N, while your known cost for a single larger study with the same error control is 1.9N. So you would save about a 0.4N or roughly a fifth of your expected resource by using a single experiment rather than having a strategy of needing 2 in 3 internal reps to be significant.\nIf your experiment is even more poorly powered, say at 30%, then the power of the 2/3 strategy is 0.3^3 + 30.3^20.7 = 0.216. So you lose power compared to a single trial. To get the same effect with a single study you’d need 1.75*N samples. Your probability of stopping early is around 0.75 (between 0.6 and 0.9), even though in a lot of cases you’d stop erroneously if the alternative hypothesis was true. So here the saving is 2.25 - 1.75 = 0.5N; again that’s just over a fifth of your expected size under the 2/3 strategy.\n\n\nReporting\nSo far I have only discussed error rates and statistical significance. The question of how you might report experimental results when you’ve used internal replication is complex.\nClearly, only showing one trial (say the first one, or a ‘representative’ trial) is an inefficient use of data, hides data, and is unacceptable. Selecting a ‘significant’ trial to support a significant finding is biased, while risking selecting a ‘non-significant’ trial to reflect an overall significant result is non-sensical. It is clear that reporting all of the data from all of the trials you have conducted is the only correct way to proceed, along with a clear description of the process by which you decided to stop conducting more studies, but this would be messy and far more difficult to interpret than simply conducting and reporting a single experiment in the first place. In short, unbiased and efficient reporting is very difficult if you are using internal replication at the level of the experiment."
  },
  {
    "objectID": "repeats.html#so-why-do-we-intuitively-have-more-trust-in-a-replicated-experiment",
    "href": "repeats.html#so-why-do-we-intuitively-have-more-trust-in-a-replicated-experiment",
    "title": "Running bigger experiments is better than internal replication of smaller experiments",
    "section": "So why do we intuitively have more trust in a replicated experiment?",
    "text": "So why do we intuitively have more trust in a replicated experiment?\nI have shown that running one big experiment is a better use of resources than two or three small ones, if the goal is to control false positive and false negative risk, and it seems obvious that a single large experiment is much easier to report than lots of small ones.\nBut casting aside the maths, at some human level I think that even would I intuitively have more faith in an idea if it is demonstrated more than once in small experiments, compared to being shown exactly once in a large experiment, even if that large experiment is statistically just as good as a set of small ones.\nSo what is going on? Can we model this (erroneous) thinking more formally to see where the problem lies?\nMy theory is as follows:\nScientists (perhaps people) don’t think about type 1 and type 2 error rates when designing, analysing or interpreting experiments. Nor do they typically see experiments as estimating some unknown population-level quantity. Instead, experiments are viewed as tests of whether an effect exists, reflected almost completely by the statistical significance of the outcome (that is whether the experiment worked or did not).\nIn a way this is natural, and our (wrong) notion of replicability reflects and reinforces this. The idea that if you replicate an experiment you should get the same result again and again is an intuitively attractive and often stated one, but it’s wrong. P-values are very unreliable. Two identical experiments can be perfectly consistent with each other even if they return very different p-values. In fact their results should always be different, because of the role of chance. (Replication results with p-values that are too similar are viewed with extreme suspicion by many).\nReplicability does not mean that the same experiment repeated should generate the same qualitative result. It does mean that the estimates arising from them should be close enough together such that their difference is attributable to natural variation in the outcome measures.\nFor example, if an experiment has 50% power under a ‘true’ effect, then statistical significance under H1 is a coin toss. The ‘results’ of successive experiments will be random and completely independent of each other, yet this is no reason to consider them inconsistent.\nBut under this (bad) interpretation, it’s easy to see why seeing successive positive results from repeated experiments is more appealing than seeing one positive from a single large experiment. It’s also easy to see how this reinforces the idea that experiments are likely to randomly ‘fail’ for mysterious methodological reasons, as under this way of thinking there is no other way to account for experimental inconsistency.\nSo, suppose we believe that our experiments should ‘work’ if the idea underlying them is correct, they should ‘not work’ if it is not, and that any deviation from this is caused by methodological error (which occurs with unknown probability independently of the size of the study). Under this model getting one success from one trial tells us almost nothing. It could be an error; it could be a true discovery and we have no way of saying which. Yet if we see two out of three trials work, then we may start to believe the more parsimonious explanation that one out of the three is a false negative rather than two out of three being false positives, which strengthens our belief in the finding.\nThe central fault(s) in this reasoning is the failure to appreciate that:\n\nAll experiments have both type 1 and type 2 error rates,\nthese errors do not occur because the experiments are faulty but occur due to chance,\nthese will naturally lead to apparently inconsistent findings, particularly if studies are underpowered, and\nlarge studies have smaller error rates, and are more reliable, such that we should update our beliefs by a greater amount based on larger studies compared to smaller ones.\n\nI agree that errors due to methodological failures can and do occur. However, splitting resources across two or three smaller trials, each of which is analysed separately, (ie internal replication) does not remove any of the randomness or methodological issues present in the alternative larger study, but does inflate their ability to derail our interpretations.\nConversely there is nothing to suggest that that a bias built into in a single study wouldn’t also be present in a replicate. In fact you’d expect that it would be. Good experimental design, good monitoring, analysis using models that can account for this variation (or random human error) and an appreciation of the ‘estimation’ paradigm for analysis is the appropriate response to this challenge, not the ‘brute force’ and frankly illusory safety net of internal replication.\nFor example, if we believe in a day-to-day variation in results that leads us to want to replicate across different days, then we can design a single blocked experiment and conduct it over multiple days, such that any ‘day’ effect is balanced over treatment conditions and a day-by-treatment effect might be estimated.\n\nSide notes and caveats\nThis is not to downplay the importance of biological replication. Biological replication is essential, and we should tend to emphasize biological replicates over technical replicates where possible. Where a source of variation exist, experiments should include replication across that source. But we should still always analyse our replicates together as part of a single experiment.\nIf we do want to retain the possibility of optional stopping, whereby we allow the possibility of getting more samples if uncertainty remains after an initial analysis, then we can, and I advocate this where appropriate, but it must be explicit in the original design, accounted for in analysis and acknowledged in the interpretation.\nExternal independent replication of published findings is also important, adds generalisability, and, given the state of the published literature, is more crucial than ever. But the aims of external replication are very different to those of internal replication.\nThis does not apply to internal replication of computational pipelines, which I think is a good idea (advocated here https://www.bitss.org/internal-replication-another-tool-for-the-reproducibility-toolkit/)\n\n\nDiscussion\nDrilling into the statistical properties of a strategy that relies on internal replication of experiments and the possible reasons for their intuitive appeal leads to clear practical suggestions and highlights some potential misunderstandings regarding the meaning of replicability and even the role of experiments and what should be inferred from them.\n\n\nSummary:\nA strategy of reporting an effect when two out of three independent trials rejects H0 at p&lt;0.05 has an overall type 1 error of 0.0075 (ie is equiv. to a single experiment reporting at p&lt;0.0075).\nWhether the power of this strategy is greater or less than the power of each individual experiment depends on whether each individual study is adequately powered in the first place. If each study is underpowered then the chance of true positive with this strategy is very low, if each individual trial is well powered then it is high.\nAn alternative strategy of running a single larger trial with the same error rates has a smaller expected sample size required in all situations tested, typically between 10 and 25% less, and is more predictable with respect to the resources needed. Using the ‘2 in 3’ strategy risks wasting more than 50% additional time and resources in the case that the third trial is needed to arbitrate between apparent inconsistencies from the first two.\nGood reporting of results from a single larger study is much easier than reporting based on a strategy of internal experimental repeats, and this uses available data much more efficiently.\nStudies are not inconsistent just because one reports a statistically significant difference while the other does not. Consistency of study findings should be judged based on whether the difference between their estimates is commensurate with the precision of each estimate. It is simply impossible to judge this with p-values alone.\nMajor sources of misunderstanding are the over-reliance on ‘statistical significance’ as the summary statistic for experimental results, which does not permit us to give more weight to larger trials compared to smaller ones, and the failure to consider type 2 errors as a source of apparent inconsistency between trials.\n\n\nRecommendations\nInternal replication of studies with identical conditions should be discouraged, where these would have been used, larger studies with more ‘biological’ replicates and stricter thresholds for error control can instead be safely recommended. These larger studies should be well designed, a good starting point for design would be using the proposed smaller studies as blocks within the larger design.\nWhere replicates have been made, we must not report findings from ‘a representative’ experiment, but should instead pool data using an appropriate method for pooling such as a mixed effects model or meta-analysis.\nStudies should be reported using estimates of differences along with the standard errors of differences or confidence intervals for differences, rather than relying on p-values.\n\n\nExisting literature (todo - fix this and add more)\nThere isn’t much literature on the value or otherwise of internal replications of experiments. I have certainly never seen a publication that formally explores and advocates this.\nThis author has roughly the same view I do. https://www.cell.com/trends/plant-science/fulltext/S1360-1385(99)01439-9.\nI couldn’t find anything on internal replication in the NC3RS website, except that the ARRIVE guidelines for reporting (rightly) insist that it is described when it was done.\nFor good introductory reading on types of replication and how to design and analyse studies with replicates appropriately, I would recommend Lazic (2016) Experimental Design for Laboratory Biologists.\nRecently with the ‘replication crisis’ there has been an instinctive reaction to do more internal replication in psychology. A blog post and an article show theoretically and empirically that this is not helpful.\nhttps://brainsidea.wordpress.com/2015/09/05/are-internal-replications-the-solution-to-the-replication-crisis-in-psychology-no/\nhttps://link.springer.com/article/10.3758/s13423-016-1030-9\nThere is a lot written more generally about the importance of replication in science.\nhttps://www.aje.com/arc/why-is-replication-in-research-important/"
  },
  {
    "objectID": "powerSolutionsJun2024.html",
    "href": "powerSolutionsJun2024.html",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "",
    "text": "The mortality rate in the treatment arm is is 34.9%. In the control arm is is 43.4%.\nThe estimate for treatment effect (risk difference) is -8.5%\nThe 95% confidence interval is -18.2 to +1.2 percentage points. This means that the treatment might plausibly improve mortality by 18.2 or worsen it by 1.2 percentage points.\nBecause the p-value is 0.06, at the confidence interval includes 0 (no treatment effect) the journal report is that there is no significant improvement.\nI would conclude there is a very good chance that the treatment is an improvement on the control, although this isn’t definitively proven, and it doesn’t increase survival by more than 20 percentage points.\nPersonally I would choose the new treatment (all else equal)\nI would do whatever I could to get the new treatment."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-1",
    "href": "powerSolutionsJun2024.html#exercise-1",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "",
    "text": "The mortality rate in the treatment arm is is 34.9%. In the control arm is is 43.4%.\nThe estimate for treatment effect (risk difference) is -8.5%\nThe 95% confidence interval is -18.2 to +1.2 percentage points. This means that the treatment might plausibly improve mortality by 18.2 or worsen it by 1.2 percentage points.\nBecause the p-value is 0.06, at the confidence interval includes 0 (no treatment effect) the journal report is that there is no significant improvement.\nI would conclude there is a very good chance that the treatment is an improvement on the control, although this isn’t definitively proven, and it doesn’t increase survival by more than 20 percentage points.\nPersonally I would choose the new treatment (all else equal)\nI would do whatever I could to get the new treatment."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-2",
    "href": "powerSolutionsJun2024.html#exercise-2",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 2",
    "text": "Exercise 2\nIf we recruit 10 patients and two have stool in their virus, then our estimate for the prevalence of virus in stool can be calculated by R as follows:\n\nbinom.test(x=2,n=10)\n\n\n    Exact binomial test\n\ndata:  2 and 10\nnumber of successes = 2, number of trials = 10, p-value = 0.1094\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.02521073 0.55609546\nsample estimates:\nprobability of success \n                   0.2 \n\n\nThe confidence interval is 0.03 (3%) to 0.56 (56%). So this is the plausible range for the true value given the data.\nWith 100 participants, if we observe 20 then our calculation would be calculated with:\n\nbinom.test(x=20,n=100)\n\n\n    Exact binomial test\n\ndata:  20 and 100\nnumber of successes = 20, number of trials = 100, p-value = 1.116e-09\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1266556 0.2918427\nsample estimates:\nprobability of success \n                   0.2 \n\n\nSo 13% to 30%.\nWe can tweak the sample size until we have a confidence interval that is 5% either side of the truth. N=250 gets us pretty close:\n\nN=250\nbinom.test(x=N*0.2 , n = N)\n\n\n    Exact binomial test\n\ndata:  N * 0.2 and N\nnumber of successes = 50, number of trials = 250, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1522402 0.2550261\nsample estimates:\nprobability of success \n                   0.2 \n\n\nNote this is the number of samples that we need data for, we might find that 10% or so of the participants do not give us valid data.\nWe can see that the sample size needed depends enormously on the precision we need for the estimate, and in this case we need to have some idea of what the answer is going to be."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-3",
    "href": "powerSolutionsJun2024.html#exercise-3",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 3:",
    "text": "Exercise 3:\nHere we can use the app to simulate some data\nhttps://georgemsavva.shinyapps.io/powerSimulator/\nor we can do it in code:\n\n# Make some assumptions\ncontrolMean = 500\ntreatmentEffect = -20\nstandardDeviation = 60\n\n# set our sample size\nN = 10\n\n# Simulate some data\ncontrolResults = rnorm(N , mean=controlMean, sd=standardDeviation)\ntestResults    = rnorm(N , mean=controlMean+treatmentEffect, sd=standardDeviation)\n\n# Plot the data\nplot(y=c(controlResults , testResults), \n     x=factor(rep(c(\"Control\", \"Test\"), each=N)), \n     ylab=\"iAUC\", xlab=\"Group\")\n\npoints(y=c(controlResults , testResults), \n       x=factor(rep(c(\"Control\", \"Test\"), each=N)))\n\n\n\n\n\n\n\n# Run a t-test\nt.test(controlResults, testResults)\n\n\n    Welch Two Sample t-test\n\ndata:  controlResults and testResults\nt = 2.9711, df = 15.048, p-value = 0.00949\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  23.00735 139.70020\nsample estimates:\nmean of x mean of y \n 540.2838  458.9301 \n\n\nIf we only had one person per group we couldn’t conclude anything, since we wouldn’t know if the difference is due to the person or due to the treatment.\nAs we get more samples we can become more sure that any differences we see are because of the treatment.\nThe estimate for effect will become more precise as we increase the sample size.\nUsing the app I can show that the width of the confidence interval will be about 40 units if there are approximately 60 participants in each group.\nIf we simulate more than one experiment, we can see that the p-value varies every time we run it. In reality we’ll only run one experiment, and we want to make sure that the probability of getting a p-value of less than 0.05 is high (given the assumption that our effect is real).\nIf we assume the true effect is 20 units, then to get 80% of trials significant we need about 150 participants per group."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-4",
    "href": "powerSolutionsJun2024.html#exercise-4",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 4",
    "text": "Exercise 4\nA study has a power of 80% if, given that a true effect exists, there is an 80% chance it will detect it as a significant difference.\nThis clearly depends on what the magnitude of the real effect is, how much variation there is etc.\nWe don’t learn a lot from studies that are underpowered. In these cases it might be better to look at the effect sizes and confidence intervals, since conclusions based on p-values will be very unreliable.\nA study is underpowered if it is too small to be able to detect meaningul effect sizes. For example, the ANDROMEDA-SHOCK study we looked at wasn’t able to detect an effect size of 12.5% as statistically significant. So it was underpowered for effects of that size or smaller."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-5",
    "href": "powerSolutionsJun2024.html#exercise-5",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 5",
    "text": "Exercise 5\nThe fault is ignoring the risk of errors, particularly false negatives, which because much more likely if the study size is small.\nStatement three should reas “If p&gt;0.05 we don’t have enough evidence to demonstrate an effect” or something like that. It could well be that an effect is present but our study could not detect it."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-6",
    "href": "powerSolutionsJun2024.html#exercise-6",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 6",
    "text": "Exercise 6\nTo find the sample size needed (per group) to estimate the precision of the risk difference to +-10 percentage points, we can use the precisely library. If you don’t like code you can use the Shiny app (https://malcolmbarrett.shinyapps.io/precisely/)\n\n# If you don't have the \n#install.packages(\"precisely\")\nlibrary(precisely)\n\nWarning: package 'precisely' was built under R version 4.1.3\n\nn_risk_difference(precision=0.2, \n                  exposed = 0.4,\n                  unexposed = 0.2, \n                  group_ratio = 1)\n\n# A tibble: 1 x 9\n  n_exposed n_unexposed n_total risk_difference precision exposed unexposed\n      &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1      154.        154.    307.             0.2       0.2     0.4       0.2\n# i 2 more variables: group_ratio &lt;dbl&gt;, ci &lt;dbl&gt;\n\n\nNow to find the possible precision with 15 patients per group:\n\nprecisely::precision_risk_difference(n_exposed=15, \n                                     exposed=0.4, \n                                     unexposed=0.2,\n                                     group_ratio = 1)\n\n# A tibble: 1 x 9\n  precision risk_difference n_exposed n_unexposed n_total exposed unexposed\n      &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1     0.640             0.2        15          15      30     0.4       0.2\n# i 2 more variables: group_ratio &lt;dbl&gt;, ci &lt;dbl&gt;\n\n\nSo with only 15 per group (a sample size of 30) we would have a confidence interval for the risk difference of 0.64! This is enormous in the context of trying to look for difference in the outcome from 20% to 40% recovery."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-7",
    "href": "powerSolutionsJun2024.html#exercise-7",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 7",
    "text": "Exercise 7\nFollowing the pwr.t.test video, we can find the sample size needed to test whether our bread leads to an improvement in iAUC of at least 20 units.\n\nlibrary(pwr)\n\nWarning: package 'pwr' was built under R version 4.1.2\n\npwr.t.test( n=NULL , d = 20/60 , power=0.8 )\n\n\n     Two-sample t test power calculation \n\n              n = 142.2462\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nTo find the smallest effect size detectable with 10 participants per group:\n\npwr.t.test( n=10 , d = NULL , power=0.8 )\n\n\n     Two-sample t test power calculation \n\n              n = 10\n              d = 1.324947\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis gives us a Cohen’s d of 1.3, which would correspond to a smallest detectable difference between treatments of about 78 units (given a standard deviation of 60 units)\n\npwr.t.test( n=100 , d = NULL , power=0.8 )\n\n\n     Two-sample t test power calculation \n\n              n = 100\n              d = 0.3981407\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nWith 100 participants per group we could detect a Cohen’s d of 0.4 or a difference in iAUC of about 24 units.\nNote there is a simple correspondence between the sample size and the Cohen’s d which will translate to every study of this design (simple two group parallel study).\nTo find the power:\n\npwr.t.test( n=10 , d=50/60 , power=NULL)\n\n\n     Two-sample t test power calculation \n\n              n = 10\n              d = 0.8333333\n      sig.level = 0.05\n          power = 0.4223915\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nMaking a power curve is a bit fiddly:\n\n# this line will extract the 'power' part of the output\npower = pwr.t.test( n=10:100 , # we can add a sequence instead of a single number here\n                    d=50/60 , \n                    power=NULL)$power\n\n# now we can plot:\nplot(x=10:100 , \n     y=power, \n     type=\"o\", \n     xlab=\"Sample size (per group)\", \n     ylab=\"power (%)\")\n\n# add some gridlines because I like gridlines\ngrid()"
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-8",
    "href": "powerSolutionsJun2024.html#exercise-8",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nMortality rates was 35% in the no treatment group.\nSuppose the smallest risk difference of interest is 10%. (this is up to you as investigator!)\n\n\n\nES.h(0.45, 0.35)\n\n[1] 0.2045252\n\n\nThis would correspond to an effect size of Cohorts H = 0.2\n\n\n\n\npwr.2p.test(h=0.2, n=NULL, power=0.8)\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.2\n              n = 392.443\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\nSo we would need 392 patients per group.\n\nSuppose we knew we could expect 200 patients per group. Then we would expect a minimum detectable effect size of:\n\n\npwr.2p.test(h=NULL, n=200, power=0.8)\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.2801491\n              n = 200\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\nSo the effect if 0.28, and the control rate is 35%, we can find the smallest detectable increase:\n\nES.h(.35 + 0.14, 0.35)\n\n[1] 0.2846913\n\n\nSo the smallest change detectable with 80% power is about 14%. If the expected increase in treatment effect is smaller than this then the study needs to be bigger."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-9",
    "href": "powerSolutionsJun2024.html#exercise-9",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nFor a paired t-test, we would use the type=\"paired\" option.\n\n\n\npwr.t.test(n=NULL, d = 20 / (sqrt(2)*30), power = 0.8 , type=\"paired\" )\n\n\n     Paired t test power calculation \n\n              n = 37.28621\n              d = 0.4714045\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nWe need ~40 participants, each of which will provide two measurements.\nIn the previous study, we would have needed 142 samples per group!\n\npwr.t.test(n=NULL, d = 20 / 60, power = 0.8  )\n\n\n     Two-sample t test power calculation \n\n              n = 142.2462\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSo pairing has helped a lot!\n\nLook at how the sample size depends on the critical threshold for p-value.\n\n\npwr.t.test(n=NULL, d = 20 / (sqrt(2)*30), power = 0.8 ,sig.level = 0.01, type=\"paired\" )\n\n\n     Paired t test power calculation \n\n              n = 55.90111\n              d = 0.4714045\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\npwr.t.test(n=NULL, d = 20 / (sqrt(2)*30), power = 0.8 ,sig.level = 0.001, type=\"paired\" )\n\n\n     Paired t test power calculation \n\n              n = 82.24009\n              d = 0.4714045\n      sig.level = 0.001\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*"
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-10",
    "href": "powerSolutionsJun2024.html#exercise-10",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nDesign 1:\n\npwr.t.test(n=NULL , d=20/30, power=0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 36.30569\n              d = 0.6666667\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nDesign 2:\nChange in concentration will have sd:\n\nsd_change = sqrt(2) * sqrt(1-0.6) * 30\n\nSo the sample size needed is:\n\npwr.t.test(n=NULL , d=20/27, power=0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 29.60082\n              d = 0.7407407\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOnly slightly fewer than design 1\n\n\nDesign 3:\nGiven the ICC of 0.6, we would expect to explain 60% of variance by adjusting for the baseline value. We can use the Superpower R package to find how this affects the sample size needed, or adjust the standard deviation directly and use the regular pwr.t.test function:\n\nlibrary(Superpower)\n\nWarning: package 'Superpower' was built under R version 4.1.3\n\npower_oneway_ancova(mu=c(0,20),n=NULL,n_cov=1,sd=30,r2=0.6, alpha_level = 0.05, beta_level = 0.2)\n\n\n     Power Calculation for 1-way ANCOVA \n\n            dfs = 1, 29\n              N = 32\n              n = 16, 16\n          n_cov = 1\n             mu = 0, 20\n             sd = 30\n             r2 = 0.6\n    alpha_level = 0.05\n     beta_level = 0.1916029\n          power = 80.83971\n           type = exact\n\npwr.t.test(n=NULL, d=20 / (sqrt(0.4)*30), power=0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 15.15109\n              d = 1.054093\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis suggests we only need 16 participants per group!\nSo this is a much more powerful approach than using the change score. How much sample size we can save depends on how well we can explain the outcome value with the baseline value, that is what is the ICC.\n\n\nDesign 4: Cross-over study\nWe saw in the last example that a cross-over trial where we use the same participant twice can be much more efficient:\n\npwr.t.test(n=NULL, d=20 / sd_change, power=0.8, type=\"paired\")\n\n\n     Paired t test power calculation \n\n              n = 16.15352\n              d = 0.745356\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nHere we would need 16 participants, but each would have to undergo two treatments and assessments. This might not be feasible."
  },
  {
    "objectID": "powerSolutionsJun2024.html#exercise-11",
    "href": "powerSolutionsJun2024.html#exercise-11",
    "title": "Solutions and notes for power and sample size course exercises",
    "section": "Exercise 11:",
    "text": "Exercise 11:\nTo power an ANOVA is a little more complicated, because the hypothesis is more complex. We have different effect sizes between different treatments, and we might be interested in the global p-value or in particular contrasts.\nThe assumptions for the power calculation will be the same as the assumptions for any ANOVA."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R and RStudio for Statistics",
    "section": "",
    "text": "This site hosts my course notes, presentations and data files accompanying my courses on R and RStudio.\nThere are also some general notes on statistics that I have created for myself (for exploring different ideas) or to help others.\n\nNBI/JIC MSc Introduction to R\n\nDay 1\nDay 2/3 (PhD version)\n\nHow many samples do I need?\n\nCourse notes\n\nStatistics notes\n\nA note on graphing\nDon’t dichotomise your data\nSimple alpha and beta diversity simulation\nSelection bias (Berkson bias) in observational studies\nSampling from clustered data in observational studies\nRun bigger experiments instead of internal replications of smaller experiments\n\nNotes on using R\n\nTidyverse, data.table and others for making descriptive tables\nUsing packages, functions and pipes\nAn analysis of the Iris dataset\n\n\nOlder stuff\n\nJIC MSc Workshop (2021/2022)\n\nTutorial"
  },
  {
    "objectID": "hi.html",
    "href": "hi.html",
    "title": "Untitled",
    "section": "",
    "text": "Hi!"
  },
  {
    "objectID": "day3.html",
    "href": "day3.html",
    "title": "Intro to R day 3 - Notes on Graphing",
    "section": "",
    "text": "Plotting data is an essential part of data analysis and reporting. Your plots communicate your results, and a good plot can be the difference between a successful and unsuccessful communication.\nIn this session we’ll think about how to plot your data, what makes a good vs a bad plot, and illustrate some concepts for plotting using R.\nLearning objectives:"
  },
  {
    "objectID": "day3.html#descriptive-plots",
    "href": "day3.html#descriptive-plots",
    "title": "Intro to R day 3 - Notes on Graphing",
    "section": "Descriptive plots",
    "text": "Descriptive plots\nBefore we model data we should visualise it. This crucial first step is often omitted in analyses and reports. Consider the follow example, that I have designed after a real analysis I was asked to comment on, where an outcome was compared between two treatment conditions:\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nset.seed(12345) # Set seed is used to ensure that 'random' samples will come out the same each time.\nN=12\ny &lt;- c(runif(N), c(runif(N-2), 4.5,6))\nx &lt;- factor(rep(c(1,2), each=N))\nggplot(data.frame(Group=x,y), aes(x,y)) + \n  stat_summary(geom=\"col\",width=0.5, fill=\"red\") + \n  stat_summary(geom=\"errorbar\", width=0.2) + \n  theme_bw()+ stat_compare_means(label.y = 2)\n\n\n\n\n\n\n\n\nOn the face of it, it looks like the outcome is higher in group 2 than group 1. If this is all you see then this is surely the conclusion you would come to.\nBut why is the p-value so high? And how well do you feel like you understand the data from this graph?\nConsider now how you would interpret this:\n\nlibrary(ggplot2)\nggplot(data.frame(Group=x,y), aes(x,y)) + \n  geom_point() + \n  theme_bw()\n\n\n\n\n\n\n\n\nQuite differently? At least with this second visualisation we can see the data and draw our conclusions about what is going on directly.\nAlso consider, the first graph only shows you four values, two means and two standard errors (if that is what they are, I never actually told you). It’s a waste of ink. But how often do you see this first presentation in the scientific papers your read? When I see one of these I can’t help but wonder what horrors it is hiding.\nThe second graph tells us quite a lot. It tells us to maybe check our outlying data points, or to try transforming our data before analysis. We might even conclude that the two groups are more-or-less the same, except for two individuals, which may well be a real effect of treatment that is limited to specific individuals. In any case we learn a lot.\nSo our most important rule is: always plot your data, and not just summaries of the data.\nIdeally, if we have a dataset with several variables, we will start by making something like a graph matrix showing every variable against every other variable. This will help us identify any potential problems or outliers in 2-dimensions.\nThe ggpairs function in the GGally package gives us a grid layout showing all of these combinations. I think its a reasonable summary.\n\nlibrary(readxl)\nlibrary(GGally)\nwalkingdata &lt;- read_excel(path=\"walkingspeed_day2.xlsx\", sheet=\"cleaned\")\nwalkingdata$department &lt;- factor(walkingdata$department)\nwalkingdata$time &lt;- as.numeric(walkingdata$time)\nwalkingdata$time[walkingdata$time&gt;100] &lt;- NA\nwalkingdata$time[walkingdata$time&lt;1] &lt;- NA\nggpairs(walkingdata, columns=1:6)\n\n\n\n\n\n\n\n\nNote I’m not wasting a lot of time making this graph pretty. It’s for me only, I don’t care what it looks like, I just want to see the data as quickly and as effectively as possible. I’ve also left patient ID in here as a variable, there’s no harm doing this, and it might show me if any data errors have occurred.\nDoes this graph meet the objectives that we set out for our ‘descriptive’ analyses above?"
  },
  {
    "objectID": "day3.html#inferential-graphs",
    "href": "day3.html#inferential-graphs",
    "title": "Intro to R day 3 - Notes on Graphing",
    "section": "Inferential graphs",
    "text": "Inferential graphs\nWhile the descriptive graphs tells us about the data, it doesn’t tell us much about the comparison we are interested in making.\nTo think about how we could graph that, first think about what exactly it is we are trying to show.\nLets go back to our linear model from last time:\n\nmodel1 &lt;- lm(data=walkingdata, log(time) ~ group + sex + age + department)\ngtsummary::tbl_regression(model1)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\ngroup\n\n\n\n\n\ncontrol\n—\n—\n\n\n\ntreat\n-0.18\n-0.30, -0.07\n0.003\n\n\nsex\n\n\n\n\n\nF\n—\n—\n\n\n\nM\n0.04\n-0.10, 0.19\n0.6\n\n\nage\n0.01\n0.00, 0.03\n0.021\n\n\ndepartment\n\n\n\n\n\n1\n—\n—\n\n\n\n2\n0.11\n-0.07, 0.28\n0.2\n\n\n3\n-0.09\n-0.26, 0.07\n0.3\n\n\n4\n-0.06\n-0.23, 0.12\n0.5\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWhat is the key information here that we need to communicate?\nWe should say what our estimate of the treatment effect is, and how sure we are of this. That is the ‘actionable’ result from this work, and that is what we want people to take away from our analysis. The fact of the treatments being ‘significantly’ different is interesting but not enough on its own. So from our analysis we should be trying to communicate the estimate of treatment effect, the standard error of treatment effect, and potentially the p-value and a confidence interval for the difference.\nThe mean time in each group is perhaps interesting descriptively, so people can understand our sample. It’s hard to see why the standard error within each group should be of interest.\nSo - do I even need a graph? Should this summary statistic, mean difference = 0.19 (standard error=0.06; p=0.0015) be enough? Recall that this was calculated on a logarithmic scale, so it’s probably best to exponentiate it and report a ratio:\n\nlibrary(emmeans)\nem1 &lt;- emmeans(model1, \n               trt.vs.ctrl~group, \n               type=\"response\")$contrast\nconfint(em1)\n\n contrast        ratio     SE  df lower.CL upper.CL\n treat / control 0.832 0.0499 124    0.739    0.937\n\nResults are averaged over the levels of: sex, department \nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nSo we could say “times for treated group were 83% of the times for the control group (95% CI=74% to 93%)”. Or: “treatment improved times by 17% (95% CI=7-26%).”\nIs this enough? I think so, if combined with a visual summary that persuades us that the model is reasonable, and we have the descriptive graph that shows us this difference in the context of the variance in the data.\nCompare this with the traditional presentation:\n\nlibrary(ggpubr) # includes the stat_compare_means function\nggplot(walkingdata, aes(x=group, y=time)) + \n  stat_summary(geom=\"col\", width=0.5, fill=\"red\") + \n  stat_summary(geom=\"errorbar\", width=.2) + \n  scale_x_discrete(na.translate=FALSE) + \n  theme_bw() + \n  stat_compare_means()\n\n\n\n\n\n\n\n\nor better, but still not so informative:\n\nlibrary(ggpubr)\nlibrary(ggbeeswarm)\nggplot(remove_missing(walkingdata), aes(x=group, y=time)) + \n  geom_beeswarm(col=\"grey\") + \n  stat_summary(geom=\"errorbar\", width=.2) + \n  stat_summary(geom=\"point\", width=.2) + \n  theme_bw()+ scale_y_log10() + \n  stat_compare_means(method=\"t.test\",\n                     label = \"p.signif\",\n                     comparisons=list(c(\"treated\", \"control\")))\n\n\n\n\n\n\n\n\nCould you say what the treatment effect is by looking at this graph? How sure would you be about it? Also, how would you represent a model other than a simple comparison of means (for example, the multiple linear regression model that we estimated). By confusing the descriptive with the inferential graph we are limiting our ability to conduct the appropriate statistical analysis.\nAn even better plot might be a Gardner-Altman estimation plot. It is unusual, but implemented in the R package dabestr.\n\nlibrary(dabestr)\nlibrary(ggplot2)\nwalkingdata$logtime &lt;- log(walkingdata$time)\nwalkingdata &lt;- remove_missing(walkingdata)\nload(walkingdata, x=group, y=logtime, idx=c(\"control\", \"treat\")) |&gt; \n  mean_diff() |&gt; \n  dabest_plot(swarm_label=\"log(time)\")\n\nNote that this plot allows us to see the distributions of the data in both groups, but crucially also an estimate of the difference between the groups and the uncertainty around that. A downside is that we cannot accommodate the adjustment for sex or department that was included in the model.\nIf we did want to show the adjusted estimates, and compare estimates with each other as well as a null hypothesis, then we could use a forest plot. For example, suppose we wanted to compare several groups, say walking speed over department, then a visualisation like this might be useful.\n\nlibrary(emmeans)\nlibrary(broom)\nem2 &lt;- emmeans(model1, \n               pairwise~department, \n               type=\"response\")$contrast\ntd &lt;- tidy(em2, conf.int = TRUE)\ntd\n\n# A tibble: 6 x 11\n  term       contrast  null.value ratio std.error    df conf.low conf.high  null\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 department departme~          0 0.897    0.0797   124    0.712      1.13     1\n2 department departme~          0 1.10     0.0933   124    0.881      1.37     1\n3 department departme~          0 1.06     0.0933   124    0.843      1.33     1\n4 department departme~          0 1.23     0.100    124    0.991      1.52     1\n5 department departme~          0 1.18     0.102    124    0.944      1.48     1\n6 department departme~          0 0.964    0.0798   124    0.777      1.20     1\n# i 2 more variables: statistic &lt;dbl&gt;, adj.p.value &lt;dbl&gt;\n\nggplot(td) + \n  aes(x=contrast, y=ratio, ymax=conf.high, ymin=conf.low) + \n  geom_pointrange() + \n  coord_flip() + \n  geom_hline(yintercept=1) + \n  theme_bw() + \n  labs(y=\"Ratio of average walking task times with Tukey adjusted 95% confidence intervals\")\n\n\n\n\n\n\n\n\nWhat might improve this graph here? Perhaps an annotation for which side of the graph corresponds to faster and which to slower. It is possible to work it out, but if you can avoid that cognitive load for your readers then that would be better!"
  },
  {
    "objectID": "day3.html#plotting-with-ggplot",
    "href": "day3.html#plotting-with-ggplot",
    "title": "Intro to R day 3 - Notes on Graphing",
    "section": "Plotting with ggplot",
    "text": "Plotting with ggplot\n\nSlides on ggplot including links to other tutorials are here: Graphing - day3.pptx"
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "Intro to R for statistics, Day 2/3",
    "section": "",
    "text": "Today’s worksheet introduces you to a real dataset.\nThe tasks you will cover are:\nYou can attempt the worksheet by following the material on your own on day 2.\nOn day 3 we will run through the work discussing any issues related to R or statistics that arise."
  },
  {
    "objectID": "day2.html#the-dataset",
    "href": "day2.html#the-dataset",
    "title": "Intro to R for statistics, Day 2/3",
    "section": "The dataset",
    "text": "The dataset\nThe data are from a randomised clinical trial of a new rehabilition intervention (compared to standard care) aimed at improving the walking speed of people recovering from stroke.\nBetter walking speed is a good indicator of general stroke recovery.\nWe have recorded:\n\nThe age and sex of each participant,\nThe treatment allocation,\nThe hospital department from which they were recruited and\nTime they take to complete a walking task after their treatment.\n\nOur research questions are:\n\nDoes the new treatment improve walking speed compared to control treatment?\nBy how much does it improve, and how certain are we of this?\n\nThe dataset can be found at walkingspeed_day2.xlsx\n\nPowerpoint slides to support this material are at:\n\nGraphing - day3.pptx\nday3_data_lm.pptx"
  },
  {
    "objectID": "day2.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "href": "day2.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "title": "Intro to R for statistics, Day 2/3",
    "section": "Make sure the data looks OK and is in the right place on your computer",
    "text": "Make sure the data looks OK and is in the right place on your computer\nBefore we dive in and import it, we need to make sure our data is in a sensible place.\n\nOpen RStudio and switch to the project you created in day 1, or start a new project if necessary.\nThen, save the example data walkingspeed_day2.xlsx for this tutorial into your project folder. Check that it has appeared in the ‘files’ pane in RStudio.\nStart a new script file that will eventually include all of the commands we need to import, clean, visualise and analyse the data.\nNow open the dataset in Excel and explore the file so that you understand what is there."
  },
  {
    "objectID": "day2.html#read-the-help",
    "href": "day2.html#read-the-help",
    "title": "Intro to R for statistics, Day 2/3",
    "section": "Read the help!",
    "text": "Read the help!\nWe are nearly ready to import our data. But before using a new function its always good to read its documentation.\nR and R packages are not as self-explanatory as other software, and so you should expect to spend a fair amount of time, particularly as you are learning R, reading documentation, vignettes, blogs, etc on what R can do, which packages exist, and how to use them.\nThe read_excel() function has a few different options so first we should look at the help file:\n\nlibrary(readxl)\n?read_excel # where does the helpfile appear?\n\nMake sure to check:\n\nDescription what does the function do,\nUsage what is the syntax\nArguments detail of what all the options mean\nValue what do I get when I run this\nExamples (usually very helpful)\n\nNote from the help file that read_excel() can extract data from different sheets and ranges of an Excel workbook, can use or ignore column names, and allows you to specify the type of data (numeric, dates, text etc) if you want to, or leave it to R to guess.\nMany R packages also have vignettes or websites including simpler guides to their use in specific cases. readxl has a website that you might find helpful: https://readxl.tidyverse.org/\nNow we’ll load the data. We want to use the ‘walking speed’ data from the walkingspeed.xlsx spreadsheet.\n\nOpen the spreadsheet using Excel and find this sheet. The data we want is in the sheet called ‘day2’.\n\nFrom the read_excel() help file we can deduce the syntax to load this data into R:\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.1.3\n\nwalkingdat &lt;- read_excel(path=\"walkingspeed_day2.xlsx\", sheet=\"day2\")\n\nThis line assumes that the file walkingspeed_day2.xlsx is in the current working directory (you can check what the current working directory is with getwd(). The current working directory is also shown just above the R console window. You can see the files in the current working directory in the ‘Files’ tab on the bottom right of the RStudio window. When you create or load a project RStudio will set the working directory to the root of the project directory.\nSo this line calls the read_excel() function, with the arguments path, sheet set. The other arguments will be set to their default values, which you can see from the help file.\nWe could have set the range of the data in the spreadsheet (I usually do this for safety), but read_excel() can figure it out automatically most of the time; by default it picks the biggest continuous chunk of data starting in the top left of the sheet.\nNow you should have a ‘data frame’ object called walkingdat in your environment, which includes the data from the Excel sheet ready to process and analyse.\nOur workflow now is:\n\nClean and code\nVisualise\nDescribe\nModel\nDiagnose model\nInterpret"
  },
  {
    "objectID": "day2.html#converting-between-types",
    "href": "day2.html#converting-between-types",
    "title": "Intro to R for statistics, Day 2/3",
    "section": "Converting between types",
    "text": "Converting between types\nYou will have noticed that one of the numerical variables, age, has been loaded as character variable and not a numeric. We cannot do a quantitative analysis with a character variable!\nWhy do you think the variable is imported as character and not numeric?\nWe can convert age to a numeric using the function as.numeric().\n\n# Tidyverse version:\nwalkingdat &lt;- walkingdat |&gt; mutate(age = as.numeric(age))\n\n# Base R version:\nwalkingdat$age &lt;- as.numeric(walkingdat$age)\n\nWhat warning message did you get when you converted this variable? What does it mean?"
  },
  {
    "objectID": "day2.html#checking-the-data",
    "href": "day2.html#checking-the-data",
    "title": "Intro to R for statistics, Day 2/3",
    "section": "Checking the data",
    "text": "Checking the data\nWe mustn’t assume that all our data had been entered correctly.\nHow might we check on the age and time variables to see if the distribution looks OK?\nWe could make some descriptive statistics and basic plots!\n\nhist(walkingdat$age)\nhist(walkingdat$time)\nrange(walkingdat$time, na.rm=TRUE)\n\nWhat do you notice from here?"
  },
  {
    "objectID": "day2.html#dealing-with-outliers",
    "href": "day2.html#dealing-with-outliers",
    "title": "Intro to R for statistics, Day 2/3",
    "section": "Dealing with outliers",
    "text": "Dealing with outliers\nIt looks like there are some unreasonably high and low values of walking time.\nWe can make another graph of walking speed against age, this time on a logarithmic scale so both the extreme high and extreme low points are visible, to see what is going on.\n\n## ggplot2 version\nlibrary(ggplot2)\nggplot(walkingdat) + aes(x=age, y=time) + geom_point() + scale_y_log10()\n\n## base R version\nplot(walkingdat$age, walkingdat$time, log=\"y\")\n\nIt seems there are some values for time that are likely to be technical errors. We can remove these values (set them to missing) in a few different ways:\n\n## base R method\nwalkingdat$time[walkingdat$time&lt;0.1] &lt;- NA\nwalkingdat$time[walkingdat$time&gt;100] &lt;- NA\n\n## Using mutate and ifelse\nwalkingdat &lt;- walkingdat |&gt; mutate(time = ifelse(time&lt;0.1,NA,time))\nwalkingdat &lt;- walkingdat |&gt; mutate(time = ifelse(time&gt;100,NA,time))\n\n## the 'pure' tidyverse way with case_when is a bit clunky.\n## look up 'case_when()' to understand this line\nwalkingdat &lt;- walkingdat |&gt; mutate( time = case_when(time&lt;0.1 ~ NA_real_ , \n                                                     time&gt;100 ~ NA_real_, \n                                                     TRUE ~ time) )\n\n# Now check the distribution of time again.\nhist(walkingdat$time, breaks=100)\nrange(walkingdat$time, na.rm = TRUE)\n\nNow we have a cleaned dataset in our environment you can proceed with our visualization and analysis, using the functions you learned in day 1.\nIf you had trouble with the cleaning exercise then use the version of the dataset found in the cleaned tab of the spreadsheet:\n\nwalkingdat &lt;- read_excel(path=\"walkingspeed_day2.xlsx\", sheet=\"cleaned\")"
  },
  {
    "objectID": "day1.html",
    "href": "day1.html",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "",
    "text": "This course will introduce\n\nR and RStudio statistical software\nExamples of performing common tasks in scientific data analysis\n\nLoading data\nDescriptive analysis\nBasic testing and modelling\nMaking graphics\n\nWhere to go for further support\n\nThe aim is to become familiar with the R/RStudio environment and some common functions and workflows. This will enable you to learn the specific functions that you need on your own or with further training.\nPowerpoint slides for today’s session are here:\n\nDay 1 slides\n\nThis handout was written in RStudio using the Quarto document preparation system. The source code is here: day1.qmd\n\n\nWe will focus on the tasks used in a typical analysis of a single scientific dataset, mirroring the tasks usually conducted in other statistical software.\n\n\nDay 1: R and RStudio basics\n\nFamiliarity with R and RStudio\nThe R language and R scripts\nExploring data and calculating descriptive statistics using example data\n\nDay 2/3: Importing data, cleaning, coding and typical analysis workflow\n\nRevise and consolidate day 1 learning\nLoading and wrangling data\nSimple hypothesis tests\nEstimating, diagnosing and interpreting linear models\nMaking graphs using the ggplot2 package"
  },
  {
    "objectID": "day1.html#learning-objectives",
    "href": "day1.html#learning-objectives",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "",
    "text": "We will focus on the tasks used in a typical analysis of a single scientific dataset, mirroring the tasks usually conducted in other statistical software.\n\n\nDay 1: R and RStudio basics\n\nFamiliarity with R and RStudio\nThe R language and R scripts\nExploring data and calculating descriptive statistics using example data\n\nDay 2/3: Importing data, cleaning, coding and typical analysis workflow\n\nRevise and consolidate day 1 learning\nLoading and wrangling data\nSimple hypothesis tests\nEstimating, diagnosing and interpreting linear models\nMaking graphs using the ggplot2 package"
  },
  {
    "objectID": "day1.html#what-are-r-and-rstudio",
    "href": "day1.html#what-are-r-and-rstudio",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.1 What are R and RStudio?",
    "text": "2.1 What are R and RStudio?\nR is a free and open source statistics package, initially developed during the 1990s, and that has now become the world’s most widely used and comprehensive statistical software. R calls itself a programming language and environment for statistic computing.\nThat is, ‘R’ refers both to the software itself and the programming language that you use to interact with it.\nRStudio is a free open source integrated development environment (IDE) for R that makes working R much easier. Most R users use RStudio and we recommend using RStudio for new users.\nThe great strength of R is in its contributed packages, these are community written add-ons that make R much more powerful and easy to use. We will introduce some commonly used packages for data management, analysis and graphing during this course."
  },
  {
    "objectID": "day1.html#getting-r-and-rstudio",
    "href": "day1.html#getting-r-and-rstudio",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.2 Getting R and RStudio",
    "text": "2.2 Getting R and RStudio\n\nIf you are using a PC in the JIC IT training suite it will already have an up-to-date version of R and RStudio.\nFor other NBI managed devices you can install R and RStudio from the NBI software catalogue.\nIf you want to install R and RStudio on your own device:\n\nDownload and install the latest version of R from https://cran.r-project.org/\nThen download and install RStudio from https://www.rstudio.com/"
  },
  {
    "objectID": "day1.html#starting-rstudio",
    "href": "day1.html#starting-rstudio",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.3 Starting RStudio",
    "text": "2.3 Starting RStudio\nStart RStudio. It will detect your installation of R, and you should see a screen like this:\n\n\n\nFigure: RStudio Window\n\n\nOn the left is the console window, where you type commands and see output. The windows on the right hold various useful tabs, including your data, graphs, help files, and your command history."
  },
  {
    "objectID": "day1.html#check-r-and-rstudio-are-working-run-your-first-command",
    "href": "day1.html#check-r-and-rstudio-are-working-run-your-first-command",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "2.4 Check R and RStudio are working, run your first command",
    "text": "2.4 Check R and RStudio are working, run your first command\n\nClick in the console window and type:\n\n\n1+2\n\nPress return on your keyboard. You should see:\n\n\n[1] 3\n\n\n\nTry a few other mathematical functions at the R console (eg):\n\n\nsin(pi/2)\nlog(10)\nexp(2)\n1e4\n1/0\n\nDo you understand the output from each of these commands?"
  },
  {
    "objectID": "day1.html#making-a-script",
    "href": "day1.html#making-a-script",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "3.1 Making a script",
    "text": "3.1 Making a script\nWe could do everything by typing commands into the console window as we have already seen, but this is not good if we want to remember or repeat something we have done, or share it with others.\nSo instead we will type our commands into files called R scripts and run the commands from there. With a script you can run and re-run bigger analyses that chain together all the functions you need for data loading, cleaning, analysing, reporting etc. R scripts have the file extension\nUsing scripts mean we can develop complex analyses, and that when we come back to them later, eg if something changes in our data that means we need to redo everything, or we want to tweak something in our analysis because of a reviewer’s comment, we can easily do this.\n\n\n\n\n\n\nAlways use a project, and keep scripts tidy!\n\n\n\nIt is good to keep a separate R script for each analysis that you do, such that each starts with the functions to load the required data, then do any cleaning or coding that is necessary, then to perform and report the data analysis.\nIf you start making more complex projects you may want to write separate scripts for each of these elements.\n\n\n\n3.1.1 Look at some example R scripts\nAn example R script, annotated with comments, is in the files that accompany this handout. Let’s look at it now:\n\nExample R script\n\nLoad this script in R studio and practice running lines one at a time, or ‘sourcing’ the whole script all at once (see the section below for how to do this).\nThere is a second R script, including an analysis of the iris dataset that we’ll use today:\n\nIris analysis R script\n\nThe iris script has also been ‘spun’ into a report, which you can see here:\n\nIris report\n\nWe won’t cover making reports, websites or presentations in this course, but you should look into the Quarto system if you are interested.\n\n\n3.1.2 Make a new script\n\nMake a new script. Click on File → New File → R Script in the main RStudio window. An empty file will appear in the top-left pane.\nSave your script with a sensible filename (even though it is empty). Having unsaved scripts is a bad idea, RStudio is sometimes unstable and while it will try to recover unsaved work it is not always guaranteed to. Get into the habit of saving your scripts regularly. Make sure it has the file extension .R\nPut some of the mathematical functions that you have already tried into your script, with one on each line.\n\nYou can now run code from scripts in several ways. Try each of these:\n\nPress ‘run’ or type Ctrl+Enter on the keyboard, RStudio will send the line that the cursor is on to the R console and will run it.\nIf you highlight an area of the script and then hit ‘run’ (or press Ctrl+Enter) then RStudio will send all the highlighted code to the R console.\nIf you save the file, then press ‘source’, R will load the file from disk and run all the commands from that file in sequence.\n\nIf you have your raw data saved, and you keep your scripts, then you don’t need to save your results or any of the variables that you generated or modified during your analysis. So long as the original data doesn’t change, running the script will reproduce all of your analysis and output. This is usually a better way of working than trying to save your environment with all of your results and tables in.\nSee also:\n\nhttps://kdestasio.github.io/post/r_best_practices/\nhttps://r4ds.had.co.nz/workflow-projects.html\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\nhttps://rstats.wtf\n\nfor more information on using projects and scripts\nFor more complex projects you might be interested in using a pipeline tool such like targets to help keep your work organised, particularly if you are running analyses in multiple stages with some elements taking a long time.\n\nhttps://books.ropensci.org/targets/"
  },
  {
    "objectID": "day1.html#functions",
    "href": "day1.html#functions",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "4.1 Functions",
    "text": "4.1 Functions\nEverything in R is done by executing ‘functions’. When you typed 1+2 at the console above you were calling the + function, with 1 and 2 as its arguments, and the result was printed in the console window.\nSimple mathematical functions can be written using standard notation in this way (eg 1+2 or 3/4) but functions are more commonly called by their name, with their arguments in brackets, separated by commas. For example, to get say the logarithm (base 10) of 100, we would type\n\n### Try this directly in the console, and by running it from your new script.\nlog(x=100, base=10)\n\n[1] 2\n\n\nHere, log is the name of the function, with x and base its arguments. The result is the value of the function (the value is what is returned).\n\nTry each of the following commands. Do you understand what they do any why?\n\n\n### From now on, keep everything you try in a script file.\n\nlog(x=100, base=10)\nlog(x=100)\nlog(base=10, x=100)\n\nlog(100,10)\nlog(10,100)\n\n1000 |&gt; log()           #  Note the use of the pipe operator |&gt; as an alternative way to call a function. \n100 |&gt; log(base=10)\n\nlog()\nlog\n\n\n4.1.1 Getting help\nLet’s find out why the log command we used in the last section worked the way it did. In the console, type:\n\n?log\n\nto read the help file for the log function. All R help files are structured in a similar way, so it’s useful to understand this now for a simple function like log.\nLook at the Usage, Arguments and Value sections. These will be invaluable when you come to use R and use functions that you do not already know."
  },
  {
    "objectID": "day1.html#objects",
    "href": "day1.html#objects",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "4.2 Objects",
    "text": "4.2 Objects\nInstead of directly displaying the value of the function (‘value’ is what R calls the result of a function), you can give it a name and store it for later use:\n\nx = 1+2\n\nor\n\nx &lt;- 1+2\n\nThis does exactly the same thing; some R users use the arrow &lt;- instead of = for assignment, so both forms will come up when you’re looking at help files or other people’s code. I (annoyingly!) tend to use either interchangeably.\nNow you have an object called x in your environment that holds the number 3 (check your environment window). You can ask R to display the value of ‘x’ by just entering x (just entering the name of an object prints that object):\n\nx\n\nOr you could do something else with x\n\nx*2\nlog(x)\nx |&gt; log()\nx |&gt; log(10)\n\nWhat does this do?\n\nx |&gt; log() |&gt; sqrt()\n\nHint: it might help to understand if you read the pipe operator |&gt; as “and then…”\nTo see the class of an object (what kind of thing is stored in the object), use the class() function.\n\nclass(x)\n\nObjects of different classes store different kinds of information. We will come across objects of different classes later."
  },
  {
    "objectID": "day1.html#test-yourself",
    "href": "day1.html#test-yourself",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "4.3 Test yourself",
    "text": "4.3 Test yourself\n\nMake a new object called y which has the value of x+3. Then display y.\nNow change the value of x (eg using x &lt;- 6 ). Does the value of y change?\nObjects can hold text strings instead of numbers. Try:\n\n\nmyname &lt;- \"George\"  # (or whatever your name is).\nmyname\n\nWhat is the class of the ‘myname’ object?\n\n(Difficult!) Look up the function to turn a text string into upper case (an internet search will help you). Use this function to make a new object which has the same text as ‘myname’ but in upper case."
  },
  {
    "objectID": "day1.html#download-and-explore-the-structure-of-the-dataset-in-excel",
    "href": "day1.html#download-and-explore-the-structure-of-the-dataset-in-excel",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.1 Download and explore the structure of the dataset in Excel",
    "text": "5.1 Download and explore the structure of the dataset in Excel\nThe ‘iris’ dataset is stored in a comma-separated value file here iris.csv\nDownload the dataset and have a look around in Excel. Note the structure of the dataset\nThe iris data is an example of a dataset in tidy form. We’ll learn more about this in the next session, but in short in has:\n\nA single rectangular table to represent the data\nA column for each variable (characteristic)\nA row for each observation (flower in this case)\n\nThis is very similar to how datasets are stored in software like SPSS or Stata, or in a database, but is different to how you might record data using GraphPad Prism. We’ll see more about how to arrange data like this later in the course.\nMake sure the dataset is stored in the root directory of your project.\nStart a new script to begin your analysis of the ‘iris’ dataset.\nWe can use the read.csv function to load data from a csv file (later we’ll see how to import Excel format data). Type:\n\niris &lt;- read.csv(\"iris.csv\")\n\nCheck your ‘environment’ tab for a new object. Click on the object name to see it in the Viewer, or click on the small blue button next to it to expand the view in the environment window."
  },
  {
    "objectID": "day1.html#exercise",
    "href": "day1.html#exercise",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.2 Exercise",
    "text": "5.2 Exercise\n\nWhat is the class of the iris object that you have created?\nWhat is included in this data?\nWhat kind of descriptive statistics might we calculate to learn about this dataset?\nWhat kind of analytic (inferential) questions might we ask of the data?"
  },
  {
    "objectID": "day1.html#explore-the-structure-of-the-dataset-using-r",
    "href": "day1.html#explore-the-structure-of-the-dataset-using-r",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.3 Explore the structure of the dataset using R",
    "text": "5.3 Explore the structure of the dataset using R\nThere are a few different functions you can use to explore a dataset. Test each with the iris data to see what they do.\n\nstr(iris)\nhead(iris)\nsummary(iris)\ndim(iris)\nnrow(iris)\nView(iris)"
  },
  {
    "objectID": "day1.html#use-the-data-in-calculations",
    "href": "day1.html#use-the-data-in-calculations",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.4 Use the data in calculations",
    "text": "5.4 Use the data in calculations\nWe can extract individual variables (characteristics) and individual elements like this:\n\niris$Sepal.Length \n\nWhat do these lines do?\n\niris$Sepal.Length[1]\n\niris$Sepal.Length[c(2,4,6)]\n\niris$Sepal.Length[10:20]\n\nc(2,4,6)\n\n10:20\n\nWhat does this do?\n\nplot(iris, col=iris$Species)\n\n\n\n\n\n\n\nNote\n\n\n\niris$Sepal.length is a ‘vector’. A vector is a list of items all of the same type. Here its a vector of numbers. A data frame is a collection of vectors.\nWhat are the types of each of the other vectors in the iris data frame?\n\n\nWe can use different R functions to get descriptive statistics for a vector. Try each of these functions and see if you can understand what it does:\n\nmean(iris$Sepal.Length)\n\niris$Sepal.Length |&gt; mean()\n\nsd(iris$Sepal.Length)\n\niris$Sepal.Width |&gt; hist()\n\nWe can work on more than one variable at a time:\n\ncor(iris$Sepal.Length, iris$Sepal.Width)\n\ncor.test(iris$Sepal.Length, iris$Sepal.Width)\n\n\nwith(iris , cor(Sepal.Length , Sepal.Width))"
  },
  {
    "objectID": "day1.html#test-yourself---more-exploration-of-the-iris-dataset",
    "href": "day1.html#test-yourself---more-exploration-of-the-iris-dataset",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.5 Test yourself - More exploration of the iris dataset",
    "text": "5.5 Test yourself - More exploration of the iris dataset\n\nHow many rows does the iris dataset have?\nWhat is the class of each of its columns?\nWhat is the mean sepal length of the flowers?\nWhat is the smallest (minimum) sepal width?\nWhat is the correlation between petal length and petal width?\nIs this correlation statistically significant?\n(Difficult) How many viriginca flowers are included in the dataset?\n\n(Difficult) Can you tabulate the species variable?\n(Difficult) Can you make a histogram of species with red bars?"
  },
  {
    "objectID": "day1.html#other-packages-for-descriptive-statistics-if-you-have-time",
    "href": "day1.html#other-packages-for-descriptive-statistics-if-you-have-time",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "5.6 Other packages for descriptive statistics (if you have time)",
    "text": "5.6 Other packages for descriptive statistics (if you have time)\n(If we have time) Base R does not have good functions for making descriptive tables and graphs. But there are many other packages that have been developed to help with this.\nR packages are add ons developed by the R community to add or improve functions. R packages are the great strength of the R system and mean that R can be used to perform almost any data-related task.\nIn particular the gt system is very good for making tables, and the GGally package has a nice function for making pairs plots. You could install these packages and try them out:\n\ninstall.packages(\"gtsummary\")\ngtsummary::tbl_summary(iris)\n\nCan you stratify this table by species? (Check the help)\n\ninstall.packages(\"GGally\")\nGGally::ggpairs(iris)\n\nCompare this to the output from the ‘base’ function pairs(iris) or plot(iris)."
  },
  {
    "objectID": "day1.html#using-a-function-from-dplyr",
    "href": "day1.html#using-a-function-from-dplyr",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "6.1 Using a function from dplyr",
    "text": "6.1 Using a function from dplyr\nThere are two ways to use functions installed as part of a package. For example, to use the glimpse function from the dplyr package to look at the iris dataset, you can type:\n\ndplyr::glimpse(iris)\n\nor\n\nlibrary(dplyr) # or library(tidyverse)\nglimpse(iris)\n\n\n\n\n\n\n\nUsing library adds the package to the search path for the whole of your R session. This means that R will be able to find all of the functions in this package without explicitly referencing the package every time.\n\n\n\nI will load the tidyverse package, which will attach all of the ‘core’ tidyverse packages into my current session:\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.4.1     v purrr   0.3.4\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.5.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "day1.html#using-dplyr-functions-to-manipulate-and-summarise-data",
    "href": "day1.html#using-dplyr-functions-to-manipulate-and-summarise-data",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "6.2 Using dplyr functions to manipulate and summarise data",
    "text": "6.2 Using dplyr functions to manipulate and summarise data\nThe five main dplyr functions are select, filter, mutate, group_by, and summarise.\nLet’s learn by example what each of these functions does:\n\niris |&gt; select(Sepal.Length, Sepal.Width)\n\niris |&gt; filter(Species==\"setosa\")\n\niris |&gt; filter(Sepal.Length &gt; 7.5)\n\niris |&gt; arrange(Sepal.Length)\n\niris |&gt; summarise(`Median Petal Width` = median(Petal.Width))\n\niris |&gt; summarise(`Mean` = median(Petal.Width), `SD` = sd(Petal.Width))\n\niris |&gt; mutate(Sepal.Area = Sepal.Length * Sepal.Width)\n\niris2 &lt;- iris |&gt; mutate(logPetalLength = log(Petal.Length))\n\niris2 &lt;- iris |&gt; mutate(PetalLengthGrouped = ntile(Petal.Length, 5) )\n\nSo with dplyr we can take subsets of our data or our samples, split our samples up into groups, create new derived variables and make summary statistics.\nUsing pipes you can chain functions together. Suppose you wanted the mean sepal length of setosa flowers:\n\niris |&gt; filter(Species==\"setosa\") |&gt; summarise(mean(Sepal.Length))\n\ngroup_by and summarise are often used together, to make summaries over groups.\n\niris |&gt; group_by(Species) |&gt; summarise(mean(Sepal.Length), mean(Sepal.Width), n=n())\n\nIt can be more readable if you write the command above using more than one line:\n\niris |&gt; \n  filter(Species != \"setosa\") |&gt; \n  group_by(Species) |&gt; \n  summarise(\n    \"Mean sepal length\"=mean(Sepal.Length), \n    \"Mean sepal width\"=mean(Sepal.Width), \n    \"Number of Samples\"=n()\n    )\n\n# A tibble: 2 x 4\n  Species    `Mean sepal length` `Mean sepal width` `Number of Samples`\n  &lt;chr&gt;                    &lt;dbl&gt;              &lt;dbl&gt;               &lt;int&gt;\n1 versicolor                5.94               2.77                  50\n2 virginica                 6.59               2.97                  50\n\n\nMake sure you understand exactly what is happening with each of the commands above. In particular the last line, with filter followed by group_by followed by summarise demonstrates how simple functions can be built into powerful data manipulation and analysis commands."
  },
  {
    "objectID": "day1.html#test-yourself-data-wrangling-and-summarising",
    "href": "day1.html#test-yourself-data-wrangling-and-summarising",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "6.3 Test yourself: data wrangling and summarising",
    "text": "6.3 Test yourself: data wrangling and summarising\nCan you use dplyr to:\n\nFind the mean sepal length\nMake a new dataset including only flowers with a petal length less than 2\nFind the mean of the sepal length for flowers with a petal length less than 2? (hint: use pipes to chain together the answers from questions 2 and 1.)\nFind the median and range for sepal width stratified across species? (hint: pipe together a group_by and a summarise call)"
  },
  {
    "objectID": "day1.html#test-yourself-model-outputs",
    "href": "day1.html#test-yourself-model-outputs",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "8.1 Test yourself (model outputs)",
    "text": "8.1 Test yourself (model outputs)\n\nCan you interpret the model summaries?\nWhich model output format do you prefer?\nCan you estimate the linear model and display the summary in one line using pipes?\n\n\n\n\n\n\n\nNote\n\n\n\nYou can run t-tests using lm, since a t-test is equivalent to a linear model with two group categorical predictor. Linear models generalise many traditional statistical procedures.\nThe only difference that you might see is that in R, the default t-test is not student’s t-test but Welch’s t-test, which does not assume equal variance between groups. Most of the time these are similar (and you should trust Welch’s test if they are not) but if you don’t see identical results between the lm and t.test functions this is likely the reason."
  },
  {
    "objectID": "day1.html#further-uses-of-linear-models.",
    "href": "day1.html#further-uses-of-linear-models.",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "8.2 Further uses of linear models.",
    "text": "8.2 Further uses of linear models.\nGenerate all pairwise comparisons using emmeans or t-tests for individual pairs with filter then lm?\n\nemmeans::emmeans(model1 , pairwise~Species)\n\n$emmeans\n Species    emmean     SE  df lower.CL upper.CL\n setosa       5.01 0.0728 147     4.86     5.15\n versicolor   5.94 0.0728 147     5.79     6.08\n virginica    6.59 0.0728 147     6.44     6.73\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast               estimate    SE  df t.ratio p.value\n setosa - versicolor      -0.930 0.103 147  -9.033  &lt;.0001\n setosa - virginica       -1.582 0.103 147 -15.366  &lt;.0001\n versicolor - virginica   -0.652 0.103 147  -6.333  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nYou can get many different contrasts or test different hypothesis based on this model using the marginaleffects package.\nModel diagnostics with the performance package.\n\nperformance::check_model(model1)"
  },
  {
    "objectID": "day1.html#more-complex-models-interactions-and-multiple-regression",
    "href": "day1.html#more-complex-models-interactions-and-multiple-regression",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "8.3 More complex models (interactions and multiple regression)",
    "text": "8.3 More complex models (interactions and multiple regression)\nSuppose we want to fit a more complex model, whereby sepal length depends on petal length, with the relationship allowed to vary by species.\n\n# First make a model with Sepal.Length depending on Species and Petal.Length\nmodel2 &lt;- lm( Sepal.Length ~ Species + Petal.Length, data=iris)\nsummary(model2)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species + Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75310 -0.23142 -0.00081  0.23085  1.03100 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.68353    0.10610  34.719  &lt; 2e-16 ***\nSpeciesversicolor -1.60097    0.19347  -8.275 7.37e-14 ***\nSpeciesvirginica  -2.11767    0.27346  -7.744 1.48e-12 ***\nPetal.Length       0.90456    0.06479  13.962  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.338 on 146 degrees of freedom\nMultiple R-squared:  0.8367,    Adjusted R-squared:  0.8334 \nF-statistic: 249.4 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n# Now add the interaction term.  These two models are the same.\nmodel3 &lt;- lm( Sepal.Length ~ Species + Petal.Length + Species:Petal.Length, data=iris)\nmodel3 &lt;- lm( Sepal.Length ~ Species * Petal.Length, data=iris)\nsummary(model3)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species * Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73479 -0.22785 -0.03132  0.24375  0.93608 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      4.2132     0.4074  10.341  &lt; 2e-16 ***\nSpeciesversicolor               -1.8056     0.5984  -3.017  0.00302 ** \nSpeciesvirginica                -3.1535     0.6341  -4.973 1.85e-06 ***\nPetal.Length                     0.5423     0.2768   1.959  0.05200 .  \nSpeciesversicolor:Petal.Length   0.2860     0.2951   0.969  0.33405    \nSpeciesvirginica:Petal.Length    0.4534     0.2901   1.563  0.12029    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3365 on 144 degrees of freedom\nMultiple R-squared:  0.8405,    Adjusted R-squared:  0.8349 \nF-statistic: 151.7 on 5 and 144 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use augment from the broom package to add predicted values to our dataset, with confidence intervals.\n\n# look carefully at the output from here:\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.1.3\n\nmodel3 |&gt; augment(interval=\"confidence\")\n\n# A tibble: 150 x 11\n   Sepal.Length Species Petal.Length .fitted .lower .upper  .resid   .hat .sigma\n          &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1          5.1 setosa           1.4    4.97   4.87   5.07  0.128  0.0226  0.337\n 2          4.9 setosa           1.4    4.97   4.87   5.07 -0.0724 0.0226  0.338\n 3          4.7 setosa           1.3    4.92   4.79   5.05 -0.218  0.0378  0.337\n 4          4.6 setosa           1.5    5.03   4.93   5.12 -0.427  0.0210  0.336\n 5          5   setosa           1.4    4.97   4.87   5.07  0.0276 0.0226  0.338\n 6          5.4 setosa           1.7    5.14   4.97   5.30  0.265  0.0583  0.337\n 7          4.6 setosa           1.4    4.97   4.87   5.07 -0.372  0.0226  0.336\n 8          5   setosa           1.5    5.03   4.93   5.12 -0.0266 0.0210  0.338\n 9          4.4 setosa           1.4    4.97   4.87   5.07 -0.572  0.0226  0.334\n10          4.9 setosa           1.5    5.03   4.93   5.12 -0.127  0.0210  0.337\n# i 140 more rows\n# i 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\nFinally we can use ggplot to show these predicted values and confidence intervals. Take care to understand how the output from each function (ie lm, augment) is piped into the next.\n\nlm( Sepal.Length ~ Species * Petal.Length, data=iris) |&gt;\n  augment(interval=\"confidence\") |&gt; \n  ggplot() + \n    aes(x=Petal.Length, y=Sepal.Length, color=Species) + \n    geom_ribbon(aes(y=.fitted,ymin=.lower, ymax=.upper), alpha=0.1) + \n    geom_point() + \n    geom_line(aes(y=.fitted),lwd=1) + \n    theme_bw()\n\n\n\n\n\n\n\n# Try changing the forumula to `Sepal.Length ~ Species + Petal.Length`\n# Which model has the better fit? (you can use anova(model2, model3) to compare models)"
  },
  {
    "objectID": "day1.html#more-detail-on-types-and-classes",
    "href": "day1.html#more-detail-on-types-and-classes",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.1 More detail on types and classes",
    "text": "9.1 More detail on types and classes\nWe saw two object of two different ‘classes’ earlier. These classes were ‘numeric’ and ‘character’. The class of an object defines what kind of information it holds, and how other functions act on it.\nThere are four basic classes (modes) that you will commonly use and should be aware of. These correspond to the types of data you might have. The basic types are:\n\n‘numeric’ – For keeping numbers, can be discrete or continuous\n‘logical’ – can only take the values (TRUE or FALSE)\n‘character’ – for strings of text\n‘factor’ – for labelled categorical variables (ordered or unordered)\n\nLater in this tutorial we will see objects of more complex classes, these can store lots of different information of different modes:\n\n‘data.frame’ – storing datasets\n‘lm’ – stores the all results of a linear regression model"
  },
  {
    "objectID": "day1.html#character-strings",
    "href": "day1.html#character-strings",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.2 Character strings",
    "text": "9.2 Character strings\nCharacter strings represent text rather than numbers. Strings are used to label categories in a dataset, to identify columns in a dataset, to make your outputs more readable. You also might find that part of your data has been entered as a string, for example patient identifiers or gene names in a database, or responses to open ended questions.\nStrings are identified in R (and in most other programming languages) by enclosing them in quotes. Single quotes and double quotes can be used (and are treated almost identically), but double quotes are preferred. For example try:\n\nprint(\"Hello\")\n\nprint('Hello')\n\n# What happens here?\nprint(Hello)\n\nA common mistake in R is to forget to enclose strings in quotes. In which case R tries to interpret your input as an object name, leading to an error message if that name doesn’t exist."
  },
  {
    "objectID": "day1.html#logicals",
    "href": "day1.html#logicals",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.3 Logicals",
    "text": "9.3 Logicals\nLogicals represent binary information in the form TRUE or FALSE. They most often arise as the result of a comparison, for example try:\n\n3&gt;2\n\n\"Hello\" == \"hello\"  # note the double equals sign, this distinguishes assignment from comparison"
  },
  {
    "objectID": "day1.html#converting-between-types",
    "href": "day1.html#converting-between-types",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.4 Converting between types",
    "text": "9.4 Converting between types\nSometimes it is possible to convert an object from one class to another. For example, a number might be stored as a character string in your data, and you will need to convert it into a numeric before you can do any analysis with it. For example:\n\nx &lt;- \"3\"\nx*2 # What is the error message here?  What does it mean?\n\ny &lt;- as.numeric(x)\ny*2"
  },
  {
    "objectID": "day1.html#missing-elements-in-vectors",
    "href": "day1.html#missing-elements-in-vectors",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.5 Missing elements in vectors",
    "text": "9.5 Missing elements in vectors\nOften your data will include missing values. R uses NA to represent missing values. For example the following creates a vector (a single variable, like a single column of a data frame) with a missing value in the fourth position:\n\nweights &lt;- c(10,21,32,NA,14)\nweights\n\n\n\n\n\n\n\nWarning\n\n\n\nNote the difference between NA (a missing value) and \"NA\" (a character string containing the letters N and A. I have been tripped up by this a few times when \"NA\" has been entered into a dataset.)"
  },
  {
    "objectID": "day1.html#exercise-1.-effect-of-missing-values",
    "href": "day1.html#exercise-1.-effect-of-missing-values",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "9.6 Exercise 1. Effect of missing values",
    "text": "9.6 Exercise 1. Effect of missing values\nTry some other functions with myvector to see what impact the missing data point has.\n\nclass(weights)\n\nplot(weights)\n\nweights&gt;20\n\nmean(weights) # what happens here?  Why?  Can you fix it?\n\nis.na(weights) # what does this do?\n\nsum(is.na(weights)) # can you explain what this does?\n\nThe behaviour of R functions with missing data can vary. Some functions will not work, some will return NA but some will just exclude the missing elements and carry on. If you have missing data it is important to understand how the R functions you are using will handle it."
  },
  {
    "objectID": "day1.html#a-continuous-variable-stratified-by-a-grouping-variable-and-the-formula-interface",
    "href": "day1.html#a-continuous-variable-stratified-by-a-grouping-variable-and-the-formula-interface",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "10.1 A continuous variable stratified by a grouping variable, and the formula interface",
    "text": "10.1 A continuous variable stratified by a grouping variable, and the formula interface\nWe already saw a way to get means over a categorical variable with tidyverse:\n\nlibrary(tidyverse)\niris |&gt; group_by(Species) |&gt; summarise(mean(Sepal.Length))\n\nIf we want more than one summary statistic for example, it’s easy with tidyverse (this is an example of a task that is difficult with base R):\n\niris |&gt; group_by(Species) |&gt; summarise(n(),mean(Sepal.Length),sd(Sepal.Length))\n\nNote here that we can write the call in more than one line to make it easier to read:\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise(\n    n(),\n    mean(Sepal.Length),\n    sd(Sepal.Length)\n    )\n\n# A tibble: 3 x 4\n  Species    `n()` `mean(Sepal.Length)` `sd(Sepal.Length)`\n  &lt;chr&gt;      &lt;int&gt;                &lt;dbl&gt;              &lt;dbl&gt;\n1 setosa        50                 5.01              0.352\n2 versicolor    50                 5.94              0.516\n3 virginica     50                 6.59              0.636\n\n\ndplyr is good for making summary statistics, but to make publication ready tables you can explore the gtsummary package.\nhttps://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html\n\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.1.3\n\n\n#BlackLivesMatter\n\niris |&gt; tbl_summary( by=Species )\n\n\n\n\n\n\n\nCharacteristic\nsetosa, N = 501\nversicolor, N = 501\nvirginica, N = 501\n\n\n\n\nX\n26 (13, 38)\n76 (63, 88)\n126 (113, 138)\n\n\nSepal.Length\n5.00 (4.80, 5.20)\n5.90 (5.60, 6.30)\n6.50 (6.23, 6.90)\n\n\nSepal.Width\n3.40 (3.20, 3.68)\n2.80 (2.52, 3.00)\n3.00 (2.80, 3.18)\n\n\nPetal.Length\n1.50 (1.40, 1.58)\n4.35 (4.00, 4.60)\n5.55 (5.10, 5.88)\n\n\nPetal.Width\n0.20 (0.20, 0.30)\n1.30 (1.20, 1.50)\n2.00 (1.80, 2.30)\n\n\n\n1 Median (IQR)"
  },
  {
    "objectID": "day1.html#recoding-a-variable-into-groups",
    "href": "day1.html#recoding-a-variable-into-groups",
    "title": "Introduction to ‘R’ for statistical analysis",
    "section": "10.2 Recoding a variable into groups",
    "text": "10.2 Recoding a variable into groups\nSuppose we want to classify flowers into three groups based on their petal length. We need to add another categorical variable to the dataset.\nWhat class should that new variable be?\nBase R has the function cut that divides continuous variables into groups. Tidyverse has a few extensions of this (weirdly in the ggplot2 package), including cut_number that can divide up a continuous variable into three equal groups, and allow us to add labels to them.\n\ncut_number(iris$Petal.Length, 3, labels=c(\"Short\", \"Medium\", \"Long\") )\n\n\n# Using tidyverse to create this variable and add it to the dataset:\nlibrary(ggplot2)\n\niris &lt;- iris |&gt; mutate(PetalLengthGrouped = cut_number(Petal.Length, 3, labels=c(\"Short\", \"Medium\", \"Long\") )) \n\nAlthough it can be useful descriptively, it is sometimes a very bad idea to group up your variables like this. See Don’t dichotomise your data for an explanation."
  },
  {
    "objectID": "callouttest.html",
    "href": "callouttest.html",
    "title": "calloutTest",
    "section": "",
    "text": "This is a nice document\n\n\n\n\n\n\nImportant tip\n\n\n\nAlways use a callout\nNever use a callout\n\n\nThis is some more text."
  },
  {
    "objectID": "berkson.html",
    "href": "berkson.html",
    "title": "Selection bias in observational studies",
    "section": "",
    "text": "This is a short illustration of “Berkson bias” using simulated data. This is the phenomenon that selection bias can induce correlations in samples that are not present in populations.\nThis bias is likely to be an issue for many observational studies but is not well understood in the medical research literature.\nThe Wikipedia page for the phenonomenon is here: https://en.wikipedia.org/wiki/Berkson%27s_paradox\n\nSuppose we are trying to understand the correlation between health and wealth.\nFirst, let’s simulation a population in which health and wealth are both normally distributed but are not correlated with each other:\n\nset.seed(1)\npopulationsize = 1e4\nsamplesize = 200\n\npopulation &lt;- data.frame(health=rnorm(populationsize), wealth=rnorm(populationsize))\nplot(population)\n\n\n\n\n\n\n\n\nNow if we take a simple random sample from this cohort from this group and test the correlation…\n\nsample1 &lt;- population[sample(populationsize, samplesize),]\nplot(sample1$health, sample1$wealth)\n\n\n\n\n\n\n\ncor.test(sample1$health, sample1$wealth)\n\n\n    Pearson's product-moment correlation\n\ndata:  sample1$health and sample1$wealth\nt = -0.78426, df = 198, p-value = 0.4338\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.19290026  0.08373896\nsample estimates:\n        cor \n-0.05564858 \n\n\nWe see no evidence that wealth and health are related, as we would expect given this is true in the population.\nHowever we know that healthier, wealthier people are more likely to participate in our studies. So let’s create a sample that reflects this:\n\npopulation$b_participate = (population$health + population$wealth)\npopulation$prob_participate = exp(population$b_participate) / (1+exp(population$b_participate))\n\nsample2 &lt;- population[sample(populationsize, samplesize, prob = population$prob_participate),]\n\nHow will the average health and wealth in our sample compare to the population?\n\nmean(sample2$health)\n\n[1] 0.3808598\n\nmean(sample2$wealth)\n\n[1] 0.3049277\n\n\nWell we know that health and wealth are higher in the sample than the population, but this should hopefullt be obvious. But how are the two things correlated in the population?\n\nplot(sample2$health, sample2$wealth)\n\n\n\n\n\n\n\ncor.test(sample2$health, sample2$wealth)\n\n\n    Pearson's product-moment correlation\n\ndata:  sample2$health and sample2$wealth\nt = -2.9629, df = 198, p-value = 0.003421\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.3352015 -0.0692823\nsample estimates:\n       cor \n-0.2060428 \n\n\nIt looks like a negative correlation in the sample, despite there being no correlation in the population!\nWas this unlucky? We can look at the distribution of p-values from repeated samples, to see what the chance of a false positive correlation is here:\n\nonepvalue &lt;- function(){\n  samplen &lt;- population[sample(populationsize, samplesize, prob = population$prob_participate),]\n  cor.test(samplen$health, samplen$wealth)$p.value\n}\nreplicate(1000, onepvalue()) |&gt; hist(breaks=20)\n\n\n\n\n\n\n\n\nIn more than 50% of cases there is now a significant correlation in the sample even though there is no correlation in the population. It is simply caused by the factors under investigation also being factors that lead people to participate in research.\nWhat are the implications of this for observational studies?\nWhat can we do about it? If we have sampling weights it is relatively easy to fix, but what if we don’t? Do we have to try to get them somehow?"
  },
  {
    "objectID": "alphabeta.html",
    "href": "alphabeta.html",
    "title": "Simulations exploring alpha and beta diversity",
    "section": "",
    "text": "This file will include short simulations to understand how differences in community compositions are reflected in alpha and beta diversity metrics.\nIn each simulation I’ll create some data with given characteristics then look at how this is reflected in the diversity measures. This could be extended into power calculations if needed."
  },
  {
    "objectID": "alphabeta.html#simulation-1",
    "href": "alphabeta.html#simulation-1",
    "title": "Simulations exploring alpha and beta diversity",
    "section": "Simulation 1",
    "text": "Simulation 1\n\nWe’ll assume that there are 100 possible species in my communities.\nThere are two different classes of environments from which we can sample communities,\nIn samples from the first environment each species is present with probability 0.4.\nIn samples from the the second environment each species is present with probability 0.5.\nWe’ll sample ten communities per class, and look at their alpha and beta diversities\n\nFirst sample a matrix of the community compositions:\n\nN = 10 # Number of samples per class\nn = 100 # Number of possible species\nset.seed(\"19122023\")\nclasses &lt;- rep(c(\"A\", \"B\"), each=N)\nclassProbabilities &lt;- c(\"A\"=0.4,\"B\"=0.5)\ndat &lt;- sapply(classes, \\(class) rbinom(n=n, size = 1, p=classProbabilities[class])) |&gt; t()\n\n# Check it looks OK\ndat[1:10,1:10]\n\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\nA    1    1    1    0    0    0    0    0    1     1\nA    0    1    1    1    0    1    0    0    1     1\nA    0    0    1    0    0    1    0    1    1     0\nA    0    0    1    0    0    0    0    0    0     0\nA    1    1    1    1    0    0    1    0    1     0\nA    0    1    1    0    0    0    1    0    1     1\nA    1    0    1    1    0    0    1    0    0     0\nA    1    1    1    0    0    0    0    1    0     1\nA    1    0    0    0    1    0    1    1    0     0\nA    1    0    0    0    1    1    1    0    0     0\n\n\nNow look at the alpha diversity (Shannon index):\n\nlibrary(vegan)\nlibrary(ggplot2)\n## Shannon index\nt.test(diversity(dat) ~ classes)\n\n\n    Welch Two Sample t-test\n\ndata:  diversity(dat) by classes\nt = -4.6706, df = 16.102, p-value = 0.0002517\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -0.3544225 -0.1332105\nsample estimates:\nmean in group A mean in group B \n       3.672421        3.916238 \n\nggplot() + aes(y=diversity(dat), x=classes) + \n  geom_boxplot()  +\n  geom_point() + \n  stat_summary()\n\n\n\n\n\n\n\n\nBeta-diversity (NMDS following Jaccard diversity):\n\n## Not so clear with the beta-diversity\nvegdist(dat, method = \"jaccard\", binary = TRUE) |&gt; \n  metaMDS(trace = 0) |&gt;\n  scores() |&gt; \n  plot(col=factor(classes), pch=20)\n  legend(\"topright\", legend=c(\"A\",\"B\"), col=1:2,pch=20)\n\n\n\n\n\n\n\ndm1 &lt;- vegdist(dat, method = \"jaccard\", binary = TRUE)\nadonis2(dm1 ~ classes  )  \n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = dm1 ~ classes)\n         Df SumOfSqs      R2      F Pr(&gt;F)\nclasses   1   0.2915 0.06188 1.1873  0.166\nResidual 18   4.4193 0.93812              \nTotal    19   4.7108 1.00000              \n\nanosim(dm1, grouping = classes)\n\n\nCall:\nanosim(x = dm1, grouping = classes) \nDissimilarity: binary jaccard \n\nANOSIM statistic R: 0.02167 \n      Significance: 0.36 \n\nPermutation: free\nNumber of permutations: 999\n\n\nSo in this case we see a clear difference in alpha diversity between sites but it’s not so clear for beta diversity. It looks like the environment B samples are slightly more clustered while the A samples are more spread out. The difference is not reflected in a PERMANOVA (via adonis2) or ANOSIM.\nThis is a good reminder that important differences in community composition might not be immediately visible in beta diversity plots. Maybe you all know that anyway but it was a good reminder for me."
  },
  {
    "objectID": "boxplotcatton.html",
    "href": "boxplotcatton.html",
    "title": "Comparing two groups",
    "section": "",
    "text": "Link to slides\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nEnter our data here\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSorted data:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFive number summary\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimple box plots for comparison\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRaincloud plot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBee swarm plot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHistogram\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "datatabletidyverse.html",
    "href": "datatabletidyverse.html",
    "title": "Descriptive tables using base R, data.table and tidyverse",
    "section": "",
    "text": "With only base R (that is, R without add on packages) it can be unexpectedly difficult to perform some simple tasks.\nA good example is making a table of summary statistics. This is difficult with base R but is simple with using function from add-on packages.\nHere I illustrate this using two widely used systems for data manipulation in R, namely data.table and tidyverse. Both can be used to make summary tables of descriptive statistics. that can be exported\nFinally I describe a package, gtsummary that is specifically designed for creation of publication ready summary tables."
  },
  {
    "objectID": "datatabletidyverse.html#breaking-down-the-data.table-syntax",
    "href": "datatabletidyverse.html#breaking-down-the-data.table-syntax",
    "title": "Descriptive tables using base R, data.table and tidyverse",
    "section": "Breaking down the data.table syntax",
    "text": "Breaking down the data.table syntax\nThe [ operator in data.table has three arguments. In short, we express a command on a dataset (here called dat) by specifying:\n\ndat[ which rows to use , what to do , which columns to group on ]\n\nIn the first version of the command above we left the first entry blank (so used all the rows), placed mean(height) in the second position and specified by=sex in the third. In the second version we expanded the second argument to return a list of elements, and gave them new names.\nFor more details of using data.table, see: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html"
  },
  {
    "objectID": "datatabletidyverse.html#the-tidyverse-dplyr-syntax",
    "href": "datatabletidyverse.html#the-tidyverse-dplyr-syntax",
    "title": "Descriptive tables using base R, data.table and tidyverse",
    "section": "The tidyverse (dplyr) syntax",
    "text": "The tidyverse (dplyr) syntax\ndplyr introduces six main functions for manipulating and summarising data, these are mutate, arrange, select, filter, summarise, and group_by. Using combinations of these functions you can perform most simple data operations. Functions are chained together using the pipe operator %&gt;% which passes the output from one into the next.\nSo the first command above reads something like: “take dat, then group it by sex, then for each group return the summary statistics we specified”.\nVisit https://www.tidyverse.org/learn for more."
  },
  {
    "objectID": "day2MSc.html",
    "href": "day2MSc.html",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "",
    "text": "Today’s worksheet introduces you to a real dataset.\nThe tasks you will cover are:"
  },
  {
    "objectID": "day2MSc.html#the-dataset",
    "href": "day2MSc.html#the-dataset",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "The dataset",
    "text": "The dataset\nThe data are from a randomised clinical trial of a new rehabilition intervention (compared to standard post-stroke care) aimed at improving the walking speed of hospital patients. Better walking speed is a good indicator of general stroke recovery.\nWe have recorded:\n\nThe age and sex of each participant,\nThe treatment allocation,\nThe hospital department from which they were recruited and\nTime they take to complete a walking task.\n\nOur research questions are:\n\nDoes the treatment improve walking speed compared to controls?\nBy how much does it improve, and how certain are we of this?\n\nThe dataset can be found at walkingspeed_day2.xlsx\n\nPowerpoint slides to support this material are at:\n\nday3.pptx\nOur workflow is:\n\nClean and code\nVisualise\nDescribe\nModel\nDiagnose model\nInterpret"
  },
  {
    "objectID": "day2MSc.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "href": "day2MSc.html#make-sure-the-data-looks-ok-and-is-in-the-right-place-on-your-computer",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Make sure the data looks OK and is in the right place on your computer",
    "text": "Make sure the data looks OK and is in the right place on your computer\nBefore we dive in and import it, we need to make sure our data is in a sensible place.\n\nOpen RStudio and open the project you created in day 1 (if not already opened).\nThen, save the example data walkingspeed_msc.xlsx for this tutorial into your project folder. Check that it has appeared in the ‘files’ pane in RStudio.\nStart a new script file that will eventually include all of the commands we need to import, clean, visualise and analyse the data.\nNow open the dataset in Excel and explore the file so that you understand what is there."
  },
  {
    "objectID": "day2MSc.html#read-the-help",
    "href": "day2MSc.html#read-the-help",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Read the help!",
    "text": "Read the help!\nWe are nearly ready to import our data. But remember that before using a new function its always good to read its documentation. The read_excel() function has a few different options so first we should look at the help file:\n\n?read_excel\n\nNote from the help file that read_excel() can extract data from different sheets and ranges of an Excel workbook, can use or ignore column names, and allows you to specify the type of data (numeric, dates, text etc) if you want to, or leave it to R to guess.\nMany R packages also have vignettes or websites including simpler guides to their use in specific cases. readxl has a website that you might find helpful: https://readxl.tidyverse.org/\nNow we’ll load the data. We want to use the ‘walking speed’ data from the walkingspeed_msc.xlsx spreadsheet.\n\nOpen the spreadsheet in Excel and find this sheet. The data we want is in the sheet called ‘day2’.\n\nFrom the read_excel() help file we can deduce the syntax to load this data into R:\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.1.3\n\nwalkingdat &lt;- read_excel(path=\"walkingspeed_msc.xlsx\", sheet=\"day2\")\n\nThis line assumes that the file walkingspeed_msc.xlsx is in the current working directory (you can check what this is with getwd()). The current working directory is shown just above the R console window. You can see the files in the current working directory in the ‘Files’ tab on the bottom right of the RStudio window. When you create or load a project RStudio will set the working directory to the root of the project directory.\nThis line calls the read_excel() function, with the arguments path, sheet set. The other arguments will be set to their default values, which you can see from the help file.\nWe could have set the range of the data in the spreadsheet (I usually do this for safety), but read_excel() can figure it out automatically most of the time; by default it picks the biggest continuous chunk of data starting in the top left of the sheet.\nNow you should have a ‘data frame’ object called walkingdat in your environment, which includes the data from the Excel sheet ready to process and analyse."
  },
  {
    "objectID": "day2MSc.html#dealing-with-outliers",
    "href": "day2MSc.html#dealing-with-outliers",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Dealing with outliers",
    "text": "Dealing with outliers\nIt looks like there are some unreasonably high and low values of walking time.\nWe can make another graph of walking speed against age, this time on a logarithmic scale so both the extreme high and extreme low points are visible, to see what is going on.\n\n## base R version\nplot(walkingdat$age, walkingdat$time, log=\"y\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): 1 y value &lt;= 0 omitted from\nlogarithmic plot\n\n\n\n\n\n\n\n\n## ggplot2 version\nlibrary(ggplot2)\nggplot(data = walkingdat) + aes(x=age, y=time) + geom_point() + scale_y_log10()\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nIt seems there are some values for time that are likely to be technical errors. We can remove these values (set them to missing) in a few different ways:\n\n## base R method to replace the values with NA ('missing')\nwalkingdat$time[walkingdat$time&lt;0.1] &lt;- NA\nwalkingdat$time[walkingdat$time&gt;100] &lt;- NA\n\n## Using filter we could remove those datapoints\nwalkingdat &lt;- walkingdat |&gt; filter(time &lt; 100) |&gt; filter(time &gt; 0.1)\n\n\n# Now check the distribution of time again.\nhist(walkingdat$time, breaks=100)\n\n\n\n\n\n\n\nrange(walkingdat$time, na.rm = TRUE)\n\n[1]  1.435 17.641\n\n\nNow we have a cleaned dataset in our environment we can proceed with our visualisation and analysis:"
  },
  {
    "objectID": "day2MSc.html#graphics",
    "href": "day2MSc.html#graphics",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Graphics",
    "text": "Graphics\n\nGraphing\n\nConsider what kind of graph you might make to illustrate the difference in walking speed between treatment groups, and try to make it using ggplot. Eg:\nMake a box plot of time by sex, with a different coloured box per sex\nLabel the axes appropriately\nTry a violin plot instead of a boxplot (with geom_violin). Which do you prefer?\nMake any other adjustments you think are informative!"
  },
  {
    "objectID": "day2MSc.html#modelling",
    "href": "day2MSc.html#modelling",
    "title": "Intro to R for statistics, Day 2 (MSc version)",
    "section": "Modelling",
    "text": "Modelling\nI have left these sections in from the PhD workshop. Attempt them if you get time.\n\nHypothesis Testing\n\nWhat is the difference in average walking speed between control patients and treated patients?\nIs this difference statistically significant? Use t-test t.test() and Mann-Whitney tests wilcox.test() to examine the differences between the groups. Look up the syntax for t.test() and wilcox.test() using the help system, and use them to check whether the time taken to complete the task varies by treatment status. In each case use the version of the function that takes a formula agument. Remember the formula syntax from the boxplot and lm functions in day 1.\nCompare the p-values for each method.\nConsider the assumptions for a t-test, and whether they are met in this dataset.\n\nDo you believe that treatment affects walking speed?\n\nLinear model diagnostics\n\nEstimate a linear model (with lm) to estimate/test the effect of treatment on task completion time.\nUse summary to get the model coefficients and p-values\nUse broom::tidy to get the model coefficients and p-values\nCompare the model results to the equivalent t-test\nExtend the model to include the effects of age and sex as potential covariates.\nUse plot() to explore the model diagnostics\nUse performance::check_model() to explore the model diagnostics\nNow estimate a new model to compare the task completion speed between groups.\nGet the model coefficients and p-values\nCheck the model diagnostics using plot or performance::check_model\nWhy are the result from this model different? How do you interpret the results?\nCan you get the confidence intervals for the treatment effect (use the help for broom::tidy)\nUse model_summary package to make a pretty model summary table"
  },
  {
    "objectID": "dichot.html",
    "href": "dichot.html",
    "title": "Why is ‘grouping’ your samples bad for analysis?",
    "section": "",
    "text": "Summary\nIf we have continuous data then we should keep it continuous in analysis. Grouping samples into (say) ‘high’ vs ‘low’ or ‘recovered’ vs ‘not recovered’ throws away information and makes it more difficult to detect associations. The power of your study is reduced and the sample size needed goes up.\n\n\nMotivation\nSuppose we are interested in the effect of an exposure on an outcome, and we have measured both in a sample. Both our exposure and our outcome are measured as continuous variables, for example we might be interested in the effect of fibre intake on gut microbial diversity.\nThere are a couple of possible approaches to the analysis. First, we could estimate or test for a correlation between the exposure and the outcome.\nAlternatively we could dichotomise the exposure, splitting the samples into a “high fibre” and a “low fibre” group, before comparing the microbial diversity in each group. Or we could dichotomise the outcome into “high diversity” and “low diversity”.\nWhich should we choose? Intuitively we might prefer the dichotomised version for “ease of interpretation”, but how does it affect our ability to detect any association?\n\n\nPackages\nWe’ll need the following R packages for this simulation:\n\nlibrary(pwr)\nlibrary(rmarkdown)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(MASS)\nlibrary(data.table)\n\n\n\nDataset\nSuppose our exposure and outcome are both normally distributed with a correlation of 0.5.\nWe can sample 50 points as follows:\n\nset.seed(21)\ndat = data.frame(mvrnorm(n=30, mu=c(10,10), Sigma = matrix(c(1,0.5,0.5,1),nrow=2)))\nnames(dat) &lt;- c(\"Exposure\", \"Outcome\")\n\nggplot(dat, aes(x=Exposure, y=Outcome)) + geom_point() + theme_bw()\n\n\n\n\n\n\n\n\nHow can we estimate or test this correlation by statistical analysis?\nThe simplest way would be to test\n\ncor.test(dat$Exposure, dat$Outcome)\n\n\n    Pearson's product-moment correlation\n\ndata:  dat$Exposure and dat$Outcome\nt = 3.0861, df = 28, p-value = 0.004535\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1753575 0.7313294\nsample estimates:\n      cor \n0.5037987 \n\n\nSo in our sample this correlation is clearly detectable (estimated r=0.50, p=0.0045).\n\n\nDichotomising variables\nDichotomising means splitting variables into two groups. In our example we might decide to compare those with a high vs a low exposure. Lets make a variable corresponding to whether the exposure for each participant is above observed median. This will split the data into two groups, and we can describe how the outcome differs between the high and low exposure groups:\n\ndat$HighExposure &lt;- dat$Exposure &gt; median(dat$Exposure)\n(ggplot(dat, aes(Exposure, Outcome, color=HighExposure)) + \n  geom_point() + theme_bw() + \n  scale_color_manual(values=c(\"black\", \"red\")))+\n(ggplot(dat, aes(HighExposure, Outcome)) + \n  geom_boxplot() + geom_point(aes(color=HighExposure)) + theme_bw()+ \n  scale_color_manual(values=c(\"black\", \"red\")))\n\n\n\n\n\n\n\n\nNow we can use (for example) a t-test to for difference in outcome between “high” and “low” exposure groups:\n\nt.test(data=dat, Outcome ~ HighExposure)\n\n\n    Welch Two Sample t-test\n\ndata:  Outcome by HighExposure\nt = -2.3855, df = 27.985, p-value = 0.02407\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -1.5826037 -0.1203041\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           9.723858           10.575312 \n\n\nThe difference is now barely detectable (p=0.024)!\nWe could go further, and dichotomise both the outcome and the exposure.\n\ndat$HighOutcome &lt;- dat$Outcome &gt; median(dat$Outcome)\n(ggplot(dat, aes(Exposure, Outcome, color=HighExposure, shape=HighOutcome)) + \n  geom_point() + theme_bw() + geom_hline(yintercept=median(dat$Outcome),lty=2)+\n  scale_color_manual(values=c(\"black\", \"red\")))+\n(ggplot(dat, aes(HighExposure, as.numeric(HighOutcome))) + \n  geom_bar(stat=\"summary\", fun.y=mean, col=\"black\", aes(fill=HighExposure)) + theme_bw()+\n   labs(y=\"Proportion with high outcome\")+\n  scale_fill_manual(values=c(\"black\", \"red\")))\n\n\n\n\n\n\n\n\nNow our relationship seems obscured. Our analysis consists of analysing a 2x2 contingency table:\n\ntable(dat$HighExposure, dat$HighOutcome)\n\n       \n        FALSE TRUE\n  FALSE    10    5\n  TRUE      5   10\n\nchisq.test(table(dat$HighExposure, dat$HighOutcome))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(dat$HighExposure, dat$HighOutcome)\nX-squared = 2.1333, df = 1, p-value = 0.1441\n\n\nThe difference in ‘high outcome’ proportions between exposure groups is not statistically significant! (p=0.14). This shows that it is much more difficult to see the relationship if we throw away the exact values of the data points.\nSo although there is a real relationship between exposure and outcome that we have been able to detect by correlating the values in our sample, it was harder to detect when we dichotomised the exposure, and we could not detect it at all in the grouped data.\n\n\nImplications for power and sample size\nOur single example illustrated that dichotomising data made analysis difficult or impossible by discarding data. But it’s easy to cherry pick examples, how does this work in general?\nWe can quantify the average loss of information by power calculations (repeated simulations), and consider how much our sample size would need to increase to overcome this.\nThe graph below shows the power to detect a correlation of 0.5 as the sample size changes using each of the methods described above.\nIf we analyse the continuous data then we need a sample size of about 28 for a power of 80%. If we dichotomise the exposure only we would need 43 samples, if we dichotomise both and analyse the 2x2 table we would need 69, more than twice the original sample.\n\nsampleSizes &lt;- seq(10,100,5)\n\npowers &lt;- data.table(t(sapply(sampleSizes, function(n){c(\n  sampleSizes=n,\n  continuous=100*pwr.r.test(r=0.5, n = n)$power,\n  dichotone=100*pwr.t.test(d=.8 / 0.91, n = n/2)$power,\n  dichotboth=100*pwr.2p.test(h=ES.h(1/3, 2/3), n = n/2)$power\n  )\n  })))\n\nggplot(melt(powers,id.vars = \"sampleSizes\", variable.name = \"Analysis\", value.name = \"Power\"), \n       aes(sampleSizes, Power, color=Analysis)) + \n  geom_line() + \n  geom_point() + \n  theme_bw() + \n  scale_x_continuous(limits=c(0,100)) + \n  labs(x=\"Sample Size (total)\", y=\"Power (%)\") + \n  scale_color_manual(labels=c(dichotboth=\"Dichotomise both\", \n                              dichotone=\"Dichtomise outcome only\", \n                              continuous=\"Correlate continuous measures\"), \n                     values=c(\"black\", \"red\", \"blue\")) + \n  theme(legend.position = c(0.7,0.3), legend.background = element_rect(linetype = 1, size = .5, colour = \"black\")) + \n  geom_hline(yintercept=80, lty=2)\n\n\n\n\n\n\n\n\n\n\nConclusion\nGrouping up our continuous variables for analysis throws away information. It makes associations more difficult to detect, and increases the number of samples we need to analyse.\nThere are other harmful consequences to dichotomising data. Sometimes a defense of the practice is that by collapsing quantiative findings into yes/no outcomes makes results easier to interpret, but Stephen Senn makes a good case that this is wrong and can even be misleading.\nSo in general, if the phenomenon you are interested in can be measured continuously then you should do so, and be sure to use all of this quantitative information in the analysis.\n\n\nReferences (todo - tidy this up)\nThis phenomenon has been discussed many times in the statistical literature and in blogs\nhttps://errorstatistics.com/2016/08/02/s-senn-painful-dichotomies-guest-post/ (Senn 2016 - a not identical but related problem with dichtomies)\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC1458573/ (Altman and Royston 2006)\nhttp://www.psychology.sunysb.edu/attachment/measures/content/maccallum_on_dichotomizing.pdf (MacCallum et al 2002)\nhttps://bjo.bmj.com/content/98/6/841 (Cumberland et al 2014)"
  },
  {
    "objectID": "independentclusters.html",
    "href": "independentclusters.html",
    "title": "Clustering independent of sampling in observational studies",
    "section": "",
    "text": "We know that random sampling is important to get good inferences from epidemiological studies.\nIf sampling is ‘clustered’ then we need to account for this in our analysis. This is well known.\nHowever we may have a situation where data points are generated in clusters, but we still sample them randomly. Is there a risk of false positive associations in such situations? How should we deal with it?\nHere I use a simple simulation to show that even with random sampling, data that is generated in clusters can still lead us to false positive conclusions."
  },
  {
    "objectID": "independentclusters.html#introduction",
    "href": "independentclusters.html#introduction",
    "title": "Clustering independent of sampling in observational studies",
    "section": "",
    "text": "We know that random sampling is important to get good inferences from epidemiological studies.\nIf sampling is ‘clustered’ then we need to account for this in our analysis. This is well known.\nHowever we may have a situation where data points are generated in clusters, but we still sample them randomly. Is there a risk of false positive associations in such situations? How should we deal with it?\nHere I use a simple simulation to show that even with random sampling, data that is generated in clusters can still lead us to false positive conclusions."
  },
  {
    "objectID": "independentclusters.html#simulation-data",
    "href": "independentclusters.html#simulation-data",
    "title": "Clustering independent of sampling in observational studies",
    "section": "Simulation data",
    "text": "Simulation data\nThe research question here is whether there is a difference between two populations.\nFor the simulation I will create a dataset of 100 points from each population. Each point is sampled independently at random, but arises from one of five clusters in each population. So, to sample each point we first select a cluster at random, then generate a point from that cluster.\nThe cluster averages in turn are selected from a normal distribution with mean zero.\nAn real example question might relate to whether there is a difference in educational attainment between two different cities. We can select children from each city completely at random. But if each city has only five schools then the data points are clustered by school, even if the children are selected completely randomly without any reference to which school they attend.\nSo although children are selected independently of each other, they are still clustered in some sense! Do we still need to run a ‘clustered’ analysis if we are interested in understanding whether ‘city’ has any effect on educational attainment?\nRemember we know that there is no effect of city on average school outcome in our simulation. Differences in outcomes do occur completely randomly at the school level and at the individual child level.\n\nNclusters &lt;- 10\nclustersMeans &lt;- rnorm(Nclusters, 0,1)\n\n## Suppose group A come from clusters 1 to 5\n## Group B come from clusters 6 to 10.\n\nNperGroup &lt;- 100\nGroupAClusters &lt;- sample(1:5,NperGroup, replace=TRUE)\nGroupBClusters &lt;- sample(6:10,NperGroup, replace=TRUE)\n\ndat &lt;- data.frame(Group=rep(c(\"A\",\"B\"),each=NperGroup), cluster=c(GroupAClusters, GroupBClusters))\ndat$y &lt;- rnorm(NperGroup*2 , clustersMeans[dat$cluster], 1)\n\nhead(dat)\n\n  Group cluster           y\n1     A       1  2.17314644\n2     A       4  0.23199408\n3     A       4 -0.14176686\n4     A       2  1.05632863\n5     A       4  0.15161343\n6     A       3  0.08051086\n\n\nNow we can plot the data:\n\nlibrary(ggplot2)\n\ndat |&gt; ggplot(aes(x=Group, y=y, shape=factor(cluster))) + \n  scale_shape_manual(values=1:10) + \n  geom_point(position = position_dodge2(width=0.2)) + theme_bw()\n\n\n\n\n\n\n\n\nThe clustering is evident in the plot. How does this affect the p-values for a linear model? Do we need to use a LMM?\n\n## A naive linear model would suggest a difference between groups\nlm(y ~ Group , data=dat) |&gt; broom::tidy()\n\n# A tibble: 2 x 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)    0.286     0.111      2.56 0.0111     \n2 GroupB        -0.821     0.158     -5.21 0.000000480\n\n## While mixed model taking the clustering into account does not\nlmer(y ~ Group + (1|cluster), data=dat) |&gt; broom.mixed::tidy() |&gt; subset(effect==\"fixed\")\n\n# A tibble: 2 x 8\n  effect group term        estimate std.error statistic    df p.value\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 fixed  &lt;NA&gt;  (Intercept)    0.313     0.291      1.07  8.08  0.313 \n2 fixed  &lt;NA&gt;  GroupB        -0.913     0.412     -2.22  8.08  0.0571\n\n\nSo if we ignore the clusters we get a false positive result (p&lt;0.001), but if we incorporate them into our analysis we do not."
  },
  {
    "objectID": "independentclusters.html#type-1-error-rate",
    "href": "independentclusters.html#type-1-error-rate",
    "title": "Clustering independent of sampling in observational studies",
    "section": "Type 1 error rate",
    "text": "Type 1 error rate\nTo quantify the problem we can look at how the false positive rate changes with the extent of the clustering.\nWe’ll simulate datasets and analysis 1000 times for each value of the cluster standard deviation, from 1.0 (the same as the between individual standard deviation) to zero. Then we’ll look at the proportion of statistically significant results (p&lt;0.05), and compare this to the nominal 5% rate.\nIn each simulation we generate new cluster means, and then a new dataset of points. We then apply the simple linear model and a mixed effects model, and extract the p-values for the hypothesis test that the effect of ‘population’ is zero.\n\noneRep &lt;- function(clusterSD){\n  clustersMeans &lt;- rnorm(Nclusters, 0,clusterSD)\n  dat$y &lt;- rnorm(NperGroup*2 , clustersMeans[dat$cluster], 1)\n  plmer &lt;- (lmer(y ~ Group + (1|cluster), data=dat) |&gt; summary() |&gt; coef())[2,5]\n  plm &lt;- (lm(y ~ Group , data=dat) |&gt; summary() |&gt; coef())[2,4]\n  c(plmer,plm)\n}\n\npvals &lt;- lapply(c(1.0,0.8,0.6,0.4,0.2,0.0) , \\(s) replicate(1000,oneRep(s)))\n\nsignificant &lt;- sapply(pvals, apply, 1, \\(p) mean(p&lt;0.05)) |&gt; t() |&gt; as.data.frame() |&gt; setNames(c(\"Mixed model\", \"Simple Linear model\"))\nsignificant$clusterVariance &lt;- c(1.0,0.8,0.6,0.4,0.2,0.0)\n\nsignificant &lt;- data.table::melt(significant, id.vars=\"clusterVariance\", variable.name=\"Method\", value.name=\"rate\")\n\nggplot(significant) + aes(x=clusterVariance, y=rate, col=Method) + geom_point() + geom_line() + \n  geom_hline(yintercept=0.05, lty=\"dashed\") + \n  labs(x=\"Between cluster standard deviation\", y=\"Type 1 error rate\")"
  },
  {
    "objectID": "independentclusters.html#conclusion-and-discussion",
    "href": "independentclusters.html#conclusion-and-discussion",
    "title": "Clustering independent of sampling in observational studies",
    "section": "Conclusion and Discussion",
    "text": "Conclusion and Discussion\nThe type 1 error rate increases markedly with the among of variance between ‘clusters’ when a simple linear model is used. A mixed model broadly corrects this, and may over-compensate when the between-cluster variance is very low.\nSo for our school analogy, if schools are very variable, then if we don’t model the clustering it may look like there are inherent differences between cities instead of there simply being random differences between the schools that are not attributable to the city they are in.\nWhether this is the correct or incorrect answer depends on exactly what the question is that we are trying to answer, that is whether we are trying to (1) identify the city that just happens (by chance) to have the best schools, or (2) whether we are asking about whether the city itself promotes good schools or good outcomes.\nIn most situations we are asking the latter question in which case we need to include the grouping in our analysis to get a meaningful result.\nTo generate our simulated datasets we included no effect of the grouping factor on the cluster means or the individual means, and we sampled each unit independently from the population. Yet a simple linear model (equivalent of t-test or ANOVA) returned a strongly significant effect. This was corrected by considering the shared variance within clusters in our model.\nThis should be another reminder that to avoid looking silly we need to understand how our datasets come to be, and what shared variance there is between our observations, even if we know that our samples are generated from our populations using simple random sampling."
  },
  {
    "objectID": "irisScript.html",
    "href": "irisScript.html",
    "title": "Iris dataset analysis walkthrough",
    "section": "",
    "text": "This file is intended as a summary of some of the important features discussed during the ‘R for Statistics’ training course.\nThe aspects covered are:\nI do not include model diagnostics, nor data wrangling with tidyverse here. These are important topics that are introduced in the individual session worksheets."
  },
  {
    "objectID": "irisScript.html#loading-and-describing-data.",
    "href": "irisScript.html#loading-and-describing-data.",
    "title": "Iris dataset analysis walkthrough",
    "section": "1 Loading and describing data.",
    "text": "1 Loading and describing data.\nLoad a package we will need:\n\nlibrary(ggplot2) # ggplot is for making publication quality graphs\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nLoad data from a csv file using read.csv\n\niris &lt;- read.csv(\"iris.csv\")\n\nGet some statistics on the structure of the dataset to make sure it’s all read in OK.\n\nnrow(iris)\n\n[1] 150\n\nstr(iris)\n\n'data.frame':   150 obs. of  6 variables:\n $ X           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr  \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n\n\nExtract single variables (vectors) from the dataset\n\niris$Sepal.Length\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1\n [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0\n [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5\n [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1\n [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5\n [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3\n[109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2\n[127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\n\nNote that R is case sensitive so this does not work:\n\niris$sepal.length\n\nNULL\n\n\nGet individual elements if we want to.\n\niris$Sepal.Length[1]\n\n[1] 5.1\n\n\nGet the 2nd, 4th and 6th elements (extract a subset by position)\n\niris$Sepal.Length[c(2,4,6)]\n\n[1] 4.9 4.6 5.4\n\n\nGet the 10th to the 20th elements of the vector\n\niris$Sepal.Length[10:20]\n\n [1] 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 5.7 5.1\n\n\nBy default, Species is read as a string (because the option stringsAsFactors is set to FALSE by default).\nSo if we want to use Species as a categorical variable we have to convert it into a ‘factor’.\n\niris$Species &lt;- factor(iris$Species)\n\nWe can use the default plotting system to make a quick plot, but it’s a bit ugly!\n\nplot(iris, col=iris$Species)\n\n\n\n\n\n\n\n\nDemonstration of using the base hist function if we want a quick histogram\n\niris$Sepal.Width |&gt; hist(breaks=20)\n\n\n\n\n\n\n\n\nSummary does different things depending on what kind of object is passed to it.\n\nsummary(iris)\n\n       X           Sepal.Length    Sepal.Width     Petal.Length  \n Min.   :  1.00   Min.   :4.300   Min.   :2.000   Min.   :1.000  \n 1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600  \n Median : 75.50   Median :5.800   Median :3.000   Median :4.350  \n Mean   : 75.50   Mean   :5.843   Mean   :3.057   Mean   :3.758  \n 3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100  \n Max.   :150.00   Max.   :7.900   Max.   :4.400   Max.   :6.900  \n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.199                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n\nsummary(iris$Sepal.Width)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   2.800   3.000   3.057   3.300   4.400 \n\nsummary(iris$Species)\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nWe can do a statistical test using two variables. (Although this association is better tested using lm as below)\n\ncor.test(iris$Sepal.Length, iris$Petal.Width)\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Petal.Width\nt = 17.296, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7568971 0.8648361\nsample estimates:\n      cor \n0.8179411 \n\n\nFind the minimum sepal width\n\nmin(iris$Sepal.Width)\n\n[1] 2\n\n\nTabulate the species to find the number of observations for each\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50"
  },
  {
    "objectID": "irisScript.html#graphing",
    "href": "irisScript.html#graphing",
    "title": "Iris dataset analysis walkthrough",
    "section": "2 Graphing",
    "text": "2 Graphing\nMake a histogram with red bars\n\nhist(iris$Sepal.Length, col=\"red\")\n\n\n\n\n\n\n\n\nTo quickly demonstrate the formula interface we made a (not very nice) boxplot.\n\nboxplot( Petal.Length ~ Species , data=iris)\n\n\n\n\n\n\n\n\nCould we have done this with the native pipe?\nThis doesn’t work (because if the arguments are unnamed iris needs to be the second argument)\n\niris |&gt; boxplot(Petal.Length ~ Species)\n\n\n\n\n\n\n\n\nBut either of these two lines does work:\n\niris |&gt; boxplot(Petal.Length ~ Species , data=_)\niris |&gt; boxplot(data=_ , Petal.Length ~ Species )\n\nWe can make a much nice boxplot using ggplot2. Make sure you understand what each of these lines does!\n\niris |&gt; ggplot() + \n  aes(x=Species , y=Petal.Length, fill=Species) + \n  geom_boxplot() + \n  ggtitle(\"Petal length by Species\") + \n  theme_bw() + \n  scale_y_log10(breaks=c(1,2,3,4,5,6,7,8,9,10)) + \n  labs(y=\"Petal length (cm)\")\n\n\n\n\n\n\n\n\nIf we want to export the plot we can assign it to an object then save that object with ggsave:\n\nboxplot1 &lt;- ggplot(iris) + \n  aes(x=Species , y=Petal.Length, fill=Species) + \n  geom_boxplot()\nggsave(\"lengthboxplot.png\",boxplot1, width=5, height=5, dpi=\"retina\")\n\nI was asked what would happen if we wanted to load data with no header. The best answer is ‘try it and see’. We make iris2.csv with no header row. When we tried to load it, the first row of values was assumed to be the header.\n\niris2 &lt;- read.csv(\"iris2.csv\")\nhead(iris2)\n\n  X1 X5.1 X3.5 X1.4 X0.2 setosa\n1  2  4.9  3.0  1.4  0.2 setosa\n2  3  4.7  3.2  1.3  0.2 setosa\n3  4  4.6  3.1  1.5  0.2 setosa\n4  5  5.0  3.6  1.4  0.2 setosa\n5  6  5.4  3.9  1.7  0.4 setosa\n6  7  4.6  3.4  1.4  0.3 setosa\n\n\nSo we looked at the help file to check how to correctly load the data, and how to assign names if we wanted to.\n\niris2 &lt;- read.csv(file = \"iris2.csv\",\n                  header=FALSE,\n                  col.names = c(\"X\", \"PL\", \"PW\", \"SW\", \"SL\", \"Species\"))\nhead(iris2)\n\n  X  PL  PW  SW  SL Species\n1 1 5.1 3.5 1.4 0.2  setosa\n2 2 4.9 3.0 1.4 0.2  setosa\n3 3 4.7 3.2 1.3 0.2  setosa\n4 4 4.6 3.1 1.5 0.2  setosa\n5 5 5.0 3.6 1.4 0.2  setosa\n6 6 5.4 3.9 1.7 0.4  setosa"
  },
  {
    "objectID": "irisScript.html#modelling",
    "href": "irisScript.html#modelling",
    "title": "Iris dataset analysis walkthrough",
    "section": "3 Modelling",
    "text": "3 Modelling\nMost of our statistical tests can be thought of in terms of statistical models. Simple linear regression models are the simplest models, we can do most things by generalisaing these.\nIn R, we use a formula to describe the model we want to estimate. Here we want to model how the average of sepal width varies (statistically, not necessarily causally) with sepal length.\n\nlm(data=iris , Sepal.Width ~ Sepal.Length)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length, data = iris)\n\nCoefficients:\n (Intercept)  Sepal.Length  \n     3.41895      -0.06188  \n\n\nTo do anything useful we need to make an lm object (here called model1)\n\nmodel1 &lt;- lm(data=iris , Sepal.Width ~ Sepal.Length)\n\nNow we extract the model summary\n\nsummary(model1)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1095 -0.2454 -0.0167  0.2763  1.3338 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.41895    0.25356   13.48   &lt;2e-16 ***\nSepal.Length -0.06188    0.04297   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4343 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\n\nGet the gtsummary package if you don’t have it already. (with install.packages(\"gtsummary\")).\nThe tbl_regression function from this package makes a nicely formatted regression table.\n\ngtsummary::tbl_regression(model1)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nSepal.Length\n-0.06\n-0.15, 0.02\n0.2\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWe can also use ggplot to draw a linear model: Note how the slope and intercept of the stat_smooth line corresponds to the model summary output\n\niris |&gt; ggplot() + \n  aes(x=Sepal.Length , \n      y=Sepal.Width, \n      ) + \n  geom_point() +\n  theme_bw() +\n  stat_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBy adding colours we can see that our first model may not be the best, because there is clearly an effect of species on the outcome variable that we have ignored. So we should estimate a separate line for each species, but consider that we have changed the reserach question quite substantially:\n\niris |&gt; ggplot() + \n  aes(x=Sepal.Length , \n      y=Sepal.Width, \n      shape=Species,\n      col=Species) + \n  geom_point() +\n  theme_bw() +\n  stat_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can make linear models corresponding to more complex relationships! This is an advantage of using linear models instead of other ‘ad hoc’ statistical testing. This model corrects for the effect of species on sepal width:\nEntering just the main effect (as below) lead to a separate but parallel line for each species. The slope (effect of sepal length) does not vary with species.\n\nmodel2 &lt;- lm(data = iris , Sepal.Width ~ Sepal.Length + Species )\nsummary(model2)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Species, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95096 -0.16522  0.00171  0.18416  0.72918 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.67650    0.23536   7.123 4.46e-11 ***\nSepal.Length       0.34988    0.04630   7.557 4.19e-12 ***\nSpeciesversicolor -0.98339    0.07207 -13.644  &lt; 2e-16 ***\nSpeciesvirginica  -1.00751    0.09331 -10.798  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.289 on 146 degrees of freedom\nMultiple R-squared:  0.5693,    Adjusted R-squared:  0.5604 \nF-statistic: 64.32 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nBut we can add an interaction term to allow the effect of sepal length to vary by species These two models are the same.\n\nmodel4 &lt;- lm(data = iris , Sepal.Width ~ Sepal.Length + Species + Sepal.Length:Species )\nmodel4 &lt;- lm(data = iris , Sepal.Width ~ Sepal.Length * Species )\nsummary(model4)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length * Species, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.72394 -0.16327 -0.00289  0.16457  0.60954 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     -0.5694     0.5539  -1.028 0.305622    \nSepal.Length                     0.7985     0.1104   7.235 2.55e-11 ***\nSpeciesversicolor                1.4416     0.7130   2.022 0.045056 *  \nSpeciesvirginica                 2.0157     0.6861   2.938 0.003848 ** \nSepal.Length:Speciesversicolor  -0.4788     0.1337  -3.582 0.000465 ***\nSepal.Length:Speciesvirginica   -0.5666     0.1262  -4.490 1.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2723 on 144 degrees of freedom\nMultiple R-squared:  0.6227,    Adjusted R-squared:  0.6096 \nF-statistic: 47.53 on 5 and 144 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use the emmeans package to extract the slope at each level of Species from model 4.\n\nemmeans::emtrends(model4 , pairwise~Species , var=\"Sepal.Length\")\n\n$emtrends\n Species    Sepal.Length.trend     SE  df lower.CL upper.CL\n setosa                  0.799 0.1104 144    0.580    1.017\n versicolor              0.320 0.0754 144    0.171    0.469\n virginica               0.232 0.0612 144    0.111    0.353\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast               estimate     SE  df t.ratio p.value\n setosa - versicolor      0.4788 0.1337 144   3.582  0.0013\n setosa - virginica       0.5666 0.1262 144   4.490  &lt;.0001\n versicolor - virginica   0.0878 0.0971 144   0.905  0.6382\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThe ANOVA function can be used to generate the old-fashioned ANOVA table corresponding to each model:\n\nanova(model4)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Width\n                      Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nSepal.Length           1  0.3913  0.3913   5.2757   0.02307 *  \nSpecies                2 15.7225  7.8613 105.9948 &lt; 2.2e-16 ***\nSepal.Length:Species   2  1.5132  0.7566  10.2011  7.19e-05 ***\nResiduals            144 10.6800  0.0742                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr to test whether one model is a significantly better fit than another:\n\nanova(model1 , model2, model4)\n\nAnalysis of Variance Table\n\nModel 1: Sepal.Width ~ Sepal.Length\nModel 2: Sepal.Width ~ Sepal.Length + Species\nModel 3: Sepal.Width ~ Sepal.Length * Species\n  Res.Df    RSS Df Sum of Sq       F    Pr(&gt;F)    \n1    148 27.916                                   \n2    146 12.193  2   15.7225 105.995 &lt; 2.2e-16 ***\n3    144 10.680  2    1.5132  10.201  7.19e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we see model 4 is better than model 2, (and model 2 is better than model 1). So there is good evidence that we should fit independent slopes for each species."
  },
  {
    "objectID": "irisScript.html#some-linear-algebra",
    "href": "irisScript.html#some-linear-algebra",
    "title": "Iris dataset analysis walkthrough",
    "section": "4 Some linear algebra",
    "text": "4 Some linear algebra\nI was asked about the maths underlying the linear regression.\nLinear Algebra for linear regression is straightforward, and is explained here (https://xebia.com/blog/the-linear-algebra-behind-linear-regression/).\nMost of you will not need to worry about this! You can calculate the coefficients by extracting the model matrix, then applying the following formula:\n\nX &lt;- model.matrix(model2)\nb = solve(t(X)%*%X) %*% \n  t(X) %*% \n  iris$Sepal.Width\nb\n\n                        [,1]\n(Intercept)        1.6765001\nSepal.Length       0.3498801\nSpeciesversicolor -0.9833885\nSpeciesvirginica  -1.0075104\n\n\nCompare it to:\n\ncoef(model2)\n\n      (Intercept)      Sepal.Length Speciesversicolor  Speciesvirginica \n        1.6765001         0.3498801        -0.9833885        -1.0075104 \n\n\nGet the residual error\n\nsigma &lt;- ((X %*% b - iris$Sepal.Width) |&gt; sd())*sqrt((149/146))\nsigma\n\n[1] 0.288989\n\n\ncompare it to:\n\nsigma(model2)\n\n[1] 0.288989"
  },
  {
    "objectID": "irisScript.html#day-3-script---a-bit-more-analysis-anova-and-log-transformation",
    "href": "irisScript.html#day-3-script---a-bit-more-analysis-anova-and-log-transformation",
    "title": "Iris dataset analysis walkthrough",
    "section": "5 Day 3 script - a bit more analysis, ANOVA and log-transformation",
    "text": "5 Day 3 script - a bit more analysis, ANOVA and log-transformation\nOn day three we looked at a few different graphs:\n\nlibrary(ggplot2)\nlibrary(ggbeeswarm)\n\nA box plot shows a great ‘five point summary’ for each distribution\n\nggplot(iris) + aes(x=Species, y=Petal.Length, fill=Species) + \n  geom_boxplot(outlier.colour = NA) + \n  geom_beeswarm(size=1)\n\n\n\n\n\n\n\n\nThe log-scale graph suggested that a log-transformed model might be more appropriate\n\nggplot(iris) + aes(x=Species, y=Petal.Length, fill=Species) + \n  geom_boxplot(outlier.colour = NA) + \n  geom_beeswarm(size=1) + \n  scale_y_log10()\n\n\n\n\n\n\n\n\nA ‘dynamite’ plot is less useful, but could be acceptable if you overlay the true data points.\n\nggplot(iris) + aes(x=Species, y=Petal.Length, fill=Species) + \n  stat_summary(geom=\"col\") + \n  stat_summary(geom=\"errorbar\", width=0.5) + \n  geom_beeswarm()\n\nNo summary function supplied, defaulting to `mean_se()`\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\n\n\nWe can estimate a model corresponding to these graphs. The 1+ tells R to include an intercept term. We don’t need to explicity say this, but if we don’t want the intercept then we need to use 0+.\n\nmodel_petal_length_species &lt;- lm(data=iris , Petal.Length ~ 1+Species)\n\nThis is an anova for the hypothesis that all the species differences are zero\n\nanova(model_petal_length_species)\n\nAnalysis of Variance Table\n\nResponse: Petal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals 147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model summary shows the individual regression coefficients\n\nsummary(model_petal_length_species)\n\n\nCall:\nlm(formula = Petal.Length ~ 1 + Species, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.260 -0.258  0.038  0.240  1.348 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.46200    0.06086   24.02   &lt;2e-16 ***\nSpeciesversicolor  2.79800    0.08607   32.51   &lt;2e-16 ***\nSpeciesvirginica   4.09000    0.08607   47.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4303 on 147 degrees of freedom\nMultiple R-squared:  0.9414,    Adjusted R-squared:  0.9406 \nF-statistic:  1180 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nWe can get the pairwise contrasts with emmeans.\n\nemmeans::emmeans(model_petal_length_species , pairwise ~ Species)\n\n$emmeans\n Species    emmean     SE  df lower.CL upper.CL\n setosa       1.46 0.0609 147     1.34     1.58\n versicolor   4.26 0.0609 147     4.14     4.38\n virginica    5.55 0.0609 147     5.43     5.67\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast               estimate     SE  df t.ratio p.value\n setosa - versicolor       -2.80 0.0861 147 -32.510  &lt;.0001\n setosa - virginica        -4.09 0.0861 147 -47.521  &lt;.0001\n versicolor - virginica    -1.29 0.0861 147 -15.012  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWe said the log-transformation might be needed. We can estimate a model for log(Petal.Length)\n\nmodel_petal_length_species_log &lt;- \n  lm(data=iris , log(Petal.Length) ~ Species)\n\nNow emmeans reports ratios instead of differences. Could make more sense biologically!\n\nemmeans::emmeans(model_petal_length_species_log , \n                 pairwise ~ Species, type=\"response\")\n\n$emmeans\n Species    response     SE  df lower.CL upper.CL\n setosa         1.45 0.0230 147     1.41     1.50\n versicolor     4.23 0.0670 147     4.10     4.37\n virginica      5.53 0.0874 147     5.36     5.70\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n$contrasts\n contrast               ratio      SE  df null t.ratio p.value\n setosa / versicolor    0.343 0.00767 147    1 -47.835  &lt;.0001\n setosa / virginica     0.263 0.00588 147    1 -59.747  &lt;.0001\n versicolor / virginica 0.766 0.01714 147    1 -11.912  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \nTests are performed on the log scale"
  },
  {
    "objectID": "praise.html",
    "href": "praise.html",
    "title": "Packages and Pipes",
    "section": "",
    "text": "R is mainly for statistical computing but there are add-on packages to do many other things. You are likely to need to use add-on packages for reading Excel files, data manipulation, running specific analyses, creating nice reports etc.\nIn this tutorial we will get used to using R packages, objects, functions, pipes and how to use the R help system by making ascii animals say nice things to us."
  },
  {
    "objectID": "praise.html#ok-thats-enough-fun.",
    "href": "praise.html#ok-thats-enough-fun.",
    "title": "Packages and Pipes",
    "section": "5.1 OK that’s enough fun.",
    "text": "5.1 OK that’s enough fun.\nOK now back to the real work."
  },
  {
    "objectID": "sampleSize.html",
    "href": "sampleSize.html",
    "title": "How many samples do I need?",
    "section": "",
    "text": "These are the course materials for the one-day course on sample size considerations, delivered for the NBI training programme.\nLatest version is June 2024.\nThe Powerpoint slides are here:\n\nSample Size Slides\n\nExercise handout is here:\n\nHandout\n\nSolutions to the exercises are here\n\nSolutions"
  }
]