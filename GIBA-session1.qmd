---
title: "Session1"
format: html
engine: knitr
webr: 
  show-startup-message: false    # Disable displaying status of webR initialization
  packages: ['ggplot2', 'ggpubr','easystats','report','ggbeeswarm'] # Install R packages on document open
filters:
  - webr
---

```{webr-r}
#| autorun: true
#| message: false
#| warning: false
library(ggplot2)
library(ggpubr)
library(report)
library(ggbeeswarm)
library(patchwork)

```


## Introduction

These pages include slides and code to support the first day of our course in experimental design and analysis.

Slides for session 1 are here: 

* [Session 1 slides](gibasession1.pptx)

The code in these windows can be run directly from your browser,
or copied into your own R session.

If you want to use your own R sesion, you'll need to have packages installed:

`ggplot2`, `see`, `ggpubr`, `ggbeeswarm`, `patchwork` and `easystats`

## Worked example

Here we simulate a simple experiment with two groups of mice and a single outcome measure.

We set the 'true' average values of our outcome measure, simulate an experiment with our chosen design, and then attempt to estimate the difference between groups using our simulated data.

Using simulations like this can help us to think through experiments, plan analyses and ultimately run sample size calculations.

### Simulate the data

```{webr-r}
#| warning: false

set.seed(1)

N1=5 # sample size for group 1
N2=3 # sample size for group 2

# true mean values in each group
mean = c("Control"=50,
         "Treated"=50)

# Make group labels
group = c( rep("Control",N1) , rep("Treated",N2) ) |> factor(levels=c("Control","Treated"))

# Simulate richness, assume it has a negative binomial distribution with size 1 and mean defined above
richness = rnbinom(N1+N2, mu=mean[group],size=1)

# Turn this into a dataset
dat = data.frame(group,richness)

# List the first few rows
head(dat)
```

### Initial analysis

Now we have simulated the data we can run our analysis, in this case a bar chart with error bars combined with a p-value to compare the groups.

```{webr-r}

ggplot(dat, aes(x=group,y=richness)) +  
  stat_summary(geom="col",col="black",fill="red",width=0.6, fun.data = "mean_se") + 
  stat_summary(geom="errorbar" , width=.30, fun.data = "mean_se") + 
  stat_compare_means(comparisons = list(1:2), method="t.test") + 
  
  theme_bw()

```

What would you conclude?

### A better graph and statistics

While this presentation of results (p-value and dynamite plot) is often used, it is extremely limiting.  It is more useful to present data point individually (perhaps with summary statistics) and then to make inferences about the estimated  mean difference between groups:

```{webr-r}

ggplot(dat, aes(x=group,y=richness)) +  
  ggbeeswarm::geom_beeswarm(pch=4) + 
  stat_summary(geom="errorbar" , width=.30, fun.data = "mean_se") +
  stat_summary(geom="point" , fun.data = "mean_se") + 
  stat_compare_means(comparisons = list(1:2), method="t.test") + 
  theme_bw()

```

```{webr-r}

report_sample(dat , by="group")

t.test(data=dat , richness ~ group) |> report::report_parameters()

```

It's interesting to compare the linear model result with the t-test result.

```{webr-r}
model1 <- lm(data=dat , richness ~ group) 
summary(model1)
confint(model1)
model1 |> parameters::parameters() |> plot()
```



## Accuracy and precision

Suppose we conduct a research study to estimate the effect of our supplement on the alpha diversity.

Each instance of our study will produce an estimate for the effect, which is (hopefully!) close to the truth.

Suppose it was possible to repeat the study many times, you would produce a different effect estimate from each one.

The study is *unbiased* if the mean average value of those repeats is equal to the true value. That is, while there will be some chance variation the estimate will not be systematically too high or too low.

Any bias in the study would cause the study estimate to be systematically wrong in either direction.  There are many sources of bias.  For example we know that healthier people tend to more willing to participate in research than the population (selection bias), or people who are treated with placebos might report better symptoms than those who are not (plaebo effect).  These can lead to systematically wrong conclusions about populations or the efficacy of treatments.

The precision of the study refers to how similar the estimates from repeated runs would be.  A precise estimates have small standard errors and narrow confidence intervals.  But it is possible to be very precisely wrong (if there is bias), or to miscalculate the precision of your study.

Precision is largely determined by the level of variation between in your experimental units, your measurements, and by the sample size.  Increasing sample size and reducing sources of variation will increase precision.


```{webr-r}

N=10
truth = 10
biasedEstimates = rnorm(N,truth+3,0.5)
impreciseEstimates = rnorm(N,truth,2)
biasedAndImprecise = rnorm(N,truth+3,2)
goodEstimates = rnorm(N,truth,0.5)

rbind(goodEstimates,biasedAndImprecise,impreciseEstimates,biasedEstimates) |> 
  as.data.frame.table() |>
  setNames(c("Type","Var","Estimate")) |>
  ggplot(aes(x=Type,y=Estimate)) + 
  geom_beeswarm() + 
  geom_hline(yintercept=truth,lty="dotted") + 
  coord_flip() + 
  theme_bw() + 
  labs(y="Estimates", x=NULL) + 
  scale_x_discrete(labels=c("biasedEstimates"="Biased Estimates",
                            "impreciseEstimates"="Imprecise Estimates",
                            "biasedAndImprecise"="Biased and Imprecise",
                            "goodEstimates"="Precise and unbiased (good!)")) + 
  annotate(geom="text",label="True effect size",x=0.5,y=truth)


```

Our goal in experimental design is to answer your research questions with low bias *and* high precision, so that your estimates are consistently close to the truth!  On the whole bias is more problematic than imprecision, because the latter can be calculated, reported, and overcome using larger studies.  Bias is much more difficult to detect, and can become compounded by larger samples or repeated studies if the same biases are present each time.

To deal with sources of bias and variance, we have to understand how they arise, and how to reduce or remove their effects on our estimates.


## Analysis of a blocked randomised study

### Introduction

The code below simulates a parallel group randomised controlled trial conducted in men and women.

There were 40 participants in total, 20 men and 20 women.

There was a blocked randomisation, so that men and women were balanced across groups (10 each per treatment group)

The blocking was performed because we know that men have typically higher responses than women.


```{webr-r}

# Define the groups
treatment = c("Treat", "Control")
sex = c("Male","Female")

# Here we set up the dataset with 10 copies of each combination of treatment and sex
dat <- expand.grid( treatment=treatment , sex=sex,rep=1:10)

# True treatment effect = 1
# True sex effect = 2
# Normally distributed indidividual variation with sd=1

set.seed(200) # Setting the seed fixes the response

# Now generate the response (y) with the known treatment effect, sex effect and random variation
dat <- transform(dat, y = 10 + (treatment=="Treat") + 2*(sex=="Male") + rnorm(nrow(dat)))

# Check the first few rows
head(dat)

```

Remember the experimental design equation for this study is:

`Outcome = Treatment + Sex + Error`

Below we show why it is important to respect this equation in our analysis phase as well as our design phase.

Since we have a two group study with a continuous outcome, we might be tempted to apply a two-groups t-test to compare the responses:

```{webr-r}
# The first graph 
graph1 = ggplot(dat) + 
  aes(x=treatment, y=y) + 
  geom_beeswarm(aes(shape=sex)) + 
  theme_bw()+ 
  stat_summary(geom="errorbar",width=.5,fun.data = "mean_se") + 
  scale_y_continuous(limits=c(8,15)) + 
  ggpubr::stat_compare_means(method="t.test",comparisons=list(1:2))  + 
  labs(x="Treatment", y="Response",shape="Sex")
```

While there is some evidence of the difference between groups (that we know is there!), it is not statistically significant.  We have type-2 error in this case (false negative).  This is caused by the high variation within each treatment group overwhelming the signal from the treatment.

But if we stratify the data by sex, we explain much of that variation.  The signal is much more obvious, and is statistically significant in both groups despite there being half the participants in each!

```{webr-r}
graph1 + (graph1 + facet_wrap(~sex) ) + plot_layout(guides="collect")
```

The linear model corresponding to the unpaired t-test is given below. Note the residual error in the output and the standard error for the effect of treatment.

```{web-r}
lm(data=dat , y ~ treatment) |> summary()
```

Next we'll try the model that corresponds to the design equation.  We have explained far more variance (smaller residual error), and so have a m.

```{web-r}
lm(data=dat , y ~ treatment + sex) |> summary()
```

Although this dataset was picked to illustrate the point, we can find the power of the study with each analytical approach by replicating the study many times, and finding what proportion of p-values is less than 0.05.

```{webr-r}

oneRep <- function(){
  dat <- transform(dat, y = 10 + (treatment=="Treat") + 2*(sex=="Male") + rnorm(nrow(dat)))
  model1_p <- lm(data=dat , y ~ treatment) |> summary() |> coef())["treatmentControl","Pr(>|t|)"]
  model2_p <- lm(data=dat , y ~ treatment + sex) |> summary() |> coef())["treatmentControl","Pr(>|t|)"]
  c(model1_p,model2_p)
}

replicate(1000,oneRep()) |> t() |> as.data.frame()|> lapply(\(x) mean(x<0.05))

```

Here, using the design equation to set up the analysis model leads to a much more powerful design, by explaining the variance associated with the identified factors.  It also means that the assumptions underlying the regression model, in particular normal random errors, more likely to be met.

```{web-r}
lm(data=dat , y ~ treatment + sex) |> report::report_model()
```

