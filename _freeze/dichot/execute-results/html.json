{
  "hash": "03a03f45ff5642d5e2b0f592e9fde60a",
  "result": {
    "markdown": "---\ntitle: \"Why is 'grouping' your samples bad for analysis?\"\nauthor: \"George Savva, (QIB)\"\n---\n\n\n\n\n# Summary\n\nIf we have continuous data then we should keep it continuous in analysis.  Grouping samples into (say) 'high' vs 'low' or 'recovered' vs 'not recovered' throws away information and makes it more difficult to detect associations.  The power of your study is reduced and the sample size needed goes up.\n\n\n::: {.cell}\n\n:::\n\n# Motivation\n\nSuppose we are interested in the effect of an exposure on an outcome, and we have measured both in a sample.  Both our exposure and our outcome are measured as continuous variables, for example we might be interested in the effect of fibre intake on gut microbial diversity.\n\nThere are a couple of possible approaches to the analysis.  First, we could estimate or test for a *correlation* between the exposure and the outcome.  \n\nAlternatively we could dichotomise the exposure, splitting the samples into a \"high fibre\" and a \"low fibre\" group, before comparing the microbial diversity in each group.  Or we could dichotomise the outcome into \"high diversity\" and \"low diversity\".  \n\nWhich should we choose?  Intuitively we might prefer the dichotomised version for \"ease of interpretation\", but how does it affect our ability to detect any association?\n\n# Packages\n\nWe'll need the following R packages for this simulation:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pwr)\nlibrary(rmarkdown)\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(MASS)\nlibrary(data.table)\n```\n:::\n\n\n\n# Dataset\n\nSuppose our exposure and outcome are both normally distributed with a correlation of 0.5.\n\nWe can sample 50 points as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\ndat = data.frame(mvrnorm(n=30, mu=c(10,10), Sigma = matrix(c(1,0.5,0.5,1),nrow=2)))\nnames(dat) <- c(\"Exposure\", \"Outcome\")\n\nggplot(dat, aes(x=Exposure, y=Outcome)) + geom_point() + theme_bw()\n```\n\n::: {.cell-output-display}\n![](dichot_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nHow can we estimate or test this correlation by statistical analysis?\n\nThe simplest way would be to test\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(dat$Exposure, dat$Outcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  dat$Exposure and dat$Outcome\nt = 3.0861, df = 28, p-value = 0.004535\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1753575 0.7313294\nsample estimates:\n      cor \n0.5037987 \n```\n:::\n:::\n\n\nSo in our sample this correlation is clearly detectable (estimated r=0.50, p=0.0045).\n\n# Dichotomising variables\n\nDichotomising means splitting variables into two groups.  In our example we might decide to compare those with a `high` vs a `low` exposure.  Lets make a variable corresponding to whether the exposure for each participant is above observed median.  This will split the data into two groups, and we can describe how the outcome differs between the high and low exposure groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$HighExposure <- dat$Exposure > median(dat$Exposure)\n(ggplot(dat, aes(Exposure, Outcome, color=HighExposure)) + \n  geom_point() + theme_bw() + \n  scale_color_manual(values=c(\"black\", \"red\")))+\n(ggplot(dat, aes(HighExposure, Outcome)) + \n  geom_boxplot() + geom_point(aes(color=HighExposure)) + theme_bw()+ \n  scale_color_manual(values=c(\"black\", \"red\")))\n```\n\n::: {.cell-output-display}\n![](dichot_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nNow we can use (for example) a t-test to for difference in outcome between \"high\" and \"low\" exposure groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(data=dat, Outcome ~ HighExposure)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  Outcome by HighExposure\nt = -2.3855, df = 27.985, p-value = 0.02407\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -1.5826037 -0.1203041\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           9.723858           10.575312 \n```\n:::\n:::\n\n\nThe difference is now barely detectable (p=0.024)!\n\nWe could go further, and dichotomise both the outcome and the exposure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$HighOutcome <- dat$Outcome > median(dat$Outcome)\n(ggplot(dat, aes(Exposure, Outcome, color=HighExposure, shape=HighOutcome)) + \n  geom_point() + theme_bw() + geom_hline(yintercept=median(dat$Outcome),lty=2)+\n  scale_color_manual(values=c(\"black\", \"red\")))+\n(ggplot(dat, aes(HighExposure, as.numeric(HighOutcome))) + \n  geom_bar(stat=\"summary\", fun.y=mean, col=\"black\", aes(fill=HighExposure)) + theme_bw()+\n   labs(y=\"Proportion with high outcome\")+\n  scale_fill_manual(values=c(\"black\", \"red\")))\n```\n\n::: {.cell-output-display}\n![](dichot_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nNow our relationship seems obscured. Our analysis consists of analysing a 2x2 contingency table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(dat$HighExposure, dat$HighOutcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       \n        FALSE TRUE\n  FALSE    10    5\n  TRUE      5   10\n```\n:::\n\n```{.r .cell-code}\nchisq.test(table(dat$HighExposure, dat$HighOutcome))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(dat$HighExposure, dat$HighOutcome)\nX-squared = 2.1333, df = 1, p-value = 0.1441\n```\n:::\n:::\n\n\nThe difference in 'high outcome' proportions between exposure groups is not statistically significant! (p=0.14).  This shows that it is much more difficult to see the relationship if we throw away the exact values of the data points.\n\nSo although there is a real relationship between exposure and outcome that we have been able to detect by correlating the values in our sample, it was harder to detect when we dichotomised the exposure, and we could not detect it at all in the grouped data.\n\n# Implications for power and sample size\n\nOur single example illustrated that dichotomising data made analysis difficult or impossible by discarding data.  But it's easy to cherry pick examples, how does this work in general?\n\nWe can quantify the average loss of information by power calculations (repeated simulations), and consider how much our sample size would need to increase to overcome this.\n\nThe graph below shows the power to detect a correlation of 0.5 as the sample size changes using each of the methods described above.\n\nIf we analyse the continuous data then we need a sample size of about 28 for a power of 80%.  If we dichotomise the exposure only we would need 43 samples, if we dichotomise both and analyse the 2x2 table we would need 69, more than twice the original sample. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsampleSizes <- seq(10,100,5)\n\npowers <- data.table(t(sapply(sampleSizes, function(n){c(\n  sampleSizes=n,\n  continuous=100*pwr.r.test(r=0.5, n = n)$power,\n  dichotone=100*pwr.t.test(d=.8 / 0.91, n = n/2)$power,\n  dichotboth=100*pwr.2p.test(h=ES.h(1/3, 2/3), n = n/2)$power\n  )\n  })))\n\nggplot(melt(powers,id.vars = \"sampleSizes\", variable.name = \"Analysis\", value.name = \"Power\"), \n       aes(sampleSizes, Power, color=Analysis)) + \n  geom_line() + \n  geom_point() + \n  theme_bw() + \n  scale_x_continuous(limits=c(0,100)) + \n  labs(x=\"Sample Size (total)\", y=\"Power (%)\") + \n  scale_color_manual(labels=c(dichotboth=\"Dichotomise both\", \n                              dichotone=\"Dichtomise outcome only\", \n                              continuous=\"Correlate continuous measures\"), \n                     values=c(\"black\", \"red\", \"blue\")) + \n  theme(legend.position = c(0.7,0.3), legend.background = element_rect(linetype = 1, size = .5, colour = \"black\")) + \n  geom_hline(yintercept=80, lty=2)\n```\n\n::: {.cell-output-display}\n![](dichot_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n# Conclusion\n\nGrouping up our continuous variables for analysis throws away information.  It makes associations more difficult to detect, and increases the number of samples we need to analyse.\n\nThere are other harmful consequences to dichotomising data.  Sometimes a defense of the practice is that by collapsing quantiative findings into yes/no outcomes makes results easier to interpret, but Stephen Senn makes a good case that this is wrong and can even be misleading.\n\nSo in general, if the phenomenon you are interested in can be measured continuously then you should do so, and be sure to use all of this quantitative information in the analysis.\n\n# References (todo - tidy this up)\n\nThis phenomenon has been discussed many times in the statistical literature and in blogs\n\n<https://errorstatistics.com/2016/08/02/s-senn-painful-dichotomies-guest-post/> (Senn 2016 - a not identical but related problem with dichtomies)\n\n<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1458573/> (Altman and Royston 2006)\n\n<http://www.psychology.sunysb.edu/attachment/measures/content/maccallum_on_dichotomizing.pdf> (MacCallum et al 2002)\n\n<https://bjo.bmj.com/content/98/6/841> (Cumberland et al 2014)\n\n",
    "supporting": [
      "dichot_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}