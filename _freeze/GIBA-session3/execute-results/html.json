{
  "hash": "4c603f2f363837615df76752ff383281",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sample Size\"\nformat: html\nengine: knitr\nwebr: \n  show-startup-message: false    # Disable displaying status of webR initialization\n  packages: ['ggplot2', 'ggpubr','easystats','report','ggbeeswarm','pwr'] # Install R packages on document open\nfilters:\n  - webr\n---\n\n\n\n\n```{webr-r}\n#| echo: false\n#| autorun: true\n#| message: false\n#| warning: false\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(report)\nlibrary(ggbeeswarm)\n```\n\n\nSlides for the afternoon session on sample size are here: \n\n* [Session 3 slides](gibasession3.pptx)\n\n## Exercise 1.  Interpretation of a clinical study\n\nConsider the following abstract reporting a clinical trial of reducing mortality by peripheral perfusion compared to lactate among patients in intensive care with septic shock.\n\n![Graphical Abstract](images/andromeda.png){.lightbox}\n \na)\tWhat was the mortality rate in the treatment arm?  In the control arm?\na)\tWhat was the estimate for treatment effect?\na)\tWhat was its confidence interval? (check our understand of 95% CI)\na)\tWhat conclusion did the authors/journal draw?  Why?\na)\tWhat conclusions do you draw from this trial?\na)\tIf you or a loved one was in septic shock, which treatment would you prefer?\na)\tWould you allow yourself to be randomised to these treatments in a future trial?\n\n```{webr-r}\n# Use this space if you like but remember it isn't saved!\n```\n\n## Exercise 2.  How big should a study to estimate a proportion be?\n\nConsider a study to estimate the prevalence of detectable virus in the stoll samples of COVID patients.\n\nWe can think about the sample size required in terms of the precision that the study needs to have\n\nSuppose we recruited 10 patients, and found two positive samples.  We can use `binom.test` in R to find the estimate and its 95% confidence interval.\n\n```{webr-r}\nbinom.test(x = 2,n=10)\n```\n\nAdapt the code to answer the following questions (leave notes in the code box if you like but remember these won't be saved):\n\na) What would be the confidence interval if we recruited 100 participants and found 20 positive samples?\na) Suppose we expect the prevalence to be about 20%.  How many samples will we need to estimate the prevalence to within +-5 percentage points.\na) What if the prevalence was only 10%?\na) How many samples should we collect for this study?\na) Based on this study, what do we need to think about when deciding on a sample size?\n\n## Exercise 3.  How big should a study to estimate a mean difference be?\n\nWe are interested in the number of mice needed to test the effect of supplement on fecal alpha diversity.\n\nFirst we'll explore replications of a single study, to revise our understanding of hypothesis tests, think about the interpretation of p-values and meaning of statistical power.\n\nThis first code block simulates a study, and generates a dataset.  Choose any number for the seed, this will ensure that we all get independent runs of the experiment!\n\nLook at the assumptions we need to make to simulate the dataset.\n\n```{webr-r}\n#| warning: false\n\nset.seed(1) # choose your own seed so that we each get a different instance of this study\n\nN=10  # number per group\nN1=N # sample size for group 1\nN2=N # sample size for group 2\nsd=10 # within-group standard deviation\n\n# true mean values in each group\nmean = c(\"Control\"=50,\n         \"Treated\"=50+10)\n\n# Make group labels\ngroup = c( rep(\"Control\",N1) , rep(\"Treated\",N2) ) |> factor(levels=c(\"Control\",\"Treated\"))\n\n# Run the experiment diversity, assume it has a normal distribution with standard deviation\nrichness = rnorm(N1+N2, mean=mean[group],sd=sd)\n\n# Turn this into a dataset\ndat = data.frame(group,richness)\n\n# List the first few rows\nhead(dat)\n\n```\n\nNext we can make beeswarm graph with a t-test to compare means:\n\n```{webr-r}\nggplot(dat, aes(x=group,y=richness)) +  \n  ggbeeswarm::geom_beeswarm(pch=4) + \n  stat_summary(geom=\"errorbar\" , width=.30, fun.data = \"mean_se\") +\n  stat_summary(geom=\"point\" , fun.data = \"mean_se\") + \n  stat_compare_means(comparisons = list(1:2), method=\"t.test\") + \n  theme_bw()\n\n```\n\nCalculate the mean and standard deviation by group, run a t-test.\n\nLets stop here to make sure that we completely understand the t-test result.\n\n```{webr-r}\nreport_sample(dat , by=\"group\")\n\ncat(\"\\n### t-test ### \\n\")\n\nt.test(data=dat, richness ~ group)\n\ncat(\"\\n### t-test report ### \\n\\n\")\n\nt.test(data=dat, richness ~ group) |> report()\n\n# This is very slightly different to the t-test, but close enough!\nlm(data=dat, richness ~ group) |> parameters::parameters() |> plot()\n\n\n```\n\nLets pause here to answer the following questions:\n\na) How precise was the your initial effect estimate with 10 mice per group?\nb) How does this change with increasing sample size?\n\n### Reproducibility of the study, implication for power\n\nTo understand what the *sampling distribution* of our estimator is, and to better understand the likely reproducibility of this study, we can run a large number of simulations:\n\n```{webr-r}\n#| warning: false\noneRep <- function(sampleSize=10,effect=10,sd=10){\n  \n  N=sampleSize  # number per group\n  N1=N # sample size for group 1\n  N2=N # sample size for group 2\n  \n  sd=sd\n  \n  # true mean values in each group\n  mean = c(\"Control\"=50,\n           \"Treated\"=50+effect)\n  \n  # Make group labels\n  group = c( rep(\"Control\",N1) , rep(\"Treated\",N2) ) |> factor(levels=c(\"Control\",\"Treated\"))\n  \n  # Run the experiment diversity, assume it has a normal distribution with standard deviation\n  richness = rnorm(N1+N2, mean =mean[group],sd = sd)\n  \n  # Turn this into a dataset\n  dat = data.frame(group,richness)\n  \n  # List the first few rows\n  test=t.test(data=dat , richness ~ group)\n  \n  # Now return the p-value, the estimate (b), and the confidence interval\n  c(\"p\"=test$p.value,\n    \"b\"=as.numeric(diff(test$estimate)),\n    \"CI\"=-rev(test$conf.int)\n    )\n}\n\noneRep()\n```\n\n```{webr-r}\nreplicate(5, oneRep() , simplify = TRUE) |> t() |> data.frame()\n```\n\n```{webr-r}\nNsim=20\nreps <- replicate(Nsim, oneRep(10,10,10) , simplify = TRUE) |> t() |> data.frame()\nreps$ber=1:nrow(reps)\n\n# Make a histogram of the p-values\nhist(reps$p,breaks=20)\n\n# What proportion of results are 'significant'\nmean(reps$p<0.05)\n\n# Forest plot of the effects \nggplot(reps) + \n  aes(x=ber,y=b,ymin=CI1,ymax=CI2,col=p<0.05) + \n  geom_pointrange() + \n  coord_flip() + \n  geom_hline(yintercept=0) + \n  scale_x_continuous(breaks=1:Nsim) + \n  geom_hline(yintercept=10,linetype=\"dotted\")\n\n```\n\nQuestions for discussion:\n\na) How reliable was the p-value across experiments?\na) Did the experiments agree or disagree with respect to the efficacy of the treatment?\na) How many mice would be ‘enough’ to reliably detect the effect that we introduced?\na) How many mice would be enough to make a power of 80% ?\na) How does this change if we change the size of the effect?\na) How does this change if the mice become more variable?\n\n### Implications for 'running a study many times'\n\nAs we've seen, p-values are not reliable.  It is expected that given the same study design, sometimes you'll get a significant difference and sometimes not.\n\nThis is a consequence of the variability of the experimental units, and does not represent a failure of the system or changing conditions in \n\nSo there is little value in repeating a study (say) three times and hoping for agreement.\n\nIf we had that resource then, it would be better to create one larger study, with blocks if necessary.\n\nThe graph below shows how the precision would increase if we had pooled results from repeats instead of trying to interpret each individually.\n\n```{webr-r}\n# Simulate a new batch of simulations\nreps <- replicate(Nsim, oneRep(10,10,10) , simplify = TRUE) |> t() |> data.frame()\n\n# Calculate the 'cumulative' estimates of effect as we add studies\nreps<-within(reps, {\n  ber<-1:nrow(reps)\n  precision<-(abs(CI2-CI1)/4)^(-2)\n  cumprecision<-cumsum(precision)\n  cummean<-cumsum(b)/1:nrow(reps)\n  cumCI_upper<-cummean+2*cumprecision^(-0.5)\n  cumCI_lower<-cummean-2*cumprecision^(-0.5)\n})\n\n# Plot the cumulative results\nggplot(reps) + \n  aes(x=ber,y=cummean,ymin=cumCI_lower,ymax=cumCI_upper) + \n  geom_pointrange() + \n  coord_flip() + \n  geom_hline(yintercept=0) + \n  scale_x_continuous(breaks=1:Nsim) + \n  geom_hline(yintercept=10,linetype=\"dotted\") + \n  ggtitle(\"'Meta-analysis' of replicated studies\")\n\n```\n\n\n\n## Exercise 4 - What is statistical power?\n\nCheck our understanding of the following terms\n\na) What does it mean if an experiment has a power of, say, 80%?\na) What is an ‘under-powered’ study?\na) What would you learn from an underpowered study?\na) How would you know if a study that you are reading was under-powered?\n\n## Exercise 5 – Logic of statistical inference\n\nWhy do scientists ignore sample size issues?\n\nIn my experience, we often have this paradigm for our statistical analyses:\n\n1)\tDo experiment\n2)\tDo analysis\n3)\tIf p>0.05 – we have proved there is no effect, \n4)\tif p<0.05 – we have proved there is an effect\n\nWhat’s wrong with this logic?  Can you correct it?\n\n## Exercise 6.  Using the pwr package to find sample size analytically\n\nWe will use a simple parallel group study to investigate this.\n\nWe believe from prior work that the within-group standard deviation for mouse alpha diversity is 1.\n\nUse the pwr function to answer the following questions:\n\n 1. Suppose we wanted our experiment to reliably detect an effect of 0.5 units, should it exist.  How many mice would we need?\n 2. If we could only afford to use ten mice per group, what would be the smallest effect size that we could detect?\n 3. What is the power to detect a difference of 0.5 units, given that we only have ten mice per group.\n 4. Remember we might not get data from all ten mice that were included.  If only 80% are expected to survive and give usable data, would would the power be?\n 5. How many mice would Mead's resource equation suggest that we use?\n 6. What would be the smallest detectable effect with this number?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reminder that the syntax for power is:\n# pwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL, \n#    type = c(\"two.sample\", \"one.sample\", \"paired\"),\n#    alternative = c(\"two.sided\", \"less\", \"greater\"))\n#\n# You need to include all but one of: n,d,sig.level and power.\n#\n# eg:\n# \n# library(pwr)\n#\n# pwr.t.test(n=NULL, d=0.5, power=0.8, sd=0.05)\n```\n:::\n\n\n\n\n## Exercise 8. Sample size for comparing two proportions\n\nSuppose we are interested in comparing an event incidence rate between two independent groups.  \n\nWe expect in our model that 50% of the participants will develop the outcome.  We expect our intervention will lower this to 20% percentage points.  How many participants would we need to be able to detect this difference?\n\n```{webr-r}\n\nlibrary(pwr)\n\nh = ES.h(0.5,0.4)\npwr.2p.test(h , n=NULL, power=0.9)\n\n```\n\nNow adapt the code to check how many participants would be needed to detect a difference between 50% and 40%.\n\n## Worked Example: Effective sample size with clustered data\n\nConsider an observational study in which retinal nerve fibre layer thickness (RNFL) is compared between patients with or without rheumatoid arthritis.\n\nPatients are to be randomly sampled from their respective populations, and each patient has a measurement taken in their left and their right eye.\n\nHow should we analyse this data, what are the implications if of using the wrong model, and what are the implications for power and sample size?  How much extra information do we get from measuring the second eye in each participant?\n\nFirst we'll set up the simulation study:\n\n```{webr-r}\n\nN=20      # Study size per group\nrho=0.5   # Intra-class correlation between left and right eye\nsigma=10  # Standard deviation of individual eyes\nmean=100  # Grand mean\neffect=10 # True effect of RA on RNFL\neyes = c(\"L\",\"R\")\ngroups = c(\"Disease\",\"Control\")\n\n# Find the standard deviation between and within samples based on the overall SD and the ICC\nsd_between = sigma*sqrt(rho)\nsd_within = sigma*sqrt(1-rho)\n\ndat <- expand.grid(eye=eyes,patient=1:(2*N))\ndat$group <- rep(groups,each=2*N)\n\npatientEffects = rnorm(2*N,0,sd_between)\n\ndat <- transform(dat , rnfl = mean + patientEffects[patient] + effect*(group==\"Disease\") + rnorm(nrow(dat),0,sd_within))\n\n```\n\nIt is valuable to plot the data to check the extent of the correlation between the eyes\n\n```{webr-r}\nggplot(dat ) + aes(x=eye , y=rnfl) + \n  geom_point() + \n  geom_line(aes(group=patient)) + \n  facet_wrap(~group)\n```\n\nCan we assume that eyes are independent observations in the analysis?\n\n```{webr-r}\nggplot(dat ) + aes(x=group , y=rnfl) + \n  ggbeeswarm::geom_beeswarm() + \n  ggpubr::stat_compare_means(method=\"t.test\",comparisons = list(1:2))\n  \n```\n\nWe could treat eyes as technical replicates, and take the average for each participant before running the t-test.\n\n```{webr-r}\naggregate(dat , rnfl ~ patient + group, mean) |>\nggplot() + aes(x=group , y=rnfl) + \n  ggbeeswarm::geom_beeswarm() + \n  ggpubr::stat_compare_means(method=\"t.test\",comparisons = list(1:2))\n```\n\nThis is OK, we now have one measure per person, and each person was sampled independently.\n\nSo did we get any benefit from using two eyes instead of one, even though our sample size was still the same?\n\nYes, because of the repeated measurement our data for each person became less variable.\n\nCompare the standard deviations if we used the left eye only compared to the aggregated data:\n\n```{webr-r}\ndat |> subset(eye==\"L\") |> report::report_sample(by = \"group\",select = \"rnfl\")\n\naggregate(dat , rnfl ~ patient + group, mean) |> report::report_sample(by = \"group\",select = \"rnfl\")\n```\nThe relative increase in precision depends on how much of the overall variation is found within eyes vs between eyes.  We set this above with the ICC (intraclass correlation coefficient).  Here we set ICC=0.5.\n\nWe can show that the effective sample size using two eyes is\n\n$$\nN\\times\\frac{2}{1+\\text{ICC}}\n$$\n\nwhich in this case is $1.33 \\times N$\n\nSo by using both eyes in this case we effectively increase the sample size by 30%.\n\nA general formula for the effective sample size, for clusters of more than two (say mice within cages, is)\n\n$$\nN_{\\text{Eff}} = N\\times\\frac{m}{1+(m-1)\\times\\text{ICC}}\n$$\nWhere $N$ is the number of units, and $m$ is the number of clusters.\n\nSo suppose we allocated a treatment to mice, and they have a shared outcome such that the intra-class correlation is about 0.3.  This might be typical for microbiome measures, stress responses, inflammatory responses etc.  Then the effective sample size increases with the number of animals as follows:\n\n```{webr-r}\nneff <- function(n,m,icc){\n  m / (1+(m-1)*icc)\n}\n\ndat <- expand.grid(m=1:5,icc=c(0.1,0.3,0.5))\ndat$neff <- neff(1,dat$m,dat$icc)\n\nggplot(dat) + aes(x=m,y=neff,linetype=factor(icc)) + geom_point() + geom_line() + \n  labs(x=\"Number of mice per cage\",y=\"Effective sample size\",linetype=\"ICC\") +\n  ggtitle(\"Effective sample size per cluster when treatments allocated on a per-cluster basis\")\n\n\n```\nThis adds an extra complication to a sample size calculation, but you make reasonable assumptions based on prior work regarding what the ICC might be.  In any case, simply assuming that mice are independent could lead you to vastly over-estimate the power of your study. \n\nClearly in terms of numbers, having fewer mice per cage is the most efficient, but optimising a sample size and a layout in this case (number of cages, number of mice per cage) depends also the relative cost of cages vs animals, and you will be limited by ethical and practical constraints.\n\nFor a full discussion see:\n\n## Randomising treatments *within* clusters\n\nA converse situation arises if we can randomise *eyes* independently to treatments.  In this case the unit of experiment is the eye, but we get the added benefit that we can remove the between-person variance from the analysis, since every person contributes one treated and one control eye.\n\n\n```{webr-r}\n\nN=20      # Study size per group\nrho=0.5   # Intra-class correlation between left and right eye\nsigma=10  # Standard deviation of individual eyes\nmean=100  # Grand mean\neffect=10 # True effect of RA on RNFL\neyes = c(\"L\",\"R\")\ngroups = c(\"Disease\",\"Control\")\n\n# Find the standard deviation between and within samples based on the overall SD and the ICC\nsd_between = sigma*sqrt(rho)\nsd_within = sigma*sqrt(1-rho)\n\ndat <- expand.grid(eye=eyes,patient=1:N)\n\n### Now we have the group randomised *within* the individual\ndat$group <- replicate(N,sample(groups),simplify = FALSE) |> Reduce(f=c)\n\npatientEffects = rnorm(2*N,0,sd_between)\n\ndat <- transform(dat , rnfl = mean + patientEffects[patient] + effect*(group==\"Disease\") + rnorm(nrow(dat),0,sd_within))\n\n```\n\nIf we plot the outcome with the individual participants identified, we should see that the effect of the treatment is more obvious, because the between-participant variation becomes irrelevant.\n\n```{webr-r}\ngraph=ggplot(dat ) + aes(x=group , y=rnfl) + \n  geom_point()\n\nlibrary(patchwork)\ngraph + (graph + geom_line(aes(group=patient)))\n```\n\nSo here, instead of losing power, because we can randomise *within* the clusters we gain power.  If we had to treat the individual and not the eye we would be randomising the clusters themselves, which would cost us power.\n\nThe effective sample size given N individuals, if we can randomise two treatments within an individual, is given by:\n\n$$\nN_\\text{eff}=N\\times\\frac{2}{1-\\text{ICC}}\n$$\n\nWhere $N$ is the number of individuals.  Now as the ICC goes up, the effective sample size also increases.\n\nA similar logic applies to cross-over vs parallel group human trials.  With an ICC of 0.5, then a parallel group study of, say, 100 individuals per group (total 200 individuals), would only require 50 individuals in total (100 eyes) if we can compare both treatments within each person.\n\nWhether this is practically possible depends on the nature of the intervention and the experimental units themselves.\n\n",
    "supporting": [
      "GIBA-session3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}