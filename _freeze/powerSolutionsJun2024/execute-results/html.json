{
  "hash": "10d8362d1521a1cbf3cb2da86d4f5f82",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Solutions and notes for power and sample size course exercises\"\nauthor: \"George Savva\"\ndate: \"January 2023\"\nformat:\n  html:\n    self-contained: true\n--- \n\n\n\n\n## Exercise 1\n\nThe mortality rate in the treatment arm is is 34.9%.  In the control arm is is 43.4%.\n\nThe estimate for treatment effect (risk difference) is -8.5% \n\nThe 95% confidence interval is -18.2 to +1.2 percentage points.  This means that the treatment might plausibly improve mortality by 18.2 or worsen it by 1.2 percentage points.\n\nBecause the p-value is 0.06, at the confidence interval includes 0 (no treatment effect) the journal report is that there is no significant improvement.\n\nI would conclude there is a very good chance that the treatment is an improvement on the control, although this isn't definitively proven, and it doesn't increase survival by more than 20 percentage points.\n\nPersonally I would choose the new treatment (all else equal)\n\nI would do whatever I could to get the new treatment.\n\n## Exercise 2\n\nIf we recruit 10 patients and two have stool in their virus, then our estimate for the prevalence of virus in stool can be calculated by R as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinom.test(x=2,n=10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact binomial test\n\ndata:  2 and 10\nnumber of successes = 2, number of trials = 10, p-value = 0.1094\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.02521073 0.55609546\nsample estimates:\nprobability of success \n                   0.2 \n```\n\n\n:::\n:::\n\n\n\nThe confidence interval is 0.03 (3%) to 0.56 (56%).  So this is the plausible range for the true value given the data.\n\nWith 100 participants, if we observe 20 then our calculation would be calculated with:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinom.test(x=20,n=100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact binomial test\n\ndata:  20 and 100\nnumber of successes = 20, number of trials = 100, p-value = 1.116e-09\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1266556 0.2918427\nsample estimates:\nprobability of success \n                   0.2 \n```\n\n\n:::\n:::\n\n\n\nSo 13% to 30%.\n\nWe can tweak the sample size until we have a confidence interval that is 5% either side of the truth.  N=250 gets us pretty close:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN=250\nbinom.test(x=N*0.2 , n = N)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact binomial test\n\ndata:  N * 0.2 and N\nnumber of successes = 50, number of trials = 250, p-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1522402 0.2550261\nsample estimates:\nprobability of success \n                   0.2 \n```\n\n\n:::\n:::\n\n\n\nNote this is the number of samples that we need data for, we might find that 10% or so of the participants do not give us valid data.\n\nWe can see that the sample size needed depends enormously on the precision we need for the estimate, and in this case we need to have some idea of what the answer is going to be.\n\n## Exercise 3:\n\nHere we can use the app to simulate some data\n\nhttps://georgemsavva.shinyapps.io/powerSimulator/\n\nor we can do it in code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make some assumptions\ncontrolMean = 500\ntreatmentEffect = -20\nstandardDeviation = 60\n\n# set our sample size\nN = 10\n\n# Simulate some data\ncontrolResults = rnorm(N , mean=controlMean, sd=standardDeviation)\ntestResults    = rnorm(N , mean=controlMean+treatmentEffect, sd=standardDeviation)\n\n# Plot the data\nplot(y=c(controlResults , testResults), \n     x=factor(rep(c(\"Control\", \"Test\"), each=N)), \n     ylab=\"iAUC\", xlab=\"Group\")\n\npoints(y=c(controlResults , testResults), \n       x=factor(rep(c(\"Control\", \"Test\"), each=N)))\n```\n\n::: {.cell-output-display}\n![](powerSolutionsJun2024_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Run a t-test\nt.test(controlResults, testResults)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  controlResults and testResults\nt = 0.40288, df = 16.369, p-value = 0.6923\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -46.65995  68.60581\nsample estimates:\nmean of x mean of y \n 522.9850  512.0121 \n```\n\n\n:::\n:::\n\n\n\n\nIf we only had one person per group we couldn't conclude anything, since we wouldn't know if the difference is due to the person or due to the treatment.\n\nAs we get more samples we can become more sure that any differences we see are because of the treatment.\n\nThe estimate for effect will become more precise as we increase the sample size.  \n\nUsing the app I can show that the width of the confidence interval will be about 40 units if there are approximately 60 participants in each group.\n\nIf we simulate more than one experiment, we can see that the p-value varies every time we run it.  In reality we'll only run one experiment, and we want to make sure that the probability of getting a p-value of less than 0.05 is high (given the assumption that our effect is real).\n\nIf we assume the true effect is 20 units, then to get 80% of trials significant we need about 150 participants per group.\n\n\n## Exercise 4\n\nA study has a power of 80% if, given that a true effect exists, there is an 80% chance it will detect it as a significant difference.\n\nThis clearly depends on what the magnitude of the real effect is, how much variation there is etc.\n\nWe don't learn a lot from studies that are underpowered.  In these cases it might be better to look at the effect sizes and confidence intervals, since conclusions based on p-values will be very unreliable.\n\nA study is underpowered if it is too small to be able to detect meaningul effect sizes.  For example, the ANDROMEDA-SHOCK study we looked at wasn't able to detect an effect size of 12.5% as statistically significant.  So it was underpowered for effects of that size or smaller.\n\n## Exercise 5\n\nThe fault is ignoring the risk of errors, particularly false negatives, which because much more likely if the study size is small.\n\nStatement three should reas \"If p>0.05 we don't have enough evidence to demonstrate an effect\" or something like that.  It could well be that an effect is present but our study could not detect it.\n\n## Exercise 6\n\nTo find the sample size needed (per group) to estimate the precision of the risk difference to +-10 percentage points, we can use the `precisely` library.  If you don't like code you can use the Shiny app (https://malcolmbarrett.shinyapps.io/precisely/)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If you don't have the \n#install.packages(\"precisely\")\nlibrary(precisely)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'precisely' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\nn_risk_difference(precision=0.2, \n                  exposed = 0.4,\n                  unexposed = 0.2, \n                  group_ratio = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  n_exposed n_unexposed n_total risk_difference precision exposed unexposed\n      <dbl>       <dbl>   <dbl>           <dbl>     <dbl>   <dbl>     <dbl>\n1      154.        154.    307.             0.2       0.2     0.4       0.2\n# ℹ 2 more variables: group_ratio <dbl>, ci <dbl>\n```\n\n\n:::\n:::\n\n\n\nNow to find the possible precision with 15 patients per group:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecisely::precision_risk_difference(n_exposed=15, \n                                     exposed=0.4, \n                                     unexposed=0.2,\n                                     group_ratio = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  precision risk_difference n_exposed n_unexposed n_total exposed unexposed\n      <dbl>           <dbl>     <dbl>       <dbl>   <dbl>   <dbl>     <dbl>\n1     0.640             0.2        15          15      30     0.4       0.2\n# ℹ 2 more variables: group_ratio <dbl>, ci <dbl>\n```\n\n\n:::\n:::\n\n\n\nSo with only 15 per group (a sample size of 30) we would have a confidence interval for the risk difference of 0.64!  This is enormous in the context of trying to look for difference in the outcome from 20% to 40% recovery.\n\n## Exercise 7\n\nFollowing the `pwr.t.test` video, we can find the sample size needed to test whether our bread leads to an improvement in iAUC of at least 20 units.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pwr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'pwr' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\npwr.t.test( n=NULL , d = 20/60 , power=0.8 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 142.2462\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\nTo find the smallest effect size detectable with 10 participants per group:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test( n=10 , d = NULL , power=0.8 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 10\n              d = 1.324947\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\nThis gives us a Cohen's `d` of 1.3, which would correspond to a smallest detectable difference between treatments of about 78 units (given a standard deviation of 60 units)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test( n=100 , d = NULL , power=0.8 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 100\n              d = 0.3981407\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\nWith 100 participants per group we could detect a Cohen's d of 0.4 or a difference in iAUC of about 24 units.\n\nNote there is a simple correspondence between the sample size and the Cohen's d which will translate to every study of this design (simple two group parallel study).\n\nTo find the power:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test( n=10 , d=50/60 , power=NULL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 10\n              d = 0.8333333\n      sig.level = 0.05\n          power = 0.4223915\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\nMaking a power curve is a bit fiddly:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# this line will extract the 'power' part of the output\npower = pwr.t.test( n=10:100 , # we can add a sequence instead of a single number here\n                    d=50/60 , \n                    power=NULL)$power\n\n# now we can plot:\nplot(x=10:100 , \n     y=power, \n     type=\"o\", \n     xlab=\"Sample size (per group)\", \n     ylab=\"power (%)\")\n\n# add some gridlines because I like gridlines\ngrid()\n```\n\n::: {.cell-output-display}\n![](powerSolutionsJun2024_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n## Exercise 8\n\na) Mortality rates was 35% in the `no treatment` group.\n\nb) Suppose the smallest risk difference of interest is 10%.  (this is up to you as investigator!)\n\nc) \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nES.h(0.45, 0.35)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2045252\n```\n\n\n:::\n:::\n\n\nThis would correspond to an effect size of Cohorts H = 0.2\n\nd) \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.2p.test(h=0.2, n=NULL, power=0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.2\n              n = 392.443\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n```\n\n\n:::\n:::\n\n\nSo we would need 392 patients per group.\n\ne) Suppose we knew we could expect 200 patients per group.  Then we would expect a minimum detectable effect size of:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.2p.test(h=NULL, n=200, power=0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.2801491\n              n = 200\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n```\n\n\n:::\n:::\n\n\nSo the effect if 0.28, and the control rate is 35%, we can find the smallest detectable increase:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nES.h(.35 + 0.14, 0.35)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2846913\n```\n\n\n:::\n:::\n\n\n\nSo the smallest change detectable with 80% power is about 14%.  If the expected increase in treatment effect is smaller than this then the study needs to be bigger.\n\n## Exercise 9 \n\n1) For a paired t-test, we would use the `type=\"paired\"` option.\n\n2) \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n=NULL, d = 20 / (sqrt(2)*30), power = 0.8 , type=\"paired\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 37.28621\n              d = 0.4714045\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n\n\nWe need ~40 participants, each of which will provide two measurements.\n\nIn the previous study, we would have needed 142 samples per group!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n=NULL, d = 20 / 60, power = 0.8  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 142.2462\n              d = 0.3333333\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\nSo pairing has helped a lot!\n\n3) Look at how the sample size depends on the critical threshold for p-value.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n=NULL, d = 20 / (sqrt(2)*30), power = 0.8 ,sig.level = 0.01, type=\"paired\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 55.90111\n              d = 0.4714045\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n\n```{.r .cell-code}\npwr.t.test(n=NULL, d = 20 / (sqrt(2)*30), power = 0.8 ,sig.level = 0.001, type=\"paired\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 82.24009\n              d = 0.4714045\n      sig.level = 0.001\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n\n\n## Exercise 10\n\n### Design 1:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n=NULL , d=20/30, power=0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 36.30569\n              d = 0.6666667\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n### Design 2:\n\nChange in concentration will have sd:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd_change = sqrt(2) * sqrt(1-0.6) * 30\n```\n:::\n\n\nSo the sample size needed is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n=NULL , d=20/27, power=0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 29.60082\n              d = 0.7407407\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\nOnly slightly fewer than design 1 \n\n### Design 3:\n\nGiven the ICC of 0.6, we would expect to explain 60% of variance by adjusting for the baseline value.  We can use the `Superpower` R package to find how this affects the sample size needed, or adjust the standard deviation directly and use the regular `pwr.t.test` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Superpower)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Superpower' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\npower_oneway_ancova(mu=c(0,20),n=NULL,n_cov=1,sd=30,r2=0.6, alpha_level = 0.05, beta_level = 0.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Power Calculation for 1-way ANCOVA \n\n            dfs = 1, 29\n              N = 32\n              n = 16, 16\n          n_cov = 1\n             mu = 0, 20\n             sd = 30\n             r2 = 0.6\n    alpha_level = 0.05\n     beta_level = 0.1916029\n          power = 80.83971\n           type = exact\n```\n\n\n:::\n\n```{.r .cell-code}\npwr.t.test(n=NULL, d=20 / (sqrt(0.4)*30), power=0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 15.15109\n              d = 1.054093\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\nThis suggests we only need 16 participants per group!\n\nSo this is a much more powerful approach than using the change score.  How much sample size we can save depends on how well we can explain the outcome value with the baseline value, that is what is the ICC.\n\n### Design 4:  Cross-over study\n\nWe saw in the last example that a cross-over trial where we use the same participant twice can be much more efficient:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n=NULL, d=20 / sd_change, power=0.8, type=\"paired\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 16.15352\n              d = 0.745356\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n\n\nHere we would need 16 participants, but each would have to undergo two treatments and assessments.  This might not be feasible.\n\n## Exercise 11:\n\nTo power an ANOVA is a little more complicated, because the hypothesis is more complex.  We have different effect sizes between different treatments, and we might be interested in the global p-value or in particular contrasts.\n\nThe assumptions for the power calculation will be the same as the assumptions for any ANOVA.\n",
    "supporting": [
      "powerSolutionsJun2024_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}