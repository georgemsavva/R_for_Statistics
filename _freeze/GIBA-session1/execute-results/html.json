{
  "hash": "1d052b014f8bd1959c5cf5aba759bec7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Session1\"\nformat: html\nengine: knitr\nwebr: \n  show-startup-message: false    # Disable displaying status of webR initialization\n  packages: ['ggplot2', 'ggpubr','easystats','report','ggbeeswarm'] # Install R packages on document open\nfilters:\n  - webr\n---\n\n\n\n\n\n```{webr-r}\n#| echo: false\n#| message: false\n#| warning: false\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(report)\nlibrary(ggbeeswarm)\n```\n\n\nWelcome to this one-day course in experimental design and analysis.\n\nOn these pages you'll find links to the \n\nSlides for sesion 1 are here: \n\n* [Session 1 slides](gibasession1.pptx)\n\n## Worked example\n\nHere we simulate a simple experiment with two groups of mice and a single outcome measure.\n\nWe set the 'true' average values of our outcome measure\n\nUsing simulations like this can help us to think through experiments, plan analyses and ultimately run sample size calculations.\n\n### Simulate the data\n\n```{webr-r}\n#| warning: false\n\nset.seed(1)\n\nN1=5 # sample size for group 1\nN2=3 # sample size for group 2\n\n# true mean values in each group\nmean = c(\"Control\"=50,\n         \"Treated\"=50)\n\n# Make group labels\ngroup = c( rep(\"Control\",N1) , rep(\"Treated\",N2) ) |> factor(levels=c(\"Control\",\"Treated\"))\n\n# Simulate richness, assume it has a negative binomial distribution with size 1 and mean defined above\nrichness = rnbinom(N1+N2, mu=mean[group],size=1)\n\n# Turn this into a dataset\ndat = data.frame(group,richness)\n\n```\n\n### Initial analysis\n\nNow we have simulated the data we can run our analysis, in this case a bar chart with error bars combined with a p-value to compare the groups.\n\n```{webr-r}\n\nggplot(dat, aes(x=group,y=richness)) +  \n  stat_summary(geom=\"col\",col=\"black\",fill=\"red\",width=0.6, fun.data = \"mean_se\") + \n  stat_summary(geom=\"errorbar\" , width=.30, fun.data = \"mean_se\") + \n  stat_compare_means(comparisons = list(1:2), method=\"t.test\") + \n  \n  theme_bw()\n\n```\n\nWhat would you conclude?\n\n### A better graph and statistics\n\nWhile this presentation of results (p-value and dynamite plot) is often used, it is extremely limiting.  It is more useful to present data point individually (perhaps with summary statistics) and then to make inferences about the estimated  mean difference between groups:\n\n```{webr-r}\n\nggplot(dat, aes(x=group,y=richness)) +  \n  ggbeeswarm::geom_beeswarm(pch=4) + \n  stat_summary(geom=\"errorbar\" , width=.30, fun.data = \"mean_se\") +\n  stat_summary(geom=\"point\" , width=.30, fun.data = \"mean_se\") + \n  stat_compare_means(comparisons = list(1:2), method=\"t.test\") + \n  theme_bw()\n\n\nreport_sample(dat , by=\"group\")\n\nt.test(data=dat , richness ~ group) |> report::report_parameters()\n\nlm(data=dat , richness ~ group) |> parameters::parameters() |> plot()\n\n```\n\n## Accuracy and precision\n\nSuppose we conduct a research study to estimate the effect of our supplement on the alpha diversity.\n\nEach instance of our study will produce an estimate for the effect, which is (hopefully!) close to the truth.\n\nSuppose it was possible to repeat the study many times, you would produce a different effect estimate from each one.\n\nThe study is *unbiased* if the mean average value of those repeats is equal to the true value. That is, while there will be some chance variation the estimate will not be systematically too high or too low.\n\nAny bias in the study would cause the study estimate to be systematically wrong in either direction.  There are many sources of bias.  For example we know that healthier people tend to more willing to participate in research than the population (selection bias), or people who are treated with placebos might report better symptoms than those who are not (plaebo effect).  These can lead to systematically wrong conclusions about populations or the efficacy of treatments.\n\nThe precision of the study refers to how similar the estimates from repeated runs would be.  A precise estimates have small standard errors and narrow confidence intervals.  But it is possible to be very precisely wrong (if there is bias), or to miscalculate the precision of your study.\n\nPrecision is largely determined by the level of variation between in your experimental units, your measurements, and by the sample size.  Increasing sample size and reducing sources of variation will increase precision.\n\n\n```{webr-r}\n\nN=10\ntruth = 10\nbiasedEstimates = rnorm(N,truth+3,0.5)\nimpreciseEstimates = rnorm(N,truth,2)\nbiasedAndImprecise = rnorm(N,truth+3,2)\ngoodEstimates = rnorm(N,truth,0.5)\n\nrbind(goodEstimates,biasedAndImprecise,impreciseEstimates,biasedEstimates) |> \n  as.data.frame.table() |>\n  setNames(c(\"Type\",\"Var\",\"Estimate\")) |>\n  ggplot(aes(x=Type,y=Estimate)) + \n  geom_beeswarm() + \n  geom_hline(yintercept=truth,lty=\"dotted\") + \n  coord_flip() + \n  theme_bw() + \n  labs(y=\"Estimates\", x=NULL) + \n  scale_x_discrete(labels=c(\"biasedEstimates\"=\"Biased Estimates\",\n                            \"impreciseEstimates\"=\"Imprecise Estimates\",\n                            \"biasedAndImprecise\"=\"Biased and Imprecise\",\n                            \"goodEstimates\"=\"Precise and unbiased (good!)\")) + \n  annotate(geom=\"text\",label=\"True effect size\",x=0.5,y=truth)\n\n\n```\n\nOur goal in experimental design is to answer your research questions with low bias *and* high precision, so that your estimates are consistently close to the truth!  On the whole bias is more problematic than imprecision, because the latter can be calculated, reported, and overcome using larger studies.  Bias is much more difficult to detect, and can become compounded by larger samples or repeated studies if the same biases are present each time.\n\nTo deal with sources of bias and variance, we have to understand how they arise, and how to reduce or remove their effects on our estimates.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}