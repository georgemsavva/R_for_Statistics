{
  "hash": "46a85a1833080cc0c848f468ed9ff027",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Session1\"\nformat: html\nengine: knitr\nwebr: \n  show-startup-message: false    # Disable displaying status of webR initialization\n  packages: ['ggplot2', 'ggpubr','easystats','report','ggbeeswarm'] # Install R packages on document open\nfilters:\n  - webr\n---\n\n\n\n\n\n```{webr-r}\n#| autorun: true\n#| message: false\n#| warning: false\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(report)\nlibrary(ggbeeswarm)\nlibrary(patchwork)\n\n```\n\n\n## Introduction\n\nThese pages include slides and code to support the first day of our course in experimental design and analysis.\n\nA revision of the slides will be available soon: \n\n\nThe code in these windows can be run directly from your browser,\nor copied into your own R session.\n\nIf you want to use your own R sesion, you'll need to have packages installed:\n\n`ggplot2`, `see`, `ggpubr`, `ggbeeswarm`, `patchwork` and `easystats`\n\n## Worked example\n\nHere we simulate a simple experiment with two groups of mice and a single outcome measure.\n\nWe set the 'true' average values of our outcome measure, simulate an experiment with our chosen design, and then attempt to estimate the difference between groups using our simulated data.\n\nUsing simulations like this can help us to think through experiments, plan analyses and ultimately run sample size calculations.\n\n### Simulate the data\n\n```{webr-r}\n#| warning: false\n\nset.seed(1)\n\nN1=5 # sample size for group 1\nN2=3 # sample size for group 2\n\n# true mean values in each group\nmean = c(\"Control\"=50,\n         \"Treated\"=50)\n\n# Make group labels\ngroup = c( rep(\"Control\",N1) , rep(\"Treated\",N2) ) |> factor(levels=c(\"Control\",\"Treated\"))\n\n# Simulate richness, assume it has a negative binomial distribution with size 1 and mean defined above\nrichness = rnbinom(N1+N2, mu=mean[group],size=1)\n\n# Turn this into a dataset\ndat = data.frame(group,richness)\n\n# List the first few rows\nhead(dat)\n```\n\n### Initial analysis\n\nNow we have simulated the data we can run our analysis, in this case a bar chart with error bars combined with a p-value to compare the groups.\n\n```{webr-r}\n\nggplot(dat, aes(x=group,y=richness)) +  \n  stat_summary(geom=\"col\",col=\"black\",fill=\"red\",width=0.6, fun.data = \"mean_se\") + \n  stat_summary(geom=\"errorbar\" , width=.30, fun.data = \"mean_se\") + \n  stat_compare_means(comparisons = list(1:2), method=\"t.test\") + \n  \n  theme_bw()\n\n```\n\nWhat would you conclude?\n\n### A better graph and statistics\n\nWhile this presentation of results (p-value and dynamite plot) is often used, it is extremely limiting.  It is more useful to present data point individually (perhaps with summary statistics) and then to make inferences about the estimated  mean difference between groups:\n\n```{webr-r}\n\nggplot(dat, aes(x=group,y=richness)) +  \n  ggbeeswarm::geom_beeswarm(pch=4) + \n  stat_summary(geom=\"errorbar\" , width=.30, fun.data = \"mean_se\") +\n  stat_summary(geom=\"point\" , fun.data = \"mean_se\") + \n  stat_compare_means(comparisons = list(1:2), method=\"t.test\") + \n  theme_bw()\n\n```\n\n```{webr-r}\n\nreport_sample(dat , by=\"group\")\n\nt.test(data=dat , richness ~ group) |> report::report_parameters()\n\n```\n\nIt's interesting to compare the linear model result with the t-test result.\n\n```{webr-r}\nmodel1 <- lm(data=dat , richness ~ group) \nsummary(model1)\nconfint(model1)\nmodel1 |> parameters::parameters() |> plot()\n```\n\n\n\n## Accuracy and precision\n\nSuppose we conduct a research study to estimate the effect of our supplement on the alpha diversity.\n\nEach instance of our study will produce an estimate for the effect, which is (hopefully!) close to the truth.\n\nSuppose it was possible to repeat the study many times, you would produce a different effect estimate from each one.\n\nThe study is *unbiased* if the mean average value of those repeats is equal to the true value. That is, while there will be some chance variation the estimate will not be systematically too high or too low.\n\nAny bias in the study would cause the study estimate to be systematically wrong in either direction.  There are many sources of bias.  For example we know that healthier people tend to more willing to participate in research than the population (selection bias), or people who are treated with placebos might report better symptoms than those who are not (plaebo effect).  These can lead to systematically wrong conclusions about populations or the efficacy of treatments.\n\nThe precision of the study refers to how similar the estimates from repeated runs would be.  A precise estimates have small standard errors and narrow confidence intervals.  But it is possible to be very precisely wrong (if there is bias), or to miscalculate the precision of your study.\n\nPrecision is largely determined by the level of variation between in your experimental units, your measurements, and by the sample size.  Increasing sample size and reducing sources of variation will increase precision.\n\n\n```{webr-r}\n\nN=10\ntruth = 10\nbiasedEstimates = rnorm(N,truth+3,0.5)\nimpreciseEstimates = rnorm(N,truth,2)\nbiasedAndImprecise = rnorm(N,truth+3,2)\ngoodEstimates = rnorm(N,truth,0.5)\n\nrbind(goodEstimates,biasedAndImprecise,impreciseEstimates,biasedEstimates) |> \n  as.data.frame.table() |>\n  setNames(c(\"Type\",\"Var\",\"Estimate\")) |>\n  ggplot(aes(x=Type,y=Estimate)) + \n  geom_beeswarm() + \n  geom_hline(yintercept=truth,lty=\"dotted\") + \n  coord_flip() + \n  theme_bw() + \n  labs(y=\"Estimates\", x=NULL) + \n  scale_x_discrete(labels=c(\"biasedEstimates\"=\"Biased Estimates\",\n                            \"impreciseEstimates\"=\"Imprecise Estimates\",\n                            \"biasedAndImprecise\"=\"Biased and Imprecise\",\n                            \"goodEstimates\"=\"Precise and unbiased (good!)\")) + \n  annotate(geom=\"text\",label=\"True effect size\",x=0.5,y=truth)\n\n\n```\n\nOur goal in experimental design is to answer your research questions with low bias *and* high precision, so that your estimates are consistently close to the truth!  On the whole bias is more problematic than imprecision, because the latter can be calculated, reported, and overcome using larger studies.  Bias is much more difficult to detect, and can become compounded by larger samples or repeated studies if the same biases are present each time.\n\nTo deal with sources of bias and variance, we have to understand how they arise, and how to reduce or remove their effects on our estimates.\n\n\n## Analysis of a blocked randomised study\n\n### Introduction\n\nThe code below simulates a parallel group randomised controlled trial conducted in men and women.\n\nThere were 40 participants in total, 20 men and 20 women.\n\nThere was a blocked randomisation, so that men and women were balanced across groups (10 each per treatment group)\n\nThe blocking was performed because we know that men have typically higher responses than women.\n\n\n```{webr-r}\n\n# Define the groups\ntreatment = c(\"Treat\", \"Control\")\nsex = c(\"Male\",\"Female\")\n\n# Here we set up the dataset with 10 copies of each combination of treatment and sex\ndat <- expand.grid( treatment=treatment , sex=sex,rep=1:10)\n\n# True treatment effect = 1\n# True sex effect = 2\n# Normally distributed indidividual variation with sd=1\n\nset.seed(200) # Setting the seed fixes the response\n\n# Now generate the response (y) with the known treatment effect, sex effect and random variation\ndat <- transform(dat, y = 10 + (treatment==\"Treat\") + 2*(sex==\"Male\") + rnorm(nrow(dat)))\n\n# Check the first few rows\nhead(dat)\n\n```\n\nRemember the experimental design equation for this study is:\n\n`Outcome = Treatment + Sex + Error`\n\nBelow we show why it is important to respect this equation in our analysis phase as well as our design phase.\n\nSince we have a two group study with a continuous outcome, we might be tempted to apply a two-groups t-test to compare the responses:\n\n```{webr-r}\n# The first graph \ngraph1 = ggplot(dat) + \n  aes(x=treatment, y=y) + \n  geom_beeswarm(aes(shape=sex)) + \n  theme_bw()+ \n  stat_summary(geom=\"errorbar\",width=.5,fun.data = \"mean_se\") + \n  scale_y_continuous(limits=c(8,15)) + \n  ggpubr::stat_compare_means(method=\"t.test\",comparisons=list(1:2))  + \n  labs(x=\"Treatment\", y=\"Response\",shape=\"Sex\")\n```\n\nWhile there is some evidence of the difference between groups (that we know is there!), it is not statistically significant.  We have type-2 error in this case (false negative).  This is caused by the high variation within each treatment group overwhelming the signal from the treatment.\n\nBut if we stratify the data by sex, we explain much of that variation.  The signal is much more obvious, and is statistically significant in both groups despite there being half the participants in each!\n\n```{webr-r}\ngraph1 + (graph1 + facet_wrap(~sex) ) + plot_layout(guides=\"collect\")\n```\n\nThe linear model corresponding to the unpaired t-test is given below. Note the residual error in the output and the standard error for the effect of treatment.\n\n```{web-r}\nlm(data=dat , y ~ treatment) |> summary()\n```\n\nNext we'll try the model that corresponds to the design equation.  We have explained far more variance (smaller residual error), and so have a m.\n\n```{web-r}\nlm(data=dat , y ~ treatment + sex) |> summary()\n```\n\nAlthough this dataset was picked to illustrate the point, we can find the power of the study with each analytical approach by replicating the study many times, and finding what proportion of p-values is less than 0.05.\n\n```{webr-r}\n\noneRep <- function(){\n  dat <- transform(dat, y = 10 + (treatment==\"Treat\") + 2*(sex==\"Male\") + rnorm(nrow(dat)))\n  model1_p <- lm(data=dat , y ~ treatment) |> summary() |> coef())[\"treatmentControl\",\"Pr(>|t|)\"]\n  model2_p <- lm(data=dat , y ~ treatment + sex) |> summary() |> coef())[\"treatmentControl\",\"Pr(>|t|)\"]\n  c(model1_p,model2_p)\n}\n\nreplicate(1000,oneRep()) |> t() |> as.data.frame()|> lapply(\\(x) mean(x<0.05))\n\n```\n\nHere, using the design equation to set up the analysis model leads to a much more powerful design, by explaining the variance associated with the identified factors.  It also means that the assumptions underlying the regression model, in particular normal random errors, more likely to be met.\n\n```{web-r}\nlm(data=dat , y ~ treatment + sex) |> report::report_model()\n```\n\n",
    "supporting": [
      "GIBA-session1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}