{
  "hash": "532afccf1a06e67358841de4ada585b8",
  "result": {
    "markdown": "---\ntitle: \"Clustering independent of sampling in observational studies\"\nauthor: \"George Savva\"\n---\n\n\n\n\n## Introduction\n\nWe know that random sampling is important to get good inferences from epidemiological studies.  \n\nIf sampling is 'clustered' then we need to account for this in our analysis.  This is well known.\n\nHowever we may have a situation where data points are generated in clusters, but we still sample them randomly.  Is there a risk of false positive associations in such situations?  How should we deal with it?\n\nHere I use a simple simulation to show that even with random sampling, data that is generated in clusters can still lead us to false positive conclusions.\n\n## Simulation data \n\nThe research question here is whether there is a difference between two populations.\n\nFor the simulation I will create a dataset of 100 points from each population.  Each point is sampled independently at random, but arises from one of five clusters in each population.  So, to sample each point we first select a cluster at random, then generate a point from that cluster.\n\nThe cluster averages in turn are selected from a normal distribution with mean zero.\n\nAn real example question might relate to whether there is a difference in educational attainment between two different cities.  We can select children from each city completely at random.  But if each city has only five schools then the data points are clustered by school, even if the children are selected completely randomly without any reference to which school they attend.\n\nSo although children are selected independently of each other, they are still clustered in some sense!  Do we still need to run a 'clustered' analysis if we are interested in understanding whether 'city' has any effect on educational attainment?\n\nRemember we know that there is no effect of city on average school outcome in our simulation.  Differences in outcomes do occur completely randomly at the school level and at the individual child level.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNclusters <- 10\nclustersMeans <- rnorm(Nclusters, 0,1)\n\n## Suppose group A come from clusters 1 to 5\n## Group B come from clusters 6 to 10.\n\nNperGroup <- 100\nGroupAClusters <- sample(1:5,NperGroup, replace=TRUE)\nGroupBClusters <- sample(6:10,NperGroup, replace=TRUE)\n\ndat <- data.frame(Group=rep(c(\"A\",\"B\"),each=NperGroup), cluster=c(GroupAClusters, GroupBClusters))\ndat$y <- rnorm(NperGroup*2 , clustersMeans[dat$cluster], 1)\n\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Group cluster           y\n1     A       1  2.17314644\n2     A       4  0.23199408\n3     A       4 -0.14176686\n4     A       2  1.05632863\n5     A       4  0.15161343\n6     A       3  0.08051086\n```\n:::\n:::\n\n\nNow we can plot the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ndat |> ggplot(aes(x=Group, y=y, shape=factor(cluster))) + \n  scale_shape_manual(values=1:10) + \n  geom_point(position = position_dodge2(width=0.2)) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](independentclusters_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe clustering is evident in the plot.  How does this affect the p-values for a linear model?  Do we need to use a LMM?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## A naive linear model would suggest a difference between groups\nlm(y ~ Group , data=dat) |> broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic     p.value\n  <chr>          <dbl>     <dbl>     <dbl>       <dbl>\n1 (Intercept)    0.286     0.111      2.56 0.0111     \n2 GroupB        -0.821     0.158     -5.21 0.000000480\n```\n:::\n\n```{.r .cell-code}\n## While mixed model taking the clustering into account does not\nlmer(y ~ Group + (1|cluster), data=dat) |> broom.mixed::tidy() |> subset(effect==\"fixed\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 8\n  effect group term        estimate std.error statistic    df p.value\n  <chr>  <chr> <chr>          <dbl>     <dbl>     <dbl> <dbl>   <dbl>\n1 fixed  <NA>  (Intercept)    0.313     0.291      1.07  8.08  0.313 \n2 fixed  <NA>  GroupB        -0.913     0.412     -2.22  8.08  0.0571\n```\n:::\n:::\n\n\nSo if we ignore the clusters we get a false positive result (p<0.001), but if we incorporate them into our analysis we do not.\n\n## Type 1 error rate\n\nTo quantify the problem we can look at how the false positive rate changes with the extent of the clustering.\n\nWe'll simulate datasets and analysis 1000 times for each value of the cluster standard deviation, from 1.0 (the same as the between individual standard deviation) to zero. Then we'll look at the proportion of statistically significant results (p<0.05), and compare this to the nominal 5% rate.\n\nIn each simulation we generate new cluster means, and then a new dataset of points.  We then apply the simple linear model and a mixed effects model, and extract the p-values for the hypothesis test that the effect of 'population' is zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noneRep <- function(clusterSD){\n  clustersMeans <- rnorm(Nclusters, 0,clusterSD)\n  dat$y <- rnorm(NperGroup*2 , clustersMeans[dat$cluster], 1)\n  plmer <- (lmer(y ~ Group + (1|cluster), data=dat) |> summary() |> coef())[2,5]\n  plm <- (lm(y ~ Group , data=dat) |> summary() |> coef())[2,4]\n  c(plmer,plm)\n}\n\npvals <- lapply(c(1.0,0.8,0.6,0.4,0.2,0.0) , \\(s) replicate(1000,oneRep(s)))\n\nsignificant <- sapply(pvals, apply, 1, \\(p) mean(p<0.05)) |> t() |> as.data.frame() |> setNames(c(\"Mixed model\", \"Simple Linear model\"))\nsignificant$clusterVariance <- c(1.0,0.8,0.6,0.4,0.2,0.0)\n\nsignificant <- data.table::melt(significant, id.vars=\"clusterVariance\", variable.name=\"Method\", value.name=\"rate\")\n\nggplot(significant) + aes(x=clusterVariance, y=rate, col=Method) + geom_point() + geom_line() + \n  geom_hline(yintercept=0.05, lty=\"dashed\") + \n  labs(x=\"Between cluster standard deviation\", y=\"Type 1 error rate\")\n```\n\n::: {.cell-output-display}\n![](independentclusters_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Conclusion and Discussion \n\nThe type 1 error rate increases markedly with the among of variance between 'clusters' when a simple linear model is used.  A mixed model broadly corrects this, and may over-compensate when the between-cluster variance is very low.\n\nSo for our school analogy, if schools are very variable, then if we don't model the clustering it may look like there are inherent differences between cities instead of there simply being random differences between the schools that are not attributable to the city they are in.\n\nWhether this is the correct or incorrect answer depends on exactly what the question is that we are trying to answer, that is whether we are trying to (1) identify the city that just happens (by chance) to have the best schools, or (2) whether we are asking about whether the city itself promotes good schools or good outcomes.  \n\nIn most situations we are asking the latter question in which case we need to include the grouping in our analysis to get a meaningful result.\n\nTo generate our simulated datasets we included no effect of the grouping factor on the cluster means or the individual means, and we sampled each unit independently from the population.  Yet a simple linear model (equivalent of t-test or ANOVA) returned a strongly significant effect.  This was corrected by considering the shared variance within clusters in our model.\n\nThis should be another reminder that to avoid looking silly we need to understand how our datasets come to be, and what shared variance there is between our observations, even if we know that our samples are generated from our populations using simple random sampling.\n\n",
    "supporting": [
      "independentclusters_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}